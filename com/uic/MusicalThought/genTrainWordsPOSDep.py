__author__ = 'vignesh'
import ConfigParser as cp
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
import nltk
from collections import defaultdict

def getWordFeatures(word):
    """
    Function to get word level features for a given word
    :param word: Word
    :return: List of word level features like Capitalized, Comma, End, Last,
    """

    wordFeatures = []
    if word[ 0].isupper():
        wordFeatures.append('Capitalized')
    if word[-1] == '.':
        wordFeatures.append('End')
    if ',' in word:
        wordFeatures.append('Comma')
    if word[-1] == '\n':
        wordFeatures.append('Last')
    return ' '.join(wordFeatures)

def gen_all_word_level_features(theWords):
    """
    Returns all word level features of the current sentence

    :param theWords: Sentence for which features need to be extracted
    :return:
    """

    d_theFeatures = {}
    d_theClasses = {}
    d_features = defaultdict(list)


    for i in range(len(theWords)):

        curr_word = theWords[i]

        # Case where the word is either // or /
        # the class of the previous word is // or / correspondingly
        if curr_word == '/' or curr_word == '//':
            d_theClasses[ i - 1] = curr_word
            wordFeatures = getWordFeatures(curr_word)
            curr_word = curr_word.lower()
            curr_word = curr_word.replace('.','').replace(',','').replace('\n','')
            if wordFeatures == []:
                wordfeatures = curr_word
            else:
                wordFeatures = wordFeatures + ' ' + curr_word
            d_theFeatures[ i] = wordFeatures # this has key as i

            # also add to features which has key as the word
            d_features[ theWords[ i]] = wordFeatures
            d_theClasses[ i] = 'None'

        elif curr_word != '/' and '/' in curr_word:

            # Replace / with ''
            curr_word = curr_word.replace('/', '')

            wordFeatures = getWordFeatures(curr_word)
            curr_word = curr_word.lower()
            curr_word = curr_word.replace('.','').replace(',','').replace('\n','')
            if wordFeatures == []:
                wordfeatures = curr_word
            else:
                wordFeatures = wordFeatures + ' ' + curr_word

            d_theFeatures[ i] = wordFeatures

            # also add to features which has key as the word
            d_features[ theWords[ i]] = wordFeatures
            d_theClasses[ i] = '/' #Marking the class for the current word as /

        elif curr_word != '//' and '//' in curr_word:

            # Replace // with ''
            curr_word = curr_word.replace('/', '')

            if wordFeatures == []:
                wordfeatures = curr_word
            else:
                wordFeatures = wordFeatures + ' ' + curr_word

            curr_word = curr_word.lower()
            curr_word = curr_word.replace('.','').replace(',','').replace('\n','')
            # if wordFeatures == []:
            #     wordFeatures = [curr_word]
            # else:
            #     wordFeatures.append(curr_word)
            wordFeatures = wordFeatures + ' ' + curr_word

            # also add to features which has key as the word
            d_features[ theWords[ i]] = wordFeatures
            d_theFeatures[ i] = wordFeatures
            d_theClasses[ i] = '//'

        else:

            wordFeatures = getWordFeatures(curr_word)
            curr_word = curr_word.lower()
            curr_word = curr_word.replace('.','').replace(',','').replace('\n','')
            if wordFeatures == []:
                wordfeatures = curr_word
            else:
                wordFeatures = wordFeatures + ' ' + curr_word

            d_theFeatures[ i] = wordFeatures

            # also add to features which has key as the word
            d_features[ theWords[ i]] = wordFeatures

            d_theClasses[ i] = 'Other'

    return (d_theClasses, d_theFeatures)


def merge_features_and_tags(d_features, tags):
    """

    :param d_features: dictionray of word features generated by gen_all_word_level_features
    :param tags: Pos tags generated by nltk
    :return: single dictionary containing both pos tags and word features
    """

    d_combined_features = {}
    keys = d_features.keys()
    tag_keys = [t[0] for t in tags]
    tag_dict = dict(tags)
    for k in keys:
        values = d_features[k]
        tag_values = tag_dict[k]

        # combine the features
        temp = tag_values + values
        d_combined_features[k] = temp
        print temp

    return d_combined_features



# Read the configuration from the configuration file

config = cp.RawConfigParser()
config.read('config.cfg')
trainfile = config.get('init', 'trainfile')

full_text = open(trainfile).read() # Gets the complete text
sentences = sent_tokenize(full_text)

# We maintain a dictionary of features, classes and tags
# to account for words having different features in
# different sentences

allfeatures = []
allclasses = []
alltags = []
tags = defaultdict(list)

# Run through sentences and collect the pos tag
# and other features

for s in sentences:

    # get the word tokens
    word_tokens = word_tokenize(s) # Generates tokens, including tags(/,//)

    # get the pos tags

    word_tags = nltk.pos_tag(word_tokens)
    tags = [t[1] for t in word_tags]

    # get the other word level features
    # these two will have the following
    # d_temp_classes : for each word in the sentence, the class ( '/', '//', 'Other')
    # d_temp_features : for each word in the sentence, word level features
    (d_temp_classes, d_temp_features) = gen_all_word_level_features(word_tokens)


    # Merge features and tags
    #combined_features = merge_features_and_tags(d_temp_features, word_tags) # for each w => word features + tags

    allfeatures.append(d_temp_features)
    allclasses.append(d_temp_classes)
    alltags.append(tags)

for i in range(len(allfeatures)):
    for j in range(len(allfeatures[i])):
        if (allclasses[i][j] != 'None'):
            #print allclasses[i][j] + "\t" + allfeatures[i][j] + " " + alltags[i][j]
            print allclasses[i][j] + "\t" + alltags[i][j]




# s1 = sentences[0]
# words = word_tokenize(s1)
#
# # pos tagging
# print words
#
# print nltk.pos_tag(words)







