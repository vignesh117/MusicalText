This is about a hidden corner of the labor market. It's the world of people who need to work ultra-flexibly, if they're to work at all. So think, for instance, of someone who has a recurring but unpredictable medical condition, or somebody who's caring for a dependent adult, or a parent with complex child care needs. Their availability for work can be such that it's, "A few hours today. Maybe I can work tomorrow, but I don't know if and when yet." And it's extraordinarily difficult for these people to find the work that they so often need very badly. Which is a tragedy because there are employers who can use pools of very flexible local people booked completely ad hoc around when that person wants to work.
Imagine that you run a cafe. It's mid-morning, the place is filling up. You're going to have a busy lunchtime rush. If you could get two extra workers for 90 minutes to start in an hour's time, you'd do it, but they'd have to be reliable, inducted in how your cafe works. They'd have to be available at very competitive rates. They'd have to be bookable in about the next minute. In reality, no recruitment agency wants to handle that sort of business, so you are going to muddle by, understaffed. And it's not just caterers, it's hoteliers, it's retailers, it's anyone who provides services to the public or businesses. There's all sorts of organizations that can use these pools of very flexible people, possibly already once they've been inducted.
At this level of the labor market, what you need is a marketplace for spare hours. They do exist. Here's how they work. So in this example, a distribution company has said, we've got a rush order that we've got to get out of the warehouse tomorrow morning. Show us everyone who's available. It's found 31 workers. Everybody on this screen is genuinely available at those specific hours tomorrow. They're all contactable in time for this booking. They've all defined the terms on which they will accept bookings. And this booking is within all the parameters for each individual. And they would all be legally compliant by doing this booking. Of course, they're all trained to work in warehouses. You can select as many of them as you want. They're from multiple agencies. It's calculated the charge rate for each person for this specific booking. And it's monitoring their reliability. The people on the top row are the provenly reliable ones. They're likely to be more expensive. In an alternative view of this pool of local, very flexible people, here's a market research company, and it's inducted maybe 25 local people in how to do street interviewing. And they've got a new campaign. They want to run it next week. And they're looking at how many of the people they've inducted are available each hour next week. And they'll then decide when to do their street interviews.
But is there more that could be done for this corner of the labor market? Because right now there are so many people who need whatever economic opportunity they can get. Let's make it personal. Imagine that a young woman -- base of the economic pyramid, very little prospect of getting a job -- what economic activity could she theoretically engage in? Well, she might be willing to work odd hours in a call center, in a reception area, in a mail room. She may be interested in providing local services to her community: babysitting, local deliveries, pet care. She may have possessions that she would like to trade at times she doesn't need them. So she might have a sofa bed in her front room that she would like to let out. She might have a bike, a video games console she only uses occasionally. And you're probably thinking -- because you're all very web-aware -- yes, and we're in the era of collaborative consumption, so she can go online and do all this. She can go to Airbnb to list her sofa bed, she can go to TaskRabbit.com and say, "I want to do local deliveries," and so on.
These are good sites, but I believe we can go a step further. And the key to that is a philosophy that we call modern markets for all. Markets have changed beyond recognition in the last 20 years, but only for organizations at the top of the economy. If you're a Wall Street trader, you now take it for granted that you sell your financial assets in a system of markets that identifies the most profitable opportunities for you in real time, executes on that in microseconds within the boundaries you've set. It analyzes supply and demand and pricing and tells you where your next wave of opportunities are coming from. It manages counterparty risk in incredibly sophisticated ways. It's all extremely low overhead. What have we gained at the bottom of the economy in terms of markets in the last 20 years? Basically classified adverts with a search facility.
So why do we have this disparity between these incredibly sophisticated markets at the top of the economy that are increasingly sucking more and more activity and resource out of the main economy into this rarefied level of trading, and what the rest of us have? A modern market is more than a website; it's a web of interoperable marketplaces, back office mechanisms, regulatory regimes, settlement mechanisms, liquidity sources and so on. And when a Wall Street trader comes into work in the morning, she does not write a listing for every financial derivative she wants to sell today and then post that listing on multiple websites and wait for potential buyers to get in touch and start negotiating the terms on which she might trade.
In the early days of this modern markets technology, the financial institutions worked out how they could leverage their buying power, their back office processes, their relationships, their networks to shape these new markets that would create all this new activity. They asked governments for supporting regulatory regimes, and in a lot of cases they got it.
But throughout the economy, there are facilities that could likewise leverage a new generation of markets for the benefit of all of us. And those facilities -- I'm talking about things like the mechanisms that prove our identity, the licensing authorities that know what each of us is allowed to do legally at any given time, the processes by which we resolve disputes through official channels. These mechanisms, these facilities are not in the gift of Craigslist or Gumtree or Yahoo, they're controlled by the state. And the policymakers who sit on top of them are, I suggest, simply not thinking about how those facilities could be used to underpin a whole new era of markets.
Like everyone else, those policymakers are taking it for granted that modern markets are the preserve of organizations powerful enough to create them for themselves. Suppose we stopped taking that for granted. Suppose tomorrow morning the prime minister of Britain or the president of the U.S., or the leader of any other developed nation, woke up and said, "I'm never going to be able to create all the jobs I need in the current climate. I have got to focus on whatever economic opportunity I can get to my citizens. And for that they have to be able to access state-of-the-art markets. How do I make that happen?"
And I think I can see a few eyes rolling. Politicians in a big, complex, sophisticated I.T. project? Oh, that's going to be a disaster waiting to happen. Not necessarily. There is a precedent for technology-enabled service that has been initiated by politicians in multiple countries and has been hugely successful: national lotteries.
Let's take Britain as an example. Our government didn't design the national lottery, it didn't fund the national lottery, it doesn't operate the national lottery. It simply passed the National Lottery Act and this is what followed. This act defines what a national lottery will look like. It specifies certain benefits that the state can uniquely bestow on the operators. And it puts some obligations on those operators. In terms of spreading gambling activity to the masses, this was an unqualified success.
But let's suppose that our aim is to bring new economic activity to the base of the pyramid. Could we use the same model? I believe we could. So imagine that policymakers outlined a facility. Let's call it national e-markets, NEMs for short. Think of it as a regulated public utility. So it's on a par with the water supply or the road network. And it's a series of markets for low-level trade that can be fulfilled by a person or a small company. And government has certain benefits it can uniquely bestow on these markets. It's about public spending going through these markets to buy public services at the local level. It's about interfacing these markets direct into the highest official channels in the land. It's about enshrining government's role as a publicist for these markets. It's about deregulating some sectors so that local people can enter them.
So, taxi journeys might be one example. And there are certain obligations that should go with those benefits to be placed on the operators, and the key one is, of course, that the operators pay for everything, including all the interfacing into the public sector. So imagine that the operators make their return by building a percentage markup into each transaction. Imagine that there's a concession period defined of maybe 15 years in which they can take all these benefits and run with them. And imagine that the consortia who bid to run it are told, whoever comes in at the lowest percentage markup on each transaction to fund the whole thing will get the deal.
So government then exits the frame. This is now in the hands of the consortium. Either they are going to unlock an awful lot of economic opportunity and make a percentage on all of it or it's all going to crash and burn, which is tough on their shareholders. It doesn't bother the taxpayer necessarily. And there would be no constraints on alternative markets. So this would just be one more choice among millions of Internet forums. But it could be very different, because having access to those state-backed facilities could incentivize this consortium to seriously invest in the service. Because they would have to get a lot of these small transactions going to start making their return.
So we're talking about sectors like home hair care, the hire of toys, farm work, hire of clothes even, meals delivered to your door, services for tourists, home care. This would be a world of very small trades, but very well-informed, because national e-markets will deliver data.
So this is a local person potentially deciding whether to enter the babysitting market. And they might be aware that they would have to fund vetting and training if they wanted to go into that market. They'd have to do assessment interviews with local parents who wanted a pool of babysitters. Is it worth their while? Should they be looking at other sectors? Should they be moving to another part of the country where there's a shortage of babysitters? This kind of data can become routine. And this data can be used by investors. So if there's a problem with a shortage of babysitters in some parts of the country and the problem is nobody can afford the vetting and training, an investor can pay for it and the system will tithe back the enhanced earnings of the individuals for maybe the next two years.
This is a world of atomized capitalism. So it's small trades by small people, but it's very informed, safe, convenient, low-overhead and immediate. Some rough research suggests this could unlock around 100 million pounds' worth a day of new economic activity in a country the size of the U.K.
Does that sound improbable to you? That's what a lot of people said about turbo trading in financial exchanges 20 years ago. Do not underestimate the transformative power of truly modern markets.
As humans, it's in our nature to want to improve our health and minimize our suffering. Whatever life throws at us, whether it's cancer, diabetes, heart disease, or even broken bones, we want to try and get better. Now I'm head of a biomaterials lab, and I'm really fascinated by the way that humans have used materials in really creative ways in the body over time.
Take, for example, this beautiful blue nacre shell. This was actually used by the Mayans as an artificial tooth replacement. We're not quite sure why they did it. It's hard. It's durable. But it also had other very nice properties. In fact, when they put it into the jawbone, it could integrate into the jaw, and we know now with very sophisticated imaging technologies that part of that integration comes from the fact that this material is designed in a very specific way, has a beautiful chemistry, has a beautiful architecture. And I think in many ways we can sort of think of the use of the blue nacre shell and the Mayans as the first real application of the bluetooth technology.
But if we move on and think throughout history how people have used different materials in the body, very often it's been physicians that have been quite creative. They've taken things off the shelf.
One of my favorite examples is that of Sir Harold Ridley, who was a famous ophthalmologist, or at least became a famous ophthalmologist. And during World War II, what he would see would be pilots coming back from their missions, and he noticed that within their eyes they had shards of small bits of material lodged within the eye, but the very interesting thing about it was that material, actually, wasn't causing any inflammatory response. So he looked into this, and he figured out that actually that material was little shards of plastic that were coming from the canopy of the Spitfires. And this led him to propose that material as a new material for intraocular lenses. It's called PMMA, and it's now used in millions of people every year and helps in preventing cataracts.
And that example, I think, is a really nice one, because it helps remind us that in the early days, people often chose materials because they were bioinert. Their very purpose was to perform a mechanical function. You'd put them in the body and you wouldn't get an adverse response. And what I want to show you is that in regenerative medicine, we've really shifted away from that idea of taking a bioinert material. We're actually actively looking for materials that will be bioactive, that will interact with the body, and that furthermore we can put in the body, they'll have their function, and then they'll dissolve away over time.
If we look at this schematic, this is showing you what we think of as the typical tissue-engineering approach. We have cells there, typically from the patient. We can put those onto a material, and we can make that material very complex if we want to, and we can then grow that up in the lab or we can put it straight back into the patient. And this is an approach that's used all over the world, including in our lab.
But one of the things that's really important when we're thinking about stem cells is that obviously stem cells can be many different things, and they want to be many different things, and so we want to make sure that the environment we put them into has enough information so that they can become the right sort of specialist tissue. And if we think about the different types of tissues that people are looking at regenerating all over the world, in all the different labs in the world, there's pretty much every tissue you can think of. And actually, the structure of those tissues is quite different, and it's going to really depend on whether your patient has any underlying disease, other conditions, in terms of how you're going to regenerate your tissue, and you're going to need to think about the materials you're going to use really carefully, their biochemistry, their mechanics, and many other properties as well.
Our tissues all have very different abilities to regenerate, and here we see poor Prometheus, who made a rather tricky career choice and was punished by the Greek gods. He was tied to a rock, and an eagle would come every day to eat his liver. But of course his liver would regenerate every day, and so day after day he was punished for eternity by the gods. And liver will regenerate in this very nice way, but actually if we think of other tissues, like cartilage, for example, even the simplest nick and you're going to find it really difficult to regenerate your cartilage. So it's going to be very different from tissue to tissue.
Now, bone is somewhere in between, and this is one of the tissues that we work on a lot in our lab. And bone is actually quite good at repairing. It has to be. We've probably all had fractures at some point or other. And one of the ways that you can think about repairing your fracture is this procedure here, called an iliac crest harvest. And what the surgeon might do is take some bone from your iliac crest, which is just here, and then transplant that somewhere else in the body. And it actually works really well, because it's your own bone, and it's well vascularized, which means it's got a really good blood supply. But the problem is, there's only so much you can take, and also when you do that operation, your patients might actually have significant pain in that defect site even two years after the operation.
So what we were thinking is, there's a tremendous need for bone repair, of course, but this iliac crest-type approach really has a lot of limitations to it, and could we perhaps recreate the generation of bone within the body on demand and then be able to transplant it without these very, very painful aftereffects that you would have with the iliac crest harvest?
And so this is what we did, and the way we did it was by coming back to this typical tissue-engineering approach but actually thinking about it rather differently. And we simplified it a lot, so we got rid of a lot of these steps. We got rid of the need to harvest cells from the patient, we got rid of the need to put in really fancy chemistries, and we got rid of the need to culture these scaffolds in the lab. And what we really focused on was our material system and making it quite simple, but because we used it in a really clever way, we were able to generate enormous amounts of bone using this approach. So we were using the body as really the catalyst to help us to make lots of new bone. And it's an approach that we call the in vivo bioreactor, and we were able to make enormous amounts of bone using this approach. And I'll talk you through this.
So what we do is, in humans, we all have a layer of stem cells on the outside of our long bones. That layer is called the periosteum. And that layer is actually normally very, very tightly bound to the underlying bone, and it's got stem cells in it. Those stem cells are really important in the embryo when it develops, and they also sort of wake up if you have a fracture to help you with repairing the bone. So we take that periosteum layer and we developed a way to inject underneath it a liquid that then, within 30 seconds, would turn into quite a rigid gel and can actually lift the periosteum away from the bone. So it creates, in essence, an artificial cavity that is right next to both the bone but also this really rich layer of stem cells. And we go in through a pinhole incision so that no other cells from the body can get in, and what happens is that that artificial in vivo bioreactor cavity can then lead to the proliferation of these stem cells, and they can form lots of new tissue, and then over time, you can harvest that tissue and use it elsewhere in the body.
This is a histology slide of what we see when we do that, and essentially what we see is very large amounts of bone. So in this picture, you can see the middle of the leg, so the bone marrow, then you can see the original bone, and you can see where that original bone finishes, and just to the left of that is the new bone that's grown within that bioreactor cavity, and you can actually make it even larger. And that demarcation that you can see between the original bone and the new bone acts as a very slight point of weakness, so actually now the surgeon can come along, can harvest away that new bone, and the periosteum can grow back, so you're left with the leg in the same sort of state as if you hadn't operated on it in the first place. So it's very, very low in terms of after-pain compared to an iliac crest harvest. And you can grow different amounts of bone depending on how much gel you put in there, so it really is an on demand sort of procedure.
Now, at the time that we did this, this received a lot of attention in the press, because it was a really nice way of generating new bone, and we got many, many contacts from different people that were interested in using this. And I'm just going to tell you, sometimes those contacts are very strange, slightly unexpected, and the very most interesting, let me put it that way, contact that I had, was actually from a team of American footballers that all wanted to have double-thickness skulls made on their head.
And so you do get these kinds of contacts, and of course, being British and also growing up in France, I tend to be very blunt, and so I had to explain to them very nicely that in their particular case, there probably wasn't that much in there to protect in the first place.
So this was our approach, and it was simple materials, but we thought about it carefully. And actually we know that those cells in the body, in the embryo, as they develop can form a different kind of tissue, cartilage, and so we developed a gel that was slightly different in nature and slightly different chemistry, put it in there, and we were able to get 100 percent cartilage instead.
And this approach works really well, I think, for pre-planned procedures, but it's something you do have to pre-plan. So for other kinds of operations, there's definitely a need for other scaffold-based approaches. And when you think about designing those other scaffolds, actually, you need a really multi-disciplinary team. And so our team has chemists, it has cell biologists, surgeons, physicists even, and those people all come together and we think really hard about designing the materials.
But we want to make them have enough information that we can get the cells to do what we want, but not be so complex as to make it difficult to get to clinic. And so one of the things we think about a lot is really trying to understand the structure of the tissues in the body. And so if we think of bone, obviously my own favorite tissue, we zoom in, we can see, even if you don't know anything about bone structure, it's beautifully organized, really beautifully organized. We've lots of blood vessels in there. And if we zoom in again, we see that the cells are actually surrounded by a 3D matrix of nano-scale fibers, and they give a lot of information to the cells. And if we zoom in again, actually in the case of bone, the matrix around the cells is beautifully organized at the nano scale, and it's a hybrid material that's part organic, part inorganic. And that's led to a whole field, really, that has looked at developing materials that have this hybrid kind of structure. And so I'm showing here just two examples where we've made some materials that have that sort of structure, and you can really tailor it. You can see here a very squishy one and now a material that's also this hybrid sort of material but actually has remarkable toughness, and it's no longer brittle. And an inorganic material would normally be really brittle, and you wouldn't be able to have that sort of strength and toughness in it.
One other thing I want to quickly mention is that many of the scaffolds we make are porous, and they have to be, because you want blood vessels to grow in there. But the pores are actually oftentimes much bigger than the cells, and so even though it's 3D, the cell might see it more as a slightly curved surface, and that's a little bit unnatural. And so one of the things you can think about doing is actually making scaffolds with slightly different dimensions that might be able to surround your cells in 3D and give them a little bit more information. And there's a lot of work going on in both of these areas.
Now finally, I just want to talk a little bit about applying this sort of thing to cardiovascular disease, because this is a really big clinical problem. And one of the things that we know is that, unfortunately, if you have a heart attack, then that tissue can start to die, and your outcome may not be very good over time. And it would be really great, actually, if we could stop that dead tissue either from dying or help it to regenerate. And there's lots and lots of stem cell trials going on worldwide, and they use many different types of cells, but one common theme that seems to be coming out is that actually, very often, those cells will die once you've implanted them. And you can either put them into the heart or into the blood system, but either way, we don't seem to be able to get quite the right number of cells getting to the location we want them to and being able to deliver the sort of beautiful cell regeneration that we would like to have to get good clinical outcomes.
And so some of the things that we're thinking of, and many other people in the field are thinking of, are actually developing materials for that. But there's a difference here. We still need chemistry, we still need mechanics, we still need really interesting topography, and we still need really interesting ways to surround the cells. But now, the cells also would probably quite like a material that's going to be able to be conductive, because the cells themselves will respond very well and will actually conduct signals between themselves. You can see them now beating synchronously on these materials, and that's a very, very exciting development that's going on.
So just to wrap up, I'd like to actually say that being able to work in this sort of field, all of us that work in this field that's not only super-exciting science, but also has the potential to impact on patients, however big or small they are, is really a great privilege. And so for that, I'd like to thank all of you as well. 
George and Charlotte Blonsky, who were a married couple living in the Bronx in New York City, invented something. They got a patent in 1965 for what they call, "a device to assist women in giving birth." This device consists of a large, round table and some machinery. When the woman is ready to deliver her child, she lies on her back, she is strapped down to the table, and the table is rotated at high speed. The child comes flying out through centrifugal force. If you look at their patent carefully, especially if you have any engineering background or talent, you may decide that you see one or two points where the design is not perfectly adequate. (Laughter)
Doctor Ivan Schwab in California is one of the people, one of the main people, who helped answer the question, "Why don't woodpeckers get headaches?" And it turns out the answer to that is because their brains are packaged inside their skulls in a way different from the way our brains, we being human beings, true, have our brains packaged. They, the woodpeckers, typically will peck, they will bang their head on a piece of wood thousands of times every day. Every day! And as far as anyone knows, that doesn't bother them in the slightest.
How does this happen? Their brain does not slosh around like ours does. Their brain is packed in very tightly, at least for blows coming right from the front. Not too many people paid attention to this research until the last few years when, in this country especially, people are becoming curious about what happens to the brains of football players who bang their heads repeatedly. And the woodpecker maybe relates to that.
There was a paper published in the medical journal The Lancet in England a few years ago called " A man who pricked his finger and smelled putrid for 5 years." Dr. Caroline Mills and her team received this patient and didn't really know what to do about it. The man had cut his finger, he worked processing chickens, and then he started to smell really, really bad. So bad that when he got in a room with the doctors and the nurses, they couldn't stand being in the room with him. It was intolerable. They tried every drug, every other treatment they could think of. After a year, he still smelled putrid. After two years, still smelled putrid. Three years, four years, still smelled putrid. After five years, it went away on its own. It's a mystery.
In New Zealand, Dr. Lianne Parkin and her team tested an old tradition in her city. They live in a city that has huge hills, San Francisco-grade hills. And in the winter there, it gets very cold and very icy. There are lots of injuries. The tradition that they tested, they tested by asking people who were on their way to work in the morning, to stop and try something out. Try one of two conditions. The tradition is that in the winter, in that city, you wear your socks on the outside of your boots. And what they discovered by experiment, and it was quite graphic when they saw it, was that it's true. That if you wear your socks on the outside rather than the inside, you're much more likely to survive and not slip and fall.
Now, I hope you will agree with me that these things I've just described to you, each of them, deserves some kind of prize. (Laughter) And that's what they got, each of them got an Ig Nobel prize. In 1991, I, together with bunch of other people, started the Ig Nobel prize ceremony. Every year we give out 10 prizes. The prizes are based on just one criteria. It's very simple. It's that you've done something that makes people laugh and then think. What you've done makes people laugh and then think. Whatever it is, there's something about it that when people encounter it at first, their only possible reaction is to laugh. And then a week later, it's still rattling around in their heads and all they want to do is tell their friends about it. That's the quality we look for.
Every year, we get in the neighborhood of 9,000 new nominations for the Ig Nobel prize. Of those, consistently between 10 percent and 20 percent of those nominations are people who nominate themselves. Those self-nominees almost never win. It's very difficult, numerically, to win a prize if you want to. Even if you don't want to, it's very difficult numerically. You should know that when we choose somebody to win an Ig Nobel prize, We get in touch with that person, very quietly. We offer them the chance to decline this great honor if they want to. Happily for us, almost everyone who's offered a prize decides to accept.
What do you get if you win an Ig Nobel prize? Well, you get several things. You get an Ig Nobel prize. The design is different every year. These are always handmade from extremely cheap materials. You're looking at a picture of the prize we gave last year, 2013. Most prizes in the world also give their winners some cash, some money. We don't have any money, so we can't give them. In fact, the winners have to pay their own way to come to the Ig Nobel ceremony, which most of them do. Last year, though, we did manage to scrape up some money. Last year, each of the 10 Ig Nobel prize winners received from us 10 trillion dollars. A $10 trillion bill from Zimbabwe. (Laughter) You may remember that Zimbabwe had a little adventure for a few years there of inflation. They ended up printing bills that were in denominations as large as 100 trillion dollars. The man responsible, who runs the national bank there, by the way, won an Ig Nobel prize in mathematics.
The other thing you win is an invitation to come to the ceremony, which happens at Harvard University. And when you get there, you come to Harvard's biggest meeting place and classroom. It fits 1,100 people, it's jammed to the gills, and up on the stage, waiting to shake your hand, waiting to hand you your Ig Nobel prize, are a bunch of Nobel prize winners. That's the heart of the ceremony. The winners are kept secret until that moment, even the Nobel laureates who will shake their hand don't know who they are until they're announced.
I am going to tell you about just a very few of the other medical-related prizes we've given. Keep in mind, we've given 230 prizes. There are lots of these people who walk among you. Maybe you have one. A paper was published about 30 years ago called "Injuries due to Falling Coconuts." It was written by Dr. Peter Barss, who is Canadian. Dr. Barss came to the ceremony and explained that as a young doctor, he wanted to see the world. So he went to Papua New Guinea. When he got there, he went to work in a hospital, and he was curious what kinds of things happen to people that bring them to the hospital. He looked through the records, and he discovered that a surprisingly large number of people in that hospital were there because of injuries due to falling coconuts. One typical thing that happens is people will come from the highlands, where there are not many coconut trees, down to visit their relatives on the coast, where there are lots. And they'll think that a coconut tree is a fine place to stand and maybe lie down. A coconut tree that is 90 feet tall, and has coconuts that weigh two pounds that can drop off at any time.
A team of doctors in Europe published a series of papers about colonoscopies. You're all familiar with colonoscopies, one way or another. Or in some cases, one way and another. They, in these papers, explained to their fellow doctors who perform colonoscopies, how to minimize the chance that when you perform a colonoscopy, your patient will explode. (Laughter) Dr. Emmanuel Ben-Soussan one of the authors, flew in from Paris to the ceremony, where he explained the history of this, that in the 1950s, when colonoscopies were becoming a common technique for the first time, people were figuring out how to do it well. And there were some difficulties at first. The basic problem, I'm sure you're familiar with, that you're looking inside a long, narrow, dark place. And so, you want to have a larger space. You add some gas to inflate it so you have room to look around. Now, that's added to the gas, the methane gas, that's already inside. The gas that they used at first, in many cases, was oxygen. So they added oxygen to methane gas. And then they wanted to be able to see, they needed light, so they'd put in a light source, which in the 1950s was very hot. So you had methane gas, which is flammable, oxygen and heat. They stopped using oxygen pretty quickly. (Laughter) Now it's rare that patients will explode, but it does still happen.
The final thing that I want to tell you about is a prize we gave to Dr. Elena Bodnar. Dr. Elena Bodnar invented a brassiere that in an emergency can be quickly separated into a pair of protective face masks. One to save your life, one to save the life of some lucky bystander. (Laughter) Why would someone do this, you might wonder. Dr. Bodnar came to the ceremony and she explained that she grew up in Ukraine. She was one of the doctors who treated victims of the Chernobyl power plant meltdown. And they later discovered that a lot of the worst medical problems came from the particles people breathed in. So she was always thinking after that about could there be some simple mask that was available everywhere when the unexpected happens. Years later, she moved to America. She had a baby, One day she looked, and on the floor, her infant son had picked up her bra, and had her bra on his face. And that's where the idea came from. She came to the Ig Nobel ceremony with the first prototype of the bra and she demonstrated: (Laughter) (Applause) ["Paul Krugman, Nobel laureate (2008) in economics"] ["Wolfgang Ketterle, Nobel laureate (2001) in physics"]
I myself own an emergency bra. (Laughter) It's my favorite bra, but I would be happy to share it with any of you, should the need arise. Thank you.
I work with children with autism. Specifically, I make technologies to help them communicate.
Now, many of the problems that children with autism face, they have a common source, and that source is that they find it difficult to understand abstraction, symbolism. And because of this, they have a lot of difficulty with language.
Let me tell you a little bit about why this is. You see that this is a picture of a bowl of soup. All of us can see it. All of us understand this. These are two other pictures of soup, but you can see that these are more abstract These are not quite as concrete. And when you get to language, you see that it becomes a word whose look, the way it looks and the way it sounds, has absolutely nothing to do with what it started with, or what it represents, which is the bowl of soup. So it's essentially a completely abstract, a completely arbitrary representation of something which is in the real world, and this is something that children with autism have an incredible amount of difficulty with. Now that's why most of the people that work with children with autism -- speech therapists, educators -- what they do is, they try to help children with autism communicate not with words, but with pictures. So if a child with autism wanted to say, "I want soup," that child would pick three different pictures, "I," "want," and "soup," and they would put these together, and then the therapist or the parent would understand that this is what the kid wants to say. And this has been incredibly effective; for the last 30, 40 years people have been doing this. In fact, a few years back, I developed an app for the iPad which does exactly this. It's called Avaz, and the way it works is that kids select different pictures. These pictures are sequenced together to form sentences, and these sentences are spoken out. So Avaz is essentially converting pictures, it's a translator, it converts pictures into speech.
Now, this was very effective. There are thousands of children using this, you know, all over the world, and I started thinking about what it does and what it doesn't do. And I realized something interesting: Avaz helps children with autism learn words. What it doesn't help them do is to learn word patterns. Let me explain this in a little more detail. Take this sentence: "I want soup tonight." Now it's not just the words here that convey the meaning. It's also the way in which these words are arranged, the way these words are modified and arranged. And that's why a sentence like "I want soup tonight" is different from a sentence like "Soup want I tonight," which is completely meaningless. So there is another hidden abstraction here which children with autism find a lot of difficulty coping with, and that's the fact that you can modify words and you can arrange them to have different meanings, to convey different ideas. Now, this is what we call grammar. And grammar is incredibly powerful, because grammar is this one component of language which takes this finite vocabulary that all of us have and allows us to convey an infinite amount of information, an infinite amount of ideas. It's the way in which you can put things together in order to convey anything you want to.
And so after I developed Avaz, I worried for a very long time about how I could give grammar to children with autism. The solution came to me from a very interesting perspective. I happened to chance upon a child with autism conversing with her mom, and this is what happened. Completely out of the blue, very spontaneously, the child got up and said, "Eat." Now what was interesting was the way in which the mom was trying to tease out the meaning of what the child wanted to say by talking to her in questions. So she asked, "Eat what? Do you want to eat ice cream? You want to eat? Somebody else wants to eat? You want to eat cream now? You want to eat ice cream in the evening?" And then it struck me that what the mother had done was something incredible. She had been able to get that child to communicate an idea to her without grammar. And it struck me that maybe this is what I was looking for. Instead of arranging words in an order, in sequence, as a sentence, you arrange them in this map, where they're all linked together not by placing them one after the other but in questions, in question-answer pairs. And so if you do this, then what you're conveying is not a sentence in English, but what you're conveying is really a meaning, the meaning of a sentence in English. Now, meaning is really the underbelly, in some sense, of language. It's what comes after thought but before language. And the idea was that this particular representation might convey meaning in its raw form.
So I was very excited by this, you know, hopping around all over the place, trying to figure out if I can convert all possible sentences that I hear into this. And I found that this is not enough. Why is this not enough? This is not enough because if you wanted to convey something like negation, you want to say, "I don't want soup," then you can't do that by asking a question. You do that by changing the word "want." Again, if you wanted to say, "I wanted soup yesterday," you do that by converting the word "want" into "wanted." It's a past tense. So this is a flourish which I added to make the system complete. This is a map of words joined together as questions and answers, and with these filters applied on top of them in order to modify them to represent certain nuances. Let me show you this with a different example.
Let's take this sentence: "I told the carpenter I could not pay him." It's a fairly complicated sentence. The way that this particular system works, you can start with any part of this sentence. I'm going to start with the word "tell." So this is the word "tell." Now this happened in the past, so I'm going to make that "told." Now, what I'm going to do is, I'm going to ask questions. So, who told? I told. I told whom? I told the carpenter. Now we start with a different part of the sentence. We start with the word "pay," and we add the ability filter to it to make it "can pay." Then we make it "can't pay," and we can make it "couldn't pay" by making it the past tense. So who couldn't pay? I couldn't pay. Couldn't pay whom? I couldn't pay the carpenter. And then you join these two together by asking this question: What did I tell the carpenter? I told the carpenter I could not pay him.
Now think about this. This is —(Applause)— this is a representation of this sentence without language. And there are two or three interesting things about this. First of all, I could have started anywhere. I didn't have to start with the word "tell." I could have started anywhere in the sentence, and I could have made this entire thing. The second thing is, if I wasn't an English speaker, if I was speaking in some other language, this map would actually hold true in any language. So long as the questions are standardized, the map is actually independent of language. So I call this FreeSpeech, and I was playing with this for many, many months. I was trying out so many different combinations of this.
And then I noticed something very interesting about FreeSpeech. I was trying to convert language, convert sentences in English into sentences in FreeSpeech, and vice versa, and back and forth. And I realized that this particular configuration, this particular way of representing language, it allowed me to actually create very concise rules that go between FreeSpeech on one side and English on the other. So I could actually write this set of rules that translates from this particular representation into English. And so I developed this thing. I developed this thing called the FreeSpeech Engine which takes any FreeSpeech sentence as the input and gives out perfectly grammatical English text. And by putting these two pieces together, the representation and the engine, I was able to create an app, a technology for children with autism, that not only gives them words but also gives them grammar.
So I tried this out with kids with autism, and I found that there was an incredible amount of identification. They were able to create sentences in FreeSpeech which were much more complicated but much more effective than equivalent sentences in English, and I started thinking about why that might be the case. And I had an idea, and I want to talk to you about this idea next. In about 1997, about 15 years back, there were a group of scientists that were trying to understand how the brain processes language, and they found something very interesting. They found that when you learn a language as a child, as a two-year-old, you learn it with a certain part of your brain, and when you learn a language as an adult -- for example, if I wanted to learn Japanese right now — a completely different part of my brain is used. Now I don't know why that's the case, but my guess is that that's because when you learn a language as an adult, you almost invariably learn it through your native language, or through your first language. So what's interesting about FreeSpeech is that when you create a sentence or when you create language, a child with autism creates language with FreeSpeech, they're not using this support language, they're not using this bridge language. They're directly constructing the sentence.
And so this gave me this idea. Is it possible to use FreeSpeech not for children with autism but to teach language to people without disabilities? And so I tried a number of experiments. The first thing I did was I built a jigsaw puzzle in which these questions and answers are coded in the form of shapes, in the form of colors, and you have people putting these together and trying to understand how this works. And I built an app out of it, a game out of it, in which children can play with words and with a reinforcement, a sound reinforcement of visual structures, they're able to learn language. And this, this has a lot of potential, a lot of promise, and the government of India recently licensed this technology from us, and they're going to try it out with millions of different children trying to teach them English. And the dream, the hope, the vision, really, is that when they learn English this way, they learn it with the same proficiency as their mother tongue.
All right, let's talk about something else. Let's talk about speech. This is speech. So speech is the primary mode of communication delivered between all of us. Now what's interesting about speech is that speech is one-dimensional. Why is it one-dimensional? It's one-dimensional because it's sound. It's also one-dimensional because our mouths are built that way. Our mouths are built to create one-dimensional sound. But if you think about the brain, the thoughts that we have in our heads are not one-dimensional. I mean, we have these rich, complicated, multi-dimensional ideas. Now, it seems to me that language is really the brain's invention to convert this rich, multi-dimensional thought on one hand into speech on the other hand. Now what's interesting is that we do a lot of work in information nowadays, and almost all of that is done in the language domain. Take Google, for example. Google trawls all these countless billions of websites, all of which are in English, and when you want to use Google, you go into Google search, and you type in English, and it matches the English with the English. What if we could do this in FreeSpeech instead? I have a suspicion that if we did this, we'd find that algorithms like searching, like retrieval, all of these things, are much simpler and also more effective, because they don't process the data structure of speech. Instead they're processing the data structure of thought. The data structure of thought. That's a provocative idea.
But let's look at this in a little more detail. So this is the FreeSpeech ecosystem. We have the Free Speech representation on one side, and we have the FreeSpeech Engine, which generates English. Now if you think about it, FreeSpeech, I told you, is completely language-independent. It doesn't have any specific information in it which is about English. So everything that this system knows about English is actually encoded into the engine. That's a pretty interesting concept in itself. You've encoded an entire human language into a software program. But if you look at what's inside the engine, it's actually not very complicated. It's not very complicated code. And what's more interesting is the fact that the vast majority of the code in that engine is not really English-specific. And that gives this interesting idea. It might be very easy for us to actually create these engines in many, many different languages, in Hindi, in French, in German, in Swahili. And that gives another interesting idea. For example, supposing I was a writer, say, for a newspaper or for a magazine. I could create content in one language, FreeSpeech, and the person who's consuming that content, the person who's reading that particular information could choose any engine, and they could read it in their own mother tongue, in their native language. I mean, this is an incredibly attractive idea, especially for India. We have so many different languages. There's a song about India, and there's a description of the country as, it says, (in Sanskrit). That means "ever-smiling speaker of beautiful languages."
Language is beautiful. I think it's the most beautiful of human creations. I think it's the loveliest thing that our brains have invented. It entertains, it educates, it enlightens, but what I like the most about language is that it empowers.
I want to leave you with this. This is a photograph of my collaborators, my earliest collaborators when I started working on language and autism and various other things. The girl's name is Pavna, and that's her mother, Kalpana. And Pavna's an entrepreneur, but her story is much more remarkable than mine, because Pavna is about 23. She has quadriplegic cerebral palsy, so ever since she was born, she could neither move nor talk. And everything that she's accomplished so far, finishing school, going to college, starting a company, collaborating with me to develop Avaz, all of these things she's done with nothing more than moving her eyes.
Daniel Webster said this: He said, "If all of my possessions were taken from me with one exception, I would choose to keep the power of communication, for with it, I would regain all the rest." And that's why, of all of these incredible applications of FreeSpeech, the one that's closest to my heart still remains the ability for this to empower children with disabilities to be able to communicate, the power of communication, to get back all the rest.
Thank you. (Applause) Thank you. (Applause) Thank you. Thank you. Thank you. (Applause) Thank you. Thank you. Thank you. 
America's favorite pie is?
Audience: Apple. Kenneth Cukier: Apple. Of course it is. How do we know it? Because of data. You look at supermarket sales. You look at supermarket sales of 30-centimeter pies that are frozen, and apple wins, no contest. The majority of the sales are apple. But then supermarkets started selling smaller, 11-centimeter pies, and suddenly, apple fell to fourth or fifth place. Why? What happened? Okay, think about it. When you buy a 30-centimeter pie, the whole family has to agree, and apple is everyone's second favorite. (Laughter) But when you buy an individual 11-centimeter pie, you can buy the one that you want. You can get your first choice. You have more data. You can see something that you couldn't see when you only had smaller amounts of it.
Now, the point here is that more data doesn't just let us see more, more of the same thing we were looking at. More data allows us to see new. It allows us to see better. It allows us to see different. In this case, it allows us to see what America's favorite pie is: not apple.
Now, you probably all have heard the term big data. In fact, you're probably sick of hearing the term big data. It is true that there is a lot of hype around the term, and that is very unfortunate, because big data is an extremely important tool by which society is going to advance. In the past, we used to look at small data and think about what it would mean to try to understand the world, and now we have a lot more of it, more than we ever could before. What we find is that when we have a large body of data, we can fundamentally do things that we couldn't do when we only had smaller amounts. Big data is important, and big data is new, and when you think about it, the only way this planet is going to deal with its global challenges — to feed people, supply them with medical care, supply them with energy, electricity, and to make sure they're not burnt to a crisp because of global warming — is because of the effective use of data.
So what is new about big data? What is the big deal? Well, to answer that question, let's think about what information looked like, physically looked like in the past. In 1908, on the island of Crete, archaeologists discovered a clay disc. They dated it from 2000 B.C., so it's 4,000 years old. Now, there's inscriptions on this disc, but we actually don't know what it means. It's a complete mystery, but the point is that this is what information used to look like 4,000 years ago. This is how society stored and transmitted information.
Now, society hasn't advanced all that much. We still store information on discs, but now we can store a lot more information, more than ever before. Searching it is easier. Copying it easier. Sharing it is easier. Processing it is easier. And what we can do is we can reuse this information for uses that we never even imagined when we first collected the data. In this respect, the data has gone from a stock to a flow, from something that is stationary and static to something that is fluid and dynamic. There is, if you will, a liquidity to information. The disc that was discovered off of Crete that's 4,000 years old, is heavy, it doesn't store a lot of information, and that information is unchangeable. By contrast, all of the files that Edward Snowden took from the National Security Agency in the United States fits on a memory stick the size of a fingernail, and it can be shared at the speed of light. More data. More.
Now, one reason why we have so much data in the world today is we are collecting things that we've always collected information on, but another reason why is we're taking things that have always been informational but have never been rendered into a data format and we are putting it into data. Think, for example, the question of location. Take, for example, Martin Luther. If we wanted to know in the 1500s where Martin Luther was, we would have to follow him at all times, maybe with a feathery quill and an inkwell, and record it, but now think about what it looks like today. You know that somewhere, probably in a telecommunications carrier's database, there is a spreadsheet or at least a database entry that records your information of where you've been at all times. If you have a cell phone, and that cell phone has GPS, but even if it doesn't have GPS, it can record your information. In this respect, location has been datafied.
Now think, for example, of the issue of posture, the way that you are all sitting right now, the way that you sit, the way that you sit, the way that you sit. It's all different, and it's a function of your leg length and your back and the contours of your back, and if I were to put sensors, maybe 100 sensors into all of your chairs right now, I could create an index that's fairly unique to you, sort of like a fingerprint, but it's not your finger.
So what could we do with this? Researchers in Tokyo are using it as a potential anti-theft device in cars. The idea is that the carjacker sits behind the wheel, tries to stream off, but the car recognizes that a non-approved driver is behind the wheel, and maybe the engine just stops, unless you type in a password into the dashboard to say, "Hey, I have authorization to drive." Great.
What if every single car in Europe had this technology in it? What could we do then? Maybe, if we aggregated the data, maybe we could identify telltale signs that best predict that a car accident is going to take place in the next five seconds. And then what we will have datafied is driver fatigue, and the service would be when the car senses that the person slumps into that position, automatically knows, hey, set an internal alarm that would vibrate the steering wheel, honk inside to say, "Hey, wake up, pay more attention to the road." These are the sorts of things we can do when we datafy more aspects of our lives.
So what is the value of big data? Well, think about it. You have more information. You can do things that you couldn't do before. One of the most impressive areas where this concept is taking place is in the area of machine learning. Machine learning is a branch of artificial intelligence, which itself is a branch of computer science. The general idea is that instead of instructing a computer what do do, we are going to simply throw data at the problem and tell the computer to figure it out for itself. And it will help you understand it by seeing its origins. In the 1950s, a computer scientist at IBM named Arthur Samuel liked to play checkers, so he wrote a computer program so he could play against the computer. He played. He won. He played. He won. He played. He won, because the computer only knew what a legal move was. Arthur Samuel knew something else. Arthur Samuel knew strategy. So he wrote a small sub-program alongside it operating in the background, and all it did was score the probability that a given board configuration would likely lead to a winning board versus a losing board after every move. He plays the computer. He wins. He plays the computer. He wins. He plays the computer. He wins. And then Arthur Samuel leaves the computer to play itself. It plays itself. It collects more data. It collects more data. It increases the accuracy of its prediction. And then Arthur Samuel goes back to the computer and he plays it, and he loses, and he plays it, and he loses, and he plays it, and he loses, and Arthur Samuel has created a machine that surpasses his ability in a task that he taught it.
And this idea of machine learning is going everywhere. How do you think we have self-driving cars? Are we any better off as a society enshrining all the rules of the road into software? No. Memory is cheaper. No. Algorithms are faster. No. Processors are better. No. All of those things matter, but that's not why. It's because we changed the nature of the problem. We changed the nature of the problem from one in which we tried to overtly and explicitly explain to the computer how to drive to one in which we say, "Here's a lot of data around the vehicle. You figure it out. You figure it out that that is a traffic light, that that traffic light is red and not green, that that means that you need to stop and not go forward."
Machine learning is at the basis of many of the things that we do online: search engines, Amazon's personalization algorithm, computer translation, voice recognition systems. Researchers recently have looked at the question of biopsies, cancerous biopsies, and they've asked the computer to identify by looking at the data and survival rates to determine whether cells are actually cancerous or not, and sure enough, when you throw the data at it, through a machine-learning algorithm, the machine was able to identify the 12 telltale signs that best predict that this biopsy of the breast cancer cells are indeed cancerous. The problem: The medical literature only knew nine of them. Three of the traits were ones that people didn't need to look for, but that the machine spotted.
Now, there are dark sides to big data as well. It will improve our lives, but there are problems that we need to be conscious of, and the first one is the idea that we may be punished for predictions, that the police may use big data for their purposes, a little bit like "Minority Report." Now, it's a term called predictive policing, or algorithmic criminology, and the idea is that if we take a lot of data, for example where past crimes have been, we know where to send the patrols. That makes sense, but the problem, of course, is that it's not simply going to stop on location data, it's going to go down to the level of the individual. Why don't we use data about the person's high school transcript? Maybe we should use the fact that they're unemployed or not, their credit score, their web-surfing behavior, whether they're up late at night. Their Fitbit, when it's able to identify biochemistries, will show that they have aggressive thoughts. We may have algorithms that are likely to predict what we are about to do, and we may be held accountable before we've actually acted. Privacy was the central challenge in a small data era. In the big data age, the challenge will be safeguarding free will, moral choice, human volition, human agency.
There is another problem: Big data is going to steal our jobs. Big data and algorithms are going to challenge white collar, professional knowledge work in the 21st century in the same way that factory automation and the assembly line challenged blue collar labor in the 20th century. Think about a lab technician who is looking through a microscope at a cancer biopsy and determining whether it's cancerous or not. The person went to university. The person buys property. He or she votes. He or she is a stakeholder in society. And that person's job, as well as an entire fleet of professionals like that person, is going to find that their jobs are radically changed or actually completely eliminated. Now, we like to think that technology creates jobs over a period of time after a short, temporary period of dislocation, and that is true for the frame of reference with which we all live, the Industrial Revolution, because that's precisely what happened. But we forget something in that analysis: There are some categories of jobs that simply get eliminated and never come back. The Industrial Revolution wasn't very good if you were a horse. So we're going to need to be careful and take big data and adjust it for our needs, our very human needs. We have to be the master of this technology, not its servant. We are just at the outset of the big data era, and honestly, we are not very good at handling all the data that we can now collect. It's not just a problem for the National Security Agency. Businesses collect lots of data, and they misuse it too, and we need to get better at this, and this will take time. It's a little bit like the challenge that was faced by primitive man and fire. This is a tool, but this is a tool that, unless we're careful, will burn us.
Big data is going to transform how we live, how we work and how we think. It is going to help us manage our careers and lead lives of satisfaction and hope and happiness and health, but in the past, we've often looked at information technology and our eyes have only seen the T, the technology, the hardware, because that's what was physical. We now need to recast our gaze at the I, the information, which is less apparent, but in some ways a lot more important. Humanity can finally learn from the information that it can collect, as part of our timeless quest to understand the world and our place in it, and that's why big data is a big deal. 
I want to talk to you about one of the biggest myths in medicine, and that is the idea that all we need are more medical breakthroughs and then all of our problems will be solved. Our society loves to romanticize the idea of the single, solo inventor who, working late in the lab one night, makes an earthshaking discovery, and voila, overnight everything's changed. That's a very appealing picture, however, it's just not true. In fact, medicine today is a team sport. And in many ways, it always has been. I'd like to share with you a story about how I've experienced this very dramatically in my own work.
I'm a surgeon, and we surgeons have always had this special relationship with light. When I make an incision inside a patient's body, it's dark. We need to shine light to see what we're doing. And this is why, traditionally, surgeries have always started so early in the morning -- to take advantage of daylight hours. And if you look at historical pictures of the early operating rooms, they have been on top of buildings. For example, this is the oldest operating room in the Western world, in London, where the operating room is actually on top of a church with a skylight coming in. And then this is a picture of one of the most famous hospitals in America. This is Mass General in Boston. And do you know where the operating room is? Here it is on the top of the building with plenty of windows to let light in.
So nowadays in the operating room, we no longer need to use sunlight. And because we no longer need to use sunlight, we have very specialized lights that are made for the operating room. We have an opportunity to bring in other kinds of lights -- lights that can allow us to see what we currently don't see. And this is what I think is the magic of fluorescence.
So let me back up a little bit. When we are in medical school, we learn our anatomy from illustrations such as this where everything's color-coded. Nerves are yellow, arteries are red, veins are blue. That's so easy anybody could become a surgeon, right? However, when we have a real patient on the table, this is the same neck dissection -- not so easy to tell the difference between different structures. We heard over the last couple days what an urgent problem cancer still is in our society, what a pressing need it is for us to not have one person die every minute. Well if cancer can be caught early, enough such that someone can have their cancer taken out, excised with surgery, I don't care if it has this gene or that gene, or if it has this protein or that protein, it's in the jar. It's done, it's out, you're cured of cancer.
This is how we excise cancers. We do our best, based upon our training and the way the cancer looks and the way it feels and its relationship to other structures and all of our experience, we say, you know what, the cancer's gone. We've made a good job. We've taken it out. That's what the surgeon is saying in the operating room when the patient's on the table. But then we actually don't know that it's all out. We actually have to take samples from the surgical bed, what's left behind in the patient, and then send those bits to the pathology lab. In the meanwhile, the patient's on the operating room table. The nurses, anesthesiologist, the surgeon, all the assistants are waiting around. And we wait. The pathologist takes that sample, freezes it, cuts it, looks in the microscope one by one and then calls back into the room. And that may be 20 minutes later per piece. So if you've sent three specimens, it's an hour later. And very often they say, "You know what, points A and B are okay, but point C, you still have some residual cancer there. Please go cut that piece out." So we go back and we do that again, and again.
And this whole process: "Okay you're done. We think the entire tumor is out." But very often several days later, the patient's gone home, we get a phone call: "I'm sorry, once we looked at the final pathology, once we looked at the final specimen, we actually found that there's a couple other spots where the margins are positive. There's still cancer in your patient." So now you're faced with telling your patient, first of all, that they may need another surgery, or that they need additional therapy such as radiation or chemotherapy. So wouldn't it be better if we could really tell, if the surgeon could really tell, whether or not there's still cancer on the surgical field? I mean, in many ways, the way that we're doing it, we're still operating in the dark.
So in 2004, during my surgical residency, I had the great fortune to meet Dr. Roger Tsien, who went on to win the Nobel Prize for chemistry in 2008. Roger and his team were working on a way to detect cancer, and they had a very clever molecule that they had come up with. The molecule they had developed had three parts. The main part of it is the blue part, polycation, and it's basically very sticky to every tissue in your body.
So imagine that you make a solution full of this sticky material and inject it into the veins of someone who has cancer, everything's going to get lit up. Nothing will be specific. There's no specificity there. So they added two additional components. The first one is a polyanionic segment, which basically acts as a non-stick backing like the back of a sticker. So when those two are together, the molecule is neutral and nothing gets stuck down. And the two pieces are then linked by something that can only be cut if you have the right molecular scissors -- for example, the kind of protease enzymes that tumors make. So here in this situation, if you make a solution full of this three-part molecule along with the dye, which is shown in green, and you inject it into the vein of someone who has cancer, normal tissue can't cut it. The molecule passes through and gets excreted. However, in the presence of the tumor, now there are molecular scissors that can break this molecule apart right there at the cleavable site. And now, boom, the tumor labels itself and it gets fluorescent.
So here's an example of a nerve that has tumor surrounding it. Can you tell where the tumor is? I couldn't when I was working on this. But here it is. It's fluorescent. Now it's green. See, so every single one in the audience now can tell where the cancer is. We can tell in the operating room, in the field, at a molecular level, where is the cancer and what the surgeon needs to do and how much more work they need to do to cut that out. And the cool thing about fluorescence is that it's not only bright, it actually can shine through tissue. The light that the fluorescence emits can go through tissue. So even if the tumor is not right on the surface, you'll still be able to see it.
In this movie, you can see that the tumor is green. There's actually normal muscle on top of it. See that? And I'm peeling that muscle away. But even before I peel that muscle away, you saw that there was a tumor underneath. So that's the beauty of having a tumor that's labeled with fluorescent molecules. That you can, not only see the margins right there on a molecular level, but you can see it even if it's not right on the top -- even if it's beyond your field of view. And this works for metastatic lymph nodes also.
Sentinel lymph node dissection has really changed the way that we manage breast cancer, melanoma. Women used to get really debilitating surgeries to excise all of the axillary lymph nodes. But when sentinel lymph node came into our treatment protocol, the surgeon basically looks for the single node that is the first draining lymph node of the cancer. And then if that node has cancer, the woman would go on to get the axillary lymph node dissection. So what that means is if the lymph node did not have cancer, the woman would be saved from having unnecessary surgery.
But sentinel lymph node, the way that we do it today, is kind of like having a road map just to know where to go. So if you're driving on the freeway and you want to know where's the next gas station, you have a map to tell you that that gas station is down the road. It doesn't tell you whether or not the gas station has gas. You have to cut it out, bring it back home, cut it up, look inside and say, "Oh yes, it does have gas." So that takes more time. Patients are still on the operating room table. Anesthesiologists, surgeons are waiting around. That takes time.
So with our technology, we can tell right away. You see a lot of little, roundish bumps there. Some of these are swollen lymph nodes that look a little larger than others. Who amongst us hasn't had swollen lymph nodes with a cold? That doesn't mean that there's cancer inside. Well with our technology, the surgeon is able to tell immediately which nodes have cancer. I won't go into this very much, but our technology, besides being able to tag tumor and metastatic lymph nodes with fluorescence, we can also use the same smart three-part molecule to tag gadolinium onto the system so you can do this noninvasively. The patient has cancer, you want to know if the lymph nodes have cancer even before you go in. Well you can see this on an MRI.
So in surgery, it's important to know what to cut out. But equally important is to preserve things that are important for function. So it's very important to avoid inadvertent injury. And what I'm talking about are nerves. Nerves, if they are injured, can cause paralysis, can cause pain. In the setting of prostate cancer, up to 60 percent of men after prostate cancer surgery may have urinary incontinence and erectile disfunction. That's a lot of people to have a lot of problems -- and this is even in so-called nerve-sparing surgery, which means that the surgeon is aware of the problem, and they are trying to avoid the nerves.
But you know what, these little nerves are so small, in the context of prostate cancer, that they are actually never seen. They are traced just by their known anatomical path along vasculature. And they're known because somebody has decided to study them, which means that we're still learning about where they are. Crazy to think that we're having surgery, we're trying to excise cancer, we don't know where the cancer is. We're trying to preserve nerves; we can't see where they are.
So I said, wouldn't it be great if we could find a way to see nerves with fluorescence? And at first this didn't get a lot of support. People said, "We've been doing it this way for all these years. What's the problem? We haven't had that many complications." But I went ahead anyway. And Roger helped me. And he brought his whole team with him. So there's that teamwork thing again. And we eventually discovered molecules that were specifically labeling nerves. And when we made a solution of this, tagged with the fluorescence and injected in the body of a mouse, their nerves literally glowed. You can see where they are.
Here you're looking at a sciatic nerve of a mouse, and you can see that that big, fat portion you can see very easily. But in fact, at the tip of that where I'm dissecting now, there's actually very fine arborizations that can't really be seen. You see what looks like little Medusa heads coming out. We have been able to see nerves for facial expression, for facial movement, for breathing -- every single nerve -- nerves for urinary function around the prostate. We've been able to see every single nerve. When we put these two probes together ... So here's a tumor. Do you guys know where the margins of this tumor is? Now you do. What about the nerve that's going into this tumor? That white portion there is easy to see. But what about the part that goes into the tumor? Do you know where it's going? Now you do.
Basically, we've come up with a way to stain tissue and color-code the surgical field. This was a bit of a breakthrough. I think that it'll change the way that we do surgery. We published our results in the proceedings of the National Academy of Sciences and in Nature Biotechnology. We received commentary in Discover magazine, in The Economist. And we showed it to a lot of my surgical colleagues. They said, "Wow! I have patients who would benefit from this. I think that this will result in my surgeries with a better outcome and fewer complications."
What needs to happen now is further development of our technology along with development of the instrumentation that allows us to see this sort of fluorescence in the operating room. The eventual goal is that we'll get this into patients. However, we've discovered that there's actually no straightforward mechanism to develop a molecule for one-time use. Understandably, the majority of the medical industry is focused on multiple-use drugs, such as long-term daily medications. We are focused on making this technology better. We're focused on adding drugs, adding growth factors, killing nerves that are causing problems and not the surrounding tissue. We know that this can be done and we're committed to doing it.
I'd like to leave you with this final thought. Successful innovation is not a single breakthrough. It is not a sprint. It is not an event for the solo runner. Successful innovation is a team sport, it's a relay race. It requires one team for the breakthrough and another team to get the breakthrough accepted and adopted. And this takes the long-term steady courage of the day-in day-out struggle to educate, to persuade and to win acceptance. And that is the light that I want to shine on health and medicine today.
It's a pleasure to be here in Edinburgh, Scotland, the birthplace of the needle and syringe. Less than a mile from here in this direction, in 1853 a Scotsman filed his very first patent on the needle and syringe. His name was Alexander Wood, and it was at the Royal College of Physicians. This is the patent. What blows my mind when I look at it even today is that it looks almost identical to the needle in use today. Yet, it's 160 years old.
So we turn to the field of vaccines. Most vaccines are delivered with the needle and syringe, this 160-year-old technology. And credit where it's due -- on many levels, vaccines are a successful technology. After clean water and sanitation, vaccines are the one technology that has increased our life span the most. That's a pretty hard act to beat.
But just like any other technology, vaccines have their shortcomings, and the needle and syringe is a key part within that narrative -- this old technology. So let's start with the obvious: Many of us don't like the needle and syringe. I share that view. However, 20 percent of the population have a thing called needle phobia. That's more than disliking the needle; that is actively avoiding being vaccinated because of needle phobia. And that's problematic in terms of the rollout of vaccines.
Now, related to this is another key issue, which is needlestick injuries. And the WHO has figures that suggest about 1.3 million deaths per year take place due to cross-contamination with needlestick injuries. These are early deaths that take place.
Now, these are two things that you probably may have heard of, but there are two other shortcomings of the needle and syringe you may not have heard about. One is it could be holding back the next generation of vaccines in terms of their immune responses. And the second is that it could be responsible for the problem of the cold chain that I'll tell you about as well.
I'm going to tell you about some work that my team and I are doing in Australia at the University of Queensland on a technology designed to tackle those four problems. And that technology is called the Nanopatch. Now, this is a specimen of the Nanopatch. To the naked eye it just looks like a square smaller than a postage stamp, but under a microscope what you see are thousands of tiny projections that are invisible to the human eye. And there's about 4,000 projections on this particular square compared to the needle. And I've designed those projections to serve a key role, which is to work with the skin's immune system. So that's a very important function tied in with the Nanopatch.
Now we make the Nanopatch with a technique called deep reactive ion etching. And this particular technique is one that's been borrowed from the semiconductor industry, and therefore is low cost and can be rolled out in large numbers.
Now we dry-coat vaccines to the projections of the Nanopatch and apply it to the skin. Now, the simplest form of application is using our finger, but our finger has some limitations, so we've devised an applicator. And it's a very simple device -- you could call it a sophisticated finger. It's a spring-operated device. What we do is when we apply the Nanopatch to the skin as so -- (Click) -- immediately a few things happen. So firstly, the projections on the Nanopatch breach through the tough outer layer and the vaccine is very quickly released -- within less than a minute, in fact. Then we can take the Nanopatch off and discard it. And indeed we can make a reuse of the applicator itself.
So that gives you an idea of the Nanopatch, and immediately you can see some key advantages. We've talked about it being needle-free -- these are projections that you can't even see -- and, of course, we get around the needle phobia issue as well.
Now, if we take a step back and think about these other two really important advantages: One is improved immune responses through delivery, and the second is getting rid of the cold chain.
So let's start with the first one, this immunogenicity idea. It takes a little while to get our heads around, but I'll try to explain it in simple terms. So I'll take a step back and explain to you how vaccines work in a simple way. So vaccines work by introducing into our body a thing called an antigen which is a safe form of a germ. Now that safe germ, that antigen, tricks our body into mounting an immune response, learning and remembering how to deal with intruders. When the real intruder comes along the body quickly mounts an immune response to deal with that vaccine and neutralizes the infection. So it does that well.
Now, the way it's done today with the needle and syringe, most vaccines are delivered that way -- with this old technology and the needle. But it could be argued that the needle is holding back our immune responses; it's missing our immune sweet spot in the skin. To describe this idea, we need to take a journey through the skin, starting with one of those projections and applying the Nanopatch to the skin. And we see this kind of data. Now, this is real data -- that thing that we can see there is one projection from the Nanopatch that's been applied to the skin and those colors are different layers. Now, to give you an idea of scale, if the needle was shown here, it would be too big. It would be 10 times bigger than the size of that screen, going 10 times deeper as well. It's off the grid entirely. You can see immediately that we have those projections in the skin. That red layer is a tough outer layer of dead skin, but the brown layer and the magenta layer are jammed full of immune cells. As one example, in the brown layer there's a certain type of cell called a Langerhans cell -- every square millimeter of our body is jammed full of those Langerhans cells, those immune cells, and there's others shown as well that we haven't stained in this image. But you can immediately see that the Nanopatch achieves that penetration indeed. We target thousands upon thousands of these particular cells just residing within a hair's width of the surface of the skin.
Now, as the guy that's invented this thing and designed it to do that, I found that exciting. But so what? So what if you've targeted cells? In the world of vaccines, what does that mean? The world of vaccines is getting better. It's getting more systematic. However, you still don't really know if a vaccine is going to work until you roll your sleeves up and vaccinate and wait. It's a gambler's game even today.
So, we had to do that gamble. We obtained an influenza vaccine, we applied it to our Nanopatches and we applied the Nanopatches to the skin, and we waited -- and this is in the live animal. We waited a month, and this is what we found out. This is a data slide showing the immune responses that we've generated with a Nanopatch compared to the needle and syringe into muscle. So on the horizontal axis we have the dose shown in nanograms. On the vertical axis we have the immune response generated, and that dashed line indicates the protection threshold. If we're above that line it's considered protective; if we're below that line it's not. So the red line is mostly below that curve and indeed there's only one point that is achieved with the needle that's protective, and that's with a high dose of 6,000 nanograms. But notice immediately the distinctly different curve that we achieve with the blue line. That's what's achieved with the Nanopatch; the delivered dose of the Nanopatch is a completely different immunogenicity curve. That's a real fresh opportunity. Suddenly we have a brand new lever in the world of vaccines. We can push it one way, where we can take a vaccine that works but is too expensive and can get protection with a hundredth of the dose compared to the needle. That can take a vaccine that's suddenly 10 dollars down to 10 cents, and that's particularly important within the developing world.
But there's another angle to this as well -- you can take vaccines that currently don't work and get them over that line and get them protective. And certainly in the world of vaccines that can be important. Let's consider the big three: HIV, malaria, tuberculosis. They're responsible for about 7 million deaths per year, and there is no adequate vaccination method for any of those. So potentially, with this new lever that we have with the Nanopatch, we can help make that happen. We can push that lever to help get those candidate vaccines over the line. Now, of course, we've worked within my lab with many other vaccines that have attained similar responses and similar curves to this, what we've achieved with influenza.
I'd like to now switch to talk about another key shortcoming of today's vaccines, and that is the need to maintain the cold chain. As the name suggests -- the cold chain -- it's the requirements of keeping a vaccine right from production all the way through to when the vaccine is applied, to keep it refrigerated. Now, that presents some logistical challenges but we have ways to do it. This is a slightly extreme case in point but it helps illustrate the logistical challenges, in particular in resource-poor settings, of what's required to get vaccines refrigerated and maintain the cold chain. If the vaccine is too warm the vaccine breaks down, but interestingly it can be too cold and the vaccine can break down as well.
Now, the stakes are very high. The WHO estimates that within Africa, up to half the vaccines used there are considered to not be working properly because at some point the cold chain has fallen over. So it's a big problem, and it's tied in with the needle and syringe because it's a liquid form vaccine, and when it's liquid it needs the refrigeration.
A key attribute of our Nanopatch is that the vaccine is dry, and when it's dry it doesn't need refrigeration. Within my lab we've shown that we can keep the vaccine stored at 23 degrees Celsius for more than a year without any loss in activity at all. That's an important improvement. (Applause) We're delighted about it as well. And the thing about it is that we have well and truly proven the Nanopatch within the laboratory setting. And as a scientist, I love that and I love science. However, as an engineer, as a biomedical engineer and also as a human being, I'm not going to be satisfied until we've rolled this thing out, taken it out of the lab and got it to people in large numbers and particularly the people that need it the most.
So we've commenced this particular journey, and we've commenced this journey in an unusual way. We've started with Papua New Guinea.
Now, Papua New Guinea is an example of a developing world country. It's about the same size as France, but it suffers from many of the key barriers existing within the world of today's vaccines. There's the logistics: Within this country there are only 800 refrigerators to keep vaccines chilled. Many of them are old, like this one in Port Moresby, many of them are breaking down and many are not in the Highlands where they are required. That's a challenge. But also, Papua New Guinea has the world's highest incidence of HPV, human papillomavirus, the cervical cancer [risk factor]. Yet, that vaccine is not available in large numbers because it's too expensive. So for those two reasons, with the attributes of the Nanopatch, we've got into the field and worked with the Nanopatch, and taken it to Papua New Guinea and we'll be following that up shortly.
Now, doing this kind of work is not easy. It's challenging, but there's nothing else in the world I'd rather be doing. And as we look ahead I'd like to share with you a thought: It's the thought of a future where the 17 million deaths per year that we currently have due to infectious disease is a historical footnote. And it's a historical footnote that has been achieved by improved, radically improved vaccines. Now standing here today in front of you at the birthplace of the needle and syringe, a device that's 160 years old, I'm presenting to you an alternative approach that could really help make that happen -- and it's the Nanopatch with its attributes of being needle-free, pain-free, the ability for removing the cold chain and improving the immunogenicity. Thank you.
Two twin domes, two radically opposed design cultures. One is made of thousands of steel parts, the other of a single silk thread. One is synthetic, the other organic. One is imposed on the environment, the other creates it. One is designed for nature, the other is designed by her.
Michelangelo said that when he looked at raw marble, he saw a figure struggling to be free. The chisel was Michelangelo's only tool. But living things are not chiseled. They grow. And in our smallest units of life, our cells, we carry all the information that's required for every other cell to function and to replicate.
Tools also have consequences. At least since the Industrial Revolution, the world of design has been dominated by the rigors of manufacturing and mass production. Assembly lines have dictated a world made of parts, framing the imagination of designers and architects who have been trained to think about their objects as assemblies of discrete parts with distinct functions.
But you don't find homogenous material assemblies in nature. Take human skin, for example. Our facial skins are thin with large pores. Our back skins are thicker, with small pores. One acts mainly as filter, the other mainly as barrier, and yet it's the same skin: no parts, no assemblies. It's a system that gradually varies its functionality by varying elasticity. So here this is a split screen to represent my split world view, the split personality of every designer and architect operating today between the chisel and the gene, between machine and organism, between assembly and growth, between Henry Ford and Charles Darwin. These two worldviews, my left brain and right brain, analysis and synthesis, will play out on the two screens behind me. My work, at its simplest level, is about uniting these two worldviews, moving away from assembly and closer into growth.
You're probably asking yourselves: Why now? Why was this not possible 10 or even five years ago? We live in a very special time in history, a rare time, a time when the confluence of four fields is giving designers access to tools we've never had access to before. These fields are computational design, allowing us to design complex forms with simple code; additive manufacturing, letting us produce parts by adding material rather than carving it out; materials engineering, which lets us design the behavior of materials in high resolution; and synthetic biology, enabling us to design new biological functionality by editing DNA. And at the intersection of these four fields, my team and I create. Please meet the minds and hands of my students.
We design objects and products and structures and tools across scales, from the large-scale, like this robotic arm with an 80-foot diameter reach with a vehicular base that will one day soon print entire buildings, to nanoscale graphics made entirely of genetically engineered microorganisms that glow in the dark. Here we've reimagined the mashrabiya, an archetype of ancient Arabic architecture, and created a screen where every aperture is uniquely sized to shape the form of light and heat moving through it.
In our next project, we explore the possibility of creating a cape and skirt -- this was for a Paris fashion show with Iris van Herpen -- like a second skin that are made of a single part, stiff at the contours, flexible around the waist. Together with my long-term 3D printing collaborator Stratasys, we 3D-printed this cape and skirt with no seams between the cells, and I'll show more objects like it. This helmet combines stiff and soft materials in 20-micron resolution. This is the resolution of a human hair. It's also the resolution of a CT scanner. That designers have access to such high-resolution analytic and synthetic tools, enables to design products that fit not only the shape of our bodies, but also the physiological makeup of our tissues. Next, we designed an acoustic chair, a chair that would be at once structural, comfortable and would also absorb sound. Professor Carter, my collaborator, and I turned to nature for inspiration, and by designing this irregular surface pattern, it becomes sound-absorbent. We printed its surface out of 44 different properties, varying in rigidity, opacity and color, corresponding to pressure points on the human body. Its surface, as in nature, varies its functionality not by adding another material or another assembly, but by continuously and delicately varying material property.
But is nature ideal? Are there no parts in nature? I wasn't raised in a religious Jewish home, but when I was young, my grandmother used to tell me stories from the Hebrew Bible, and one of them stuck with me and came to define much of what I care about. As she recounts: "On the third day of Creation, God commands the Earth to grow a fruit-bearing fruit tree." For this first fruit tree, there was to be no differentiation between trunk, branches, leaves and fruit. The whole tree was a fruit. Instead, the land grew trees that have bark and stems and flowers. The land created a world made of parts. I often ask myself, "What would design be like if objects were made of a single part? Would we return to a better state of creation?"
So we looked for that biblical material, that fruit-bearing fruit tree kind of material, and we found it. The second-most abundant biopolymer on the planet is called chitin, and some 100 million tons of it are produced every year by organisms such as shrimps, crabs, scorpions and butterflies. We thought if we could tune its properties, we could generate structures that are multifunctional out of a single part. So that's what we did. We called Legal Seafood --
we ordered a bunch of shrimp shells, we grinded them and we produced chitosan paste. By varying chemical concentrations, we were able to achieve a wide array of properties -- from dark, stiff and opaque, to light, soft and transparent. In order to print the structures in large scale, we built a robotically controlled extrusion system with multiple nozzles. The robot would vary material properties on the fly and create these 12-foot-long structures made of a single material, 100 percent recyclable. When the parts are ready, they're left to dry and find a form naturally upon contact with air. So why are we still designing with plastics? The air bubbles that were a byproduct of the printing process were used to contain photosynthetic microorganisms that first appeared on our planet 3.5 billion year ago, as we learned yesterday. Together with our collaborators at Harvard and MIT, we embedded bacteria that were genetically engineered to rapidly capture carbon from the atmosphere and convert it into sugar. For the first time, we were able to generate structures that would seamlessly transition from beam to mesh, and if scaled even larger, to windows. A fruit-bearing fruit tree. Working with an ancient material, one of the first lifeforms on the planet, plenty of water and a little bit of synthetic biology, we were able to transform a structure made of shrimp shells into an architecture that behaves like a tree. And here's the best part: for objects designed to biodegrade, put them in the sea, and they will nourish marine life; place them in soil, and they will help grow a tree.
The setting for our next exploration using the same design principles was the solar system. We looked for the possibility of creating life-sustaining clothing for interplanetary voyages. To do that, we needed to contain bacteria and be able to control their flow. So like the periodic table, we came up with our own table of the elements: new lifeforms that were computationally grown, additively manufactured and biologically augmented. I like to think of synthetic biology as liquid alchemy, only instead of transmuting precious metals, you're synthesizing new biological functionality inside very small channels. It's called microfluidics. We 3D-printed our own channels in order to control the flow of these liquid bacterial cultures. In our first piece of clothing, we combined two microorganisms. The first is cyanobacteria. It lives in our oceans and in freshwater ponds. And the second, E. coli, the bacterium that inhabits the human gut. One converts light into sugar, the other consumes that sugar and produces biofuels useful for the built environment. Now, these two microorganisms never interact in nature. In fact, they never met each other. They've been here, engineered for the first time, to have a relationship inside a piece of clothing. Think of it as evolution not by natural selection, but evolution by design. In order to contain these relationships, we've created a single channel that resembles the digestive tract, that will help flow these bacteria and alter their function along the way. We then started growing these channels on the human body, varying material properties according to the desired functionality. Where we wanted more photosynthesis, we would design more transparent channels. This wearable digestive system, when it's stretched end to end, spans 60 meters. This is half the length of a football field, and 10 times as long as our small intestines. And here it is for the first time unveiled at TED -- our first photosynthetic wearable, liquid channels glowing with life inside a wearable clothing.
Mary Shelley said, "We are unfashioned creatures, but only half made up." What if design could provide that other half? What if we could create structures that would augment living matter? What if we could create personal microbiomes that would scan our skins, repair damaged tissue and sustain our bodies? Think of this as a form of edited biology. This entire collection, Wanderers, that was named after planets, was not to me really about fashion per se, but it provided an opportunity to speculate about the future of our race on our planet and beyond, to combine scientific insight with lots of mystery and to move away from the age of the machine to a new age of symbiosis between our bodies, the microorganisms that we inhabit, our products and even our buildings. I call this material ecology.
To do this, we always need to return back to nature. By now, you know that a 3D printer prints material in layers. You also know that nature doesn't. It grows. It adds with sophistication. This silkworm cocoon, for example, creates a highly sophisticated architecture, a home inside which to metamorphisize. No additive manufacturing today gets even close to this level of sophistication. It does so by combining not two materials, but two proteins in different concentrations. One acts as the structure, the other is the glue, or the matrix, holding those fibers together. And this happens across scales. The silkworm first attaches itself to the environment -- it creates a tensile structure -- and it then starts spinning a compressive cocoon. Tension and compression, the two forces of life, manifested in a single material.
In order to better understand how this complex process works, we glued a tiny earth magnet to the head of a silkworm, to the spinneret. We placed it inside a box with magnetic sensors, and that allowed us to create this 3-dimensional point cloud and visualize the complex architecture of the silkworm cocoon. However, when we placed the silkworm on a flat patch, not inside a box, we realized it would spin a flat cocoon and it would still healthily metamorphisize. So we started designing different environments, different scaffolds, and we discovered that the shape, the composition, the structure of the cocoon, was directly informed by the environment.
Silkworms are often boiled to death inside their cocoons, their silk unraveled and used in the textile industry. We realized that designing these templates allowed us to give shape to raw silk without boiling a single cocoon.
They would healthily metamorphisize, and we would be able to create these things.
So we scaled this process up to architectural scale. We had a robot spin the template out of silk, and we placed it on our site. We knew silkworms migrated toward darker and colder areas, so we used a sun path diagram to reveal the distribution of light and heat on our structure. We then created holes, or apertures, that would lock in the rays of light and heat, distributing those silkworms on the structure.
We were ready to receive the caterpillars. We ordered 6,500 silkworms from an online silk farm. And after four weeks of feeding, they were ready to spin with us. We placed them carefully at the bottom rim of the scaffold, and as they spin they pupate, they mate, they lay eggs, and life begins all over again -- just like us but much, much shorter.
Bucky Fuller said that tension is the great integrity, and he was right. As they spin biological silk over robotically spun silk, they give this entire pavilion its integrity. And over two to three weeks, 6,500 silkworms spin 6,500 kilometers. In a curious symmetry, this is also the length of the Silk Road. The moths, after they hatch, produce 1.5 million eggs. This could be used for 250 additional pavilions for the future.
So here they are, the two worldviews. One spins silk out of a robotic arm, the other fills in the gaps.
If the final frontier of design is to breathe life into the products and the buildings around us, to form a two-material ecology, then designers must unite these two worldviews. Which brings us back, of course, to the beginning. Here's to a new age of design, a new age of creation, that takes us from a nature-inspired design to a design-inspired nature, and that demands of us for the first time that we mother nature.
Technology can change our understanding of nature.
Take for example the case of lions. For centuries, it's been said that female lions do all of the hunting out in the open savanna, and male lions do nothing until it's time for dinner. You've heard this too, I can tell. Well recently, I led an airborne mapping campaign in the Kruger National Park in South Africa. Our colleagues put GPS tracking collars on male and female lions, and we mapped their hunting behavior from the air. The lower left shows a lion sizing up a herd of impala for a kill, and the right shows what I call the lion viewshed. That's how far the lion can see in all directions until his or her view is obstructed by vegetation. And what we found is that male lions are not the lazy hunters we thought them to be. They just use a different strategy. Whereas the female lions hunt out in the open savanna over long distances, usually during the day, male lions use an ambush strategy in dense vegetation, and often at night. This video shows the actual hunting viewsheds of male lions on the left and females on the right. Red and darker colors show more dense vegetation, and the white are wide open spaces. And this is the viewshed right literally at the eye level of hunting male and female lions. All of a sudden, you get a very clear understanding of the very spooky conditions under which male lions do their hunting.
I bring up this example to begin, because it emphasizes how little we know about nature. There's been a huge amount of work done so far to try to slow down our losses of tropical forests, and we are losing our forests at a rapid rate, as shown in red on the slide. I find it ironic that we're doing so much, yet these areas are fairly unknown to science. So how can we save what we don't understand?
Now I'm a global ecologist and an Earth explorer with a background in physics and chemistry and biology and a lot of other boring subjects, but above all, I'm obsessed with what we don't know about our planet. So I created this, the Carnegie Airborne Observatory, or CAO. It may look like a plane with a fancy paint job, but I packed it with over 1,000 kilos of high-tech sensors, computers, and a very motivated staff of Earth scientists and pilots. Two of our instruments are very unique: one is called an imaging spectrometer that can actually measure the chemical composition of plants as we fly over them. Another one is a set of lasers, very high-powered lasers, that fire out of the bottom of the plane, sweeping across the ecosystem and measuring it at nearly 500,000 times per second in high-resolution 3D. Here's an image of the Golden Gate Bridge in San Francisco, not far from where I live. Although we flew straight over this bridge, we imaged it in 3D, captured its color in just a few seconds. But the real power of the CAO is its ability to capture the actual building blocks of ecosystems. This is a small town in the Amazon, imaged with the CAO. We can slice through our data and see, for example, the 3D structure of the vegetation and the buildings, or we can use the chemical information to actually figure out how fast the plants are growing as we fly over them. The hottest pinks are the fastest-growing plants. And we can see biodiversity in ways that you never could have imagined. This is what a rainforest might look like as you fly over it in a hot air balloon. This is how we see a rainforest, in kaleidoscopic color that tells us that there are many species living with one another. But you have to remember that these trees are literally bigger than whales, and what that means is that they're impossible to understand just by walking on the ground below them. So our imagery is 3D, it's chemical, it's biological, and this tells us not only the species that are living in the canopy, but it tells us a lot of information about the rest of the species that occupy the rainforest.
Now I created the CAO in order to answer questions that have proven extremely challenging to answer from any other vantage point, such as from the ground, or from satellite sensors. I want to share three of those questions with you today. The first questions is, how do we manage our carbon reserves in tropical forests? Tropical forests contain a huge amount of carbon in the trees, and we need to keep that carbon in those forests if we're going to avoid any further global warming. Unfortunately, global carbon emissions from deforestation now equals the global transportation sector. That's all ships, airplanes, trains and automobiles combined. So it's understandable that policy negotiators have been working hard to reduce deforestation, but they're doing it on landscapes that are hardly known to science. If you don't know where the carbon is exactly, in detail, how can you know what you're losing? Basically, we need a high-tech accounting system. With our system, we're able to see the carbon stocks of tropical forests in utter detail. The red shows, obviously, closed-canopy tropical forest, and then you see the cookie cutting, or the cutting of the forest in yellows and greens. It's like cutting a cake except this cake is about whale deep. And yet, we can zoom in and see the forest and the trees at the same time. And what's amazing is, even though we flew very high above this forest, later on in analysis, we can go in and actually experience the treetrops, leaf by leaf, branch by branch, just as the other species that live in this forest experience it along with the trees themselves.
We've been using the technology to explore and to actually put out the first carbon geographies in high resolution in faraway places like the Amazon Basin and not-so-faraway places like the United States and Central America. What I'm going to do is I'm going to take you on a high-resolution, first-time tour of the carbon landscapes of Peru and then Panama. The colors are going to be going from red to blue. Red is extremely high carbon stocks, your largest cathedral forests you can imagine, and blue are very low carbon stocks. And let me tell you, Peru alone is an amazing place, totally unknown in terms of its carbon geography until today. We can fly to this area in northern Peru and see super high carbon stocks in red, and the Amazon River and floodplain cutting right through it. We can go to an area of utter devastation caused by deforestation in blue, and the virus of deforestation spreading out in orange. We can also fly to the southern Andes to see the tree line and see exactly how the carbon geography ends as we go up into the mountain system. And we can go to the biggest swamp in the western Amazon. It's a watery dreamworld akin to Jim Cameron's "Avatar." We can go to one of the smallest tropical countries, Panama, and see also a huge range of carbon variation, from high in red to low in blue. Unfortunately, most of the carbon is lost in the lowlands, but what you see that's left, in terms of high carbon stocks in greens and reds, is the stuff that's up in the mountains. One interesting exception to this is right in the middle of your screen. You're seeing the buffer zone around the Panama Canal. That's in the reds and yellows. The canal authorities are using force to protect their watershed and global commerce. This kind of carbon mapping has transformed conservation and resource policy development. It's really advancing our ability to save forests and to curb climate change.
My second question: How do we prepare for climate change in a place like the Amazon rainforest? Let me tell you, I spend a lot of time in these places, and we're seeing the climate changing already. Temperatures are increasing, and what's really happening is we're getting a lot of droughts, recurring droughts. The 2010 mega-drought is shown here with red showing an area about the size of Western Europe. The Amazon was so dry in 2010 that even the main stem of the Amazon river itself dried up partially, as you see in the photo in the lower portion of the slide. What we found is that in very remote areas, these droughts are having a big negative impact on tropical forests. For example, these are all of the dead trees in red that suffered mortality following the 2010 drought. This area happens to be on the border of Peru and Brazil, totally unexplored, almost totally unknown scientifically.
So what we think, as Earth scientists, is species are going to have to migrate with climate change from the east in Brazil all the way west into the Andes and up into the mountains in order to minimize their exposure to climate change. One of the problems with this is that humans are taking apart the western Amazon as we speak. Look at this 100-square-kilometer gash in the forest created by gold miners. You see the forest in green in 3D, and you see the effects of gold mining down below the soil surface. Species have nowhere to migrate in a system like this, obviously.
If you haven't been to the Amazon, you should go. It's an amazing experience every time, no matter where you go. You're going to probably see it this way, on a river. But what happens is a lot of times the rivers hide what's really going on back in the forest itself. We flew over this same river, imaged the system in 3D. The forest is on the left. And then we can digitally remove the forest and see what's going on below the canopy. And in this case, we found gold mining activity, all of it illegal, set back away from the river's edge, as you'll see in those strange pockmarks coming up on your screen on the right. Don't worry, we're working with the authorities to deal with this and many, many other problems in the region.
So in order to put together a conservation plan for these unique, important corridors like the western Amazon and the Andes Amazon corridor, we have to start making geographically explicit plans now. How do we do that if we don't know the geography of biodiversity in the region, if it's so unknown to science? So what we've been doing is using the laser-guided spectroscopy from the CAO to map for the first time the biodiversity of the Amazon rainforest. Here you see actual data showing different species in different colors. Reds are one type of species, blues are another, and greens are yet another. And when we take this together and scale up to the regional level, we get a completely new geography of biodiversity unknown prior to this work. This tells us where the big biodiversity changes occur from habitat to habitat, and that's really important because it tells us a lot about where species may migrate to and migrate from as the climate shifts. And this is the pivotal information that's needed by decision makers to develop protected areas in the context of their regional development plans.
And third and final question is, how do we manage biodiversity on a planet of protected ecosystems? The example I started out with about lions hunting, that was a study we did behind the fence line of a protected area in South Africa. And the truth is, much of Africa's nature is going to persist into the future in protected areas like I show in blue on the screen. This puts incredible pressure and responsibility on park management. They need to do and make decisions that will benefit all of the species that they're protecting. Some of their decisions have really big impacts. For example, how much and where to use fire as a management tool? Or, how to deal with a large species like elephants, which may, if their populations get too large, have a negative impact on the ecosystem and on other species. And let me tell you, these types of dynamics really play out on the landscape. In the foreground is an area with lots of fire and lots of elephants: wide open savanna in blue, and just a few trees. As we cross this fence line, now we're getting into an area that has had protection from fire and zero elephants: dense vegetation, a radically different ecosystem. And in a place like Kruger, the soaring elephant densities are a real problem. I know it's a sensitive issue for many of you, and there are no easy answers with this. But what's good is that the technology we've developed and we're working with in South Africa, for example, is allowing us to map every single tree in the savanna, and then through repeat flights we're able to see which trees are being pushed over by elephants, in the red as you see on the screen, and how much that's happening in different types of landscapes in the savanna. That's giving park managers a very first opportunity to use tactical management strategies that are more nuanced and don't lead to those extremes that I just showed you. So really, the way we're looking at protected areas nowadays is to think of it as tending to a circle of life, where we have fire management, elephant management, those impacts on the structure of the ecosystem, and then those impacts affecting everything from insects up to apex predators like lions.
Going forward, I plan to greatly expand the airborne observatory. I'm hoping to actually put the technology into orbit so we can manage the entire planet with technologies like this. Until then, you're going to find me flying in some remote place that you've never heard of. I just want to end by saying that technology is absolutely critical to managing our planet, but even more important is the understanding and wisdom to apply it.
Some years ago, I set out to try to understand if there was a possibility to develop biofuels on a scale that would actually compete with fossil fuels but not compete with agriculture for water, fertilizer or land.
So here's what I came up with. Imagine that we build an enclosure where we put it just underwater, and we fill it with wastewater and some form of microalgae that produces oil, and we make it out of some kind of flexible material that moves with waves underwater, and the system that we're going to build, of course, will use solar energy to grow the algae, and they use CO2, which is good, and they produce oxygen as they grow. The algae that grow are in a container that distributes the heat to the surrounding water, and you can harvest them and make biofuels and cosmetics and fertilizer and animal feed, and of course you'd have to make a large area of this, so you'd have to worry about other stakeholders like fishermen and ships and such things, but hey, we're talking about biofuels, and we know the importance of potentially getting an alternative liquid fuel.
Why are we talking about microalgae? Here you see a graph showing you the different types of crops that are being considered for making biofuels, so you can see some things like soybean, which makes 50 gallons per acre per year, or sunflower or canola or jatropha or palm, and that tall graph there shows what microalgae can contribute. That is to say, microalgae contributes between 2,000 and 5,000 gallons per acre per year, compared to the 50 gallons per acre per year from soy.
So what are microalgae? Microalgae are micro -- that is, they're extremely small, as you can see here a picture of those single-celled organisms compared to a human hair. Those small organisms have been around for millions of years and there's thousands of different species of microalgae in the world, some of which are the fastest-growing plants on the planet, and produce, as I just showed you, lots and lots of oil.
Now, why do we want to do this offshore? Well, the reason we're doing this offshore is because if you look at our coastal cities, there isn't a choice, because we're going to use waste water, as I suggested, and if you look at where most of the waste water treatment plants are, they're embedded in the cities. This is the city of San Francisco, which has 900 miles of sewer pipes under the city already, and it releases its waste water offshore. So different cities around the world treat their waste water differently. Some cities process it. Some cities just release the water. But in all cases, the water that's released is perfectly adequate for growing microalgae. So let's envision what the system might look like. We call it OMEGA, which is an acronym for Offshore Membrane Enclosures for Growing Algae. At NASA, you have to have good acronyms.
So how does it work? I sort of showed you how it works already. We put waste water and some source of CO2 into our floating structure, and the waste water provides nutrients for the algae to grow, and they sequester CO2 that would otherwise go off into the atmosphere as a greenhouse gas. They of course use solar energy to grow, and the wave energy on the surface provides energy for mixing the algae, and the temperature is controlled by the surrounding water temperature. The algae that grow produce oxygen, as I've mentioned, and they also produce biofuels and fertilizer and food and other bi-algal products of interest.
And the system is contained. What do I mean by that? It's modular. Let's say something happens that's totally unexpected to one of the modules. It leaks. It's struck by lightning. The waste water that leaks out is water that already now goes into that coastal environment, and the algae that leak out are biodegradable, and because they're living in waste water, they're fresh water algae, which means they can't live in salt water, so they die. The plastic we'll build it out of is some kind of well-known plastic that we have good experience with, and we'll rebuild our modules to be able to reuse them again.
So we may be able to go beyond that when thinking about this system that I'm showing you, and that is to say we need to think in terms of the water, the fresh water, which is also going to be an issue in the future, and we're working on methods now for recovering the waste water.
The other thing to consider is the structure itself. It provides a surface for things in the ocean, and this surface, which is covered by seaweeds and other organisms in the ocean, will become enhanced marine habitat so it increases biodiversity. And finally, because it's an offshore structure, we can think in terms of how it might contribute to an aquaculture activity offshore.
So you're probably thinking, "Gee, this sounds like a good idea. What can we do to try to see if it's real?" Well, I set up laboratories in Santa Cruz at the California Fish and Game facility, and that facility allowed us to have big seawater tanks to test some of these ideas. We also set up experiments in San Francisco at one of the three waste water treatment plants, again a facility to test ideas. And finally, we wanted to see where we could look at what the impact of this structure would be in the marine environment, and we set up a field site at a place called Moss Landing Marine Lab in Monterey Bay, where we worked in a harbor to see what impact this would have on marine organisms.
The laboratory that we set up in Santa Cruz was our skunkworks. It was a place where we were growing algae and welding plastic and building tools and making a lot of mistakes, or, as Edison said, we were finding the 10,000 ways that the system wouldn't work. Now, we grew algae in waste water, and we built tools that allowed us to get into the lives of algae so that we could monitor the way they grow, what makes them happy, how do we make sure that we're going to have a culture that will survive and thrive. So the most important feature that we needed to develop were these so-called photobioreactors, or PBRs. These were the structures that would be floating at the surface made out of some inexpensive plastic material that'll allow the algae to grow, and we had built lots and lots of designs, most of which were horrible failures, and when we finally got to a design that worked, at about 30 gallons, we scaled it up to 450 gallons in San Francisco.
So let me show you how the system works. We basically take waste water with algae of our choice in it, and we circulate it through this floating structure, this tubular, flexible plastic structure, and it circulates through this thing, and there's sunlight of course, it's at the surface, and the algae grow on the nutrients.
But this is a bit like putting your head in a plastic bag. The algae are not going to suffocate because of CO2, as we would. They suffocate because they produce oxygen, and they don't really suffocate, but the oxygen that they produce is problematic, and they use up all the CO2. So the next thing we had to figure out was how we could remove the oxygen, which we did by building this column which circulated some of the water, and put back CO2, which we did by bubbling the system before we recirculated the water. And what you see here is the prototype, which was the first attempt at building this type of column. The larger column that we then installed in San Francisco in the installed system.
So the column actually had another very nice feature, and that is the algae settle in the column, and this allowed us to accumulate the algal biomass in a context where we could easily harvest it. So we would remove the algaes that concentrated in the bottom of this column, and then we could harvest that by a procedure where you float the algae to the surface and can skim it off with a net.
So we wanted to also investigate what would be the impact of this system in the marine environment, and I mentioned we set up this experiment at a field site in Moss Landing Marine Lab. Well, we found of course that this material became overgrown with algae, and we needed then to develop a cleaning procedure, and we also looked at how seabirds and marine mammals interacted, and in fact you see here a sea otter that found this incredibly interesting, and would periodically work its way across this little floating water bed, and we wanted to hire this guy or train him to be able to clean the surface of these things, but that's for the future.
Now really what we were doing, we were working in four areas. Our research covered the biology of the system, which included studying the way algae grew, but also what eats the algae, and what kills the algae. We did engineering to understand what we would need to be able to do to build this structure, not only on the small scale, but how we would build it on this enormous scale that will ultimately be required. I mentioned we looked at birds and marine mammals and looked at basically the environmental impact of the system, and finally we looked at the economics, and what I mean by economics is, what is the energy required to run the system? Do you get more energy out of the system than you have to put into the system to be able to make the system run? And what about operating costs? And what about capital costs? And what about, just, the whole economic structure?
So let me tell you that it's not going to be easy, and there's lots more work to do in all four of those areas to be able to really make the system work. But we don't have a lot of time, and I'd like to show you the artist's conception of how this system might look if we find ourselves in a protected bay somewhere in the world, and we have in the background in this image, the waste water treatment plant and a source of flue gas for the CO2, but when you do the economics of this system, you find that in fact it will be difficult to make it work. Unless you look at the system as a way to treat waste water, sequester carbon, and potentially for photovoltaic panels or wave energy or even wind energy, and if you start thinking in terms of integrating all of these different activities, you could also include in such a facility aquaculture. So we would have under this system a shellfish aquaculture where we're growing mussels or scallops. We'd be growing oysters and things that would be producing high value products and food, and this would be a market driver as we build the system to larger and larger scales so that it becomes, ultimately, competitive with the idea of doing it for fuels.
So there's always a big question that comes up, because plastic in the ocean has got a really bad reputation right now, and so we've been thinking cradle to cradle. What are we going to do with all this plastic that we're going to need to use in our marine environment? Well, I don't know if you know about this, but in California, there's a huge amount of plastic that's used in fields right now as plastic mulch, and this is plastic that's making these tiny little greenhouses right along the surface of the soil, and this provides warming the soil to increase the growing season, it allows us to control weeds, and, of course, it makes the watering much more efficient. So the OMEGA system will be part of this type of an outcome, and that when we're finished using it in the marine environment, we'll be using it, hopefully, on fields.
Where are we going to put this, and what will it look like offshore? Here's an image of what we could do in San Francisco Bay. San Francisco produces 65 million gallons a day of waste water. If we imagine a five-day retention time for this system, we'd need 325 million gallons to accomodate, and that would be about 1,280 acres of these OMEGA modules floating in San Francisco Bay. Well, that's less than one percent of the surface area of the bay. It would produce, at 2,000 gallons per acre per year, it would produce over 2 million gallons of fuel, which is about 20 percent of the biodiesel, or of the diesel that would be required in San Francisco, and that's without doing anything about efficiency.
Where else could we potentially put this system? There's lots of possibilities. There's, of course, San Francisco Bay, as I mentioned. San Diego Bay is another example, Mobile Bay or Chesapeake Bay, but the reality is, as sea level rises, there's going to be lots and lots of new opportunities to consider. (Laughter)
So what I'm telling you about is a system of integrated activities. Biofuels production is integrated with alternative energy is integrated with aquaculture.
I set out to find a pathway to innovative production of sustainable biofuels, and en route I discovered that what's really required for sustainability is integration more than innovation.
Long term, I have great faith in our collective and connected ingenuity. I think there is almost no limit to what we can accomplish if we are radically open and we don't care who gets the credit. Sustainable solutions for our future problems are going to be diverse and are going to be many. I think we need to consider everything, everything from alpha to OMEGA. Thank you. (Applause) (Applause) Chris Anderson: Just a quick question for you, Jonathan. Can this project continue to move forward within NASA or do you need some very ambitious green energy fund to come and take it by the throat? Jonathan Trent: So it's really gotten to a stage now in NASA where they would like to spin it out into something which would go offshore, and there are a lot of issues with doing it in the United States because of limited permitting issues and the time required to get permits to do things offshore. It really requires, at this point, people on the outside, and we're being radically open with this technology in which we're going to launch it out there for anybody and everybody who's interested to take it on and try to make it real. CA: So that's interesting. You're not patenting it. You're publishing it. JT: Absolutely. CA: All right. Thank you so much. JT: Thank you.
I'm going to be showing some of the cybercriminals' latest and nastiest creations. So basically, please don't go and download any of the viruses that I show you.
Some of you might be wondering what a cybersecurity specialist looks like, and I thought I'd give you a quick insight into my career so far. It's a pretty accurate description. This is what someone that specializes in malware and hacking looks like.
So today, computer viruses and trojans, designed to do everything from stealing data to watching you in your webcam to the theft of billions of dollars. Some malicious code today goes as far as targeting power, utilities and infrastructure.
Let me give you a quick snapshot of what malicious code is capable of today. Right now, every second, eight new users are joining the Internet. Today, we will see 250,000 individual new computer viruses. We will see 30,000 new infected websites. And, just to kind of tear down a myth here, lots of people think that when you get infected with a computer virus, it's because you went to a porn site. Right? Well, actually, statistically speaking, if you only visit porn sites, you're safer. People normally write that down, by the way. (Laughter) Actually, about 80 percent of these are small business websites getting infected.
Today's cybercriminal, what do they look like? Well, many of you have the image, don't you, of the spotty teenager sitting in a basement, hacking away for notoriety. But actually today, cybercriminals are wonderfully professional and organized. In fact, they have product adverts. You can go online and buy a hacking service to knock your business competitor offline. Check out this one I found.
 (Video) Man: So you're here for one reason, and that reason is because you need your business competitors, rivals, haters, or whatever the reason is, or who, they are to go down. Well you, my friend, you've came to the right place. If you want your business competitors to go down, well, they can. If you want your rivals to go offline, well, they will. Not only that, we are providing a short-term-to-long-term DDOS service or scheduled attack, starting five dollars per hour for small personal websites to 10 to 50 dollars per hour.
James Lyne: Now, I did actually pay one of these cybercriminals to attack my own website. Things got a bit tricky when I tried to expense it at the company. Turns out that's not cool. But regardless, it's amazing how many products and services are available now to cybercriminals. For example, this testing platform, which enables the cybercriminals to test the quality of their viruses before they release them on the world. For a small fee, they can upload it and make sure everything is good.
But it goes further. Cybercriminals now have crime packs with business intelligence reporting dashboards to manage the distribution of their malicious code. This is the market leader in malware distribution, the Black Hole Exploit Pack, responsible for nearly one third of malware distribution in the last couple of quarters. It comes with technical installation guides, video setup routines, and get this, technical support. You can email the cybercriminals and they'll tell you how to set up your illegal hacking server.
So let me show you what malicious code looks like today. What I've got here is two systems, an attacker, which I've made look all Matrix-y and scary, and a victim, which you might recognize from home or work. Now normally, these would be on different sides of the planet or of the Internet, but I've put them side by side because it makes things much more interesting.
Now, there are many ways you can get infected. You will have come in contact with some of them. Maybe some of you have received an email that says something like, "Hi, I'm a Nigerian banker, and I'd like to give you 53 billion dollars because I like your face." Or funnycats.exe, which rumor has it was quite successful in China's recent campaign against America.
Now there are many ways you can get infected. I want to show you a couple of my favorites. This is a little USB key. Now how do you get a USB key to run in a business? Well, you could try looking really cute. Awww. Or, in my case, awkward and pathetic. So imagine this scenario: I walk into one of your businesses, looking very awkward and pathetic, with a copy of my C.V. which I've covered in coffee, and I ask the receptionist to plug in this USB key and print me a new one. So let's have a look here on my victim computer. What I'm going to do is plug in the USB key. After a couple of seconds, things start to happen on the computer on their own, usually a bad sign. This would, of course, normally happen in a couple of seconds, really, really quickly, but I've kind of slowed it down so you can actually see the attack occurring. Malware is very boring otherwise. So this is writing out the malicious code, and a few seconds later, on the left-hand side, you'll see the attacker's screen get some interesting new text. Now if I place the mouse cursor over it, this is what we call a command prompt, and using this we can navigate around the computer. We can access your documents, your data. You can turn on the webcam. That can be very embarrassing. Or just to really prove a point, we can launch programs like my personal favorite, the Windows Calculator.
So isn't it amazing how much control the attackers can get with such a simple operation? Let me show you how most malware is now distributed today. What I'm going to do is open up a website that I wrote. It's a terrible website. It's got really awful graphics. And it's got a comments section here where we can submit comments to the website. Many of you will have used something a bit like this before. Unfortunately, when this was implemented, the developer was slightly inebriated and managed to forget all of the secure coding practices he had learned. So let's imagine that our attacker, called Evil Hacker just for comedy value, inserts something a little nasty. This is a script. It's code which will be interpreted on the webpage. So I'm going to submit this post, and then, on my victim computer, I'm going to open up the web browser and browse to my website, www.incrediblyhacked.com. Notice that after a couple of seconds, I get redirected. That website address at the top there, which you can just about see, microshaft.com, the browser crashes as it hits one of these exploit packs, and up pops fake antivirus. This is a virus pretending to look like antivirus software, and it will go through and it will scan the system, have a look at what its popping up here. It creates some very serious alerts. Oh look, a child porn proxy server. We really should clean that up. What's really insulting about this is not only does it provide the attackers with access to your data, but when the scan finishes, they tell you in order to clean up the fake viruses, you have to register the product. Now I liked it better when viruses were free. (Laughter) People now pay cybercriminals money to run viruses, which I find utterly bizarre.
So anyway, let me change pace a little bit. Chasing 250,000 pieces of malware a day is a massive challenge, and those numbers are only growing directly in proportion to the length of my stress line, you'll note here. So I want to talk to you briefly about a group of hackers we tracked for a year and actually found -- and this is a rare treat in our job. Now this was a cross-industry collaboration, people from Facebook, independent researchers, guys from Sophos. So here we have a couple of documents which our cybercriminals had uploaded to a cloud service, kind of like Dropbox or SkyDrive, like many of you might use. At the top, you'll notice a section of source code. What this would do is send the cybercriminals a text message every day telling them how much money they'd made that day, so a kind of cybercriminal billings report, if you will. If you look closely, you'll notice a series of what are Russian telephone numbers. Now that's obviously interesting, because that gives us a way of finding our cybercriminals. Down below, highlighted in red, in the other section of source code, is this bit "leded:leded." That's a username, kind of like you might have on Twitter.
So let's take this a little further. There are a few other interesting pieces the cybercriminals had uploaded. Lots of you here will use smartphones to take photos and post them from the conference. An interesting feature of lots of modern smartphones is that when you take a photo, it embeds GPS data about where that photo was taken. In fact, I've been spending a lot of time on Internet dating sites recently, obviously for research purposes, and I've noticed that about 60 percent of the profile pictures on Internet dating sites contain the GPS coordinates of where the photo was taken, which is kind of scary because you wouldn't give out your home address to lots of strangers, but we're happy to give away our GPS coordinates to plus or minus 15 meters. And our cybercriminals had done the same thing. So here's a photo which resolves to St. Petersburg. We then deploy the incredibly advanced hacking tool. We used Google. Using the email address, the telephone number and the GPS data, on the left you see an advert for a BMW that one of our cybercriminals is selling, on the other side an advert for the sale of sphynx kittens. One of these was more stereotypical for me. A little more searching, and here's our cybercriminal. Imagine, these are hardened cybercriminals sharing information scarcely. Imagine what you could find about each of the people in this room. A bit more searching through the profile and there's a photo of their office. They were working on the third floor. And you can also see some photos from his business companion where he has a taste in a certain kind of image. It turns out he's a member of the Russian Adult Webmasters Federation.
But this is where our investigation starts to slow down. The cybercriminals have locked down their profiles quite well. And herein is the greatest lesson of social media and mobile devices for all of us right now. Our friends, our families and our colleagues can break our security even when we do the right things. This is MobSoft, one of the companies that this cybercriminal gang owned, and an interesting thing about MobSoft is the 50-percent owner of this posted a job advert, and this job advert matched one of the telephone numbers from the code earlier. This woman was Maria, and Maria is the wife of one of our cybercriminals. And it's kind of like she went into her social media settings and clicked on every option imaginable to make herself really, really insecure. By the end of the investigation, where you can read the full 27-page report at that link, we had photos of the cybercriminals, even the office Christmas party when they were out on an outing. That's right, cybercriminals do have Christmas parties, as it turns out. Now you're probably wondering what happened to these guys. Let me come back to that in just a minute.
I want to change pace to one last little demonstration, a technique that is wonderfully simple and basic, but is interesting in exposing how much information we're all giving away, and it's relevant because it applies to us as a TED audience. This is normally when people start kind of shuffling in their pockets trying to turn their phones onto airplane mode desperately.
Many of you all know about the concept of scanning for wireless networks. You do it every time you take out your iPhone or your Blackberry and connect to something like TEDAttendees. But what you might not know is that you're also beaming out a list of networks you've previously connected to, even when you're not using wireless actively. So I ran a little scan. I was relatively inhibited compared to the cybercriminals, who wouldn't be so concerned by law, and here you can see my mobile device. Okay? So you can see a list of wireless networks. TEDAttendees, HyattLB. Where do you think I'm staying? My home network, PrettyFlyForAWifi, which I think is a great name. Sophos_Visitors, SANSEMEA, companies I work with. Loganwifi, that's in Boston. HiltonLondon. CIASurveillanceVan. We called it that at one of our conferences because we thought that would freak people out, which is quite fun. This is how geeks party.
So let's make this a little bit more interesting. Let's talk about you. Twenty-three percent of you have been to Starbucks recently and used the wireless network. Things get more interesting. Forty-six percent of you I could link to a business, XYZ Employee network. This isn't an exact science, but it gets pretty accurate. Seven hundred and sixty-one of you I could identify a hotel you'd been to recently, absolutely with pinpoint precision somewhere on the globe. Two hundred and thirty-four of you, well, I know where you live. Your wireless network name is so unique that I was able to pinpoint it using data available openly on the Internet with no hacking or clever, clever tricks. And I should mention as well that some of you do use your names, "James Lyne's iPhone," for example. And two percent of you have a tendency to extreme profanity.
14:40
So something for you to think about: As we adopt these new applications and mobile devices, as we play with these shiny new toys, how much are we trading off convenience for privacy and security? Next time you install something, look at the settings and ask yourself, "Is this information that I want to share? Would someone be able to abuse it?"
We also need to think very carefully about how we develop our future talent pool. You see, technology's changing at a staggering rate, and that 250,000 pieces of malware won't stay the same for long. There's a very concerning trend that whilst many people coming out of schools now are much more technology-savvy, they know how to use technology, fewer and fewer people are following the feeder subjects to know how that technology works under the covers. In the U.K., a 60 percent reduction since 2003, and there are similar statistics all over the world.
We also need to think about the legal issues in this area. The cybercriminals I talked about, despite theft of millions of dollars, actually still haven't been arrested, and at this point possibly never will. Most laws are national in their implementation, despite cybercrime conventions, where the Internet is borderless and international by definition. Countries do not agree, which makes this area exceptionally challenging from a legal perspective.
But my biggest ask is this: You see, you're going to leave here and you're going to see some astonishing stories in the news. You're going to read about malware doing incredible and terrifying, scary things. However, 99 percent of it works because people fail to do the basics. So my ask is this: Go online, find these simple best practices, find out how to update and patch your computer. Get a secure password. Make sure you use a different password on each of your sites and services online. Find these resources. Apply them.
The Internet is a fantastic resource for business, for political expression, for art and for learning. Help me and the security community make life much, much more difficult for cybercriminals.
Openness. It's a word that denotes opportunity and possibilities. Open-ended, open hearth, open source, open door policy, open bar. (Laughter)
And everywhere the world is opening up, and it's a good thing.
Why is this happening? The technology revolution is opening the world.
Yesterday's Internet was a platform for the presentation of content. The Internet of today is a platform for computation. The Internet is becoming a giant global computer, and every time you go on it, you upload a video, you do a Google search, you remix something, you're programming this big global computer that we all share. Humanity is building a machine, and this enables us to collaborate in new ways. Collaboration can occur on an astronomical basis.
Now a new generation is opening up the world as well. I started studying kids about 15 years ago, -- so actually 20 years ago now -- and I noticed how my own children were effortlessly able to use all this sophisticated technology, and at first I thought, "My children are prodigies!" (Laughter) But then I noticed all their friends were like them, so that was a bad theory. So I've started working with a few hundred kids, and I came to the conclusion that this is the first generation to come of age in the digital age, to be bathed in bits. I call them the Net Generation. I said, these kids are different. They have no fear of technology, because it's not there. It's like the air. It's sort of like, I have no fear of a refrigerator. And — (Laughter)
And there's no more powerful force to change every institution than the first generation of digital natives. I'm a digital immigrant. I had to learn the language.
The global economic crisis is opening up the world as well. Our opaque institutions from the Industrial Age, everything from old models of the corporation, government, media, Wall Street, are in various stages of being stalled or frozen or in atrophy or even failing, and this is now creating a burning platform in the world. I mean, think about Wall Street. The core modus operandi of Wall Street almost brought down global capitalism.
Now, you know the idea of a burning platform, that you're somewhere where the costs of staying where you are become greater than the costs of moving to something different, perhaps something radically different. And we need to change and open up all of our institutions.
So this technology push, a demographic kick from a new generation and a demand pull from a new economic global environment is causing the world to open up.
Now, I think, in fact, we're at a turning point in human history, where we can finally now rebuild many of the institutions of the Industrial Age around a new set of principles.
Now, what is openness? Well, as it turns out, openness has a number of different meanings, and for each there's a corresponding principle for the transformation of civilization. The first is collaboration. Now, this is openness in the sense of the boundaries of organizations becoming more porous and fluid and open.
The guy in the picture here, I'll tell you his story. His name is Rob McEwen. I'd like to say, "I have this think tank, we scour the world for amazing case studies." The reason I know this story is because he's my neighbor. (Laughter) He actually moved across the street from us, and he held a cocktail party to meet the neighbors, and he says, "You're Don Tapscott. I've read some of your books." I said, "Great. What do you do?" And he says, "Well I used to be a banker and now I'm a gold miner." And he tells me this amazing story. He takes over this gold mine, and his geologists can't tell him where the gold is. He gives them more money for geological data, they come back, they can't tell him where to go into production. After a few years, he's so frustrated he's ready to give up, but he has an epiphany one day. He wonders, "If my geologists don't know where the gold is, maybe somebody else does." So he does a "radical" thing. He takes his geological data, he publishes it and he holds a contest on the Internet called the Goldcorp Challenge. It's basically half a million dollars in prize money for anybody who can tell me, do I have any gold, and if so, where is it? (Laughter)
He gets submissions from all around the world. They use techniques that he's never heard of, and for his half a million dollars in prize money, Rob McEwen finds 3.4 billion dollars worth of gold. The market value of his company goes from 90 million to 10 billion dollars, and I can tell you, because he's my neighbor, he's a happy camper. (Laughter)
You know, conventional wisdom says talent is inside, right? Your most precious asset goes out the elevator every night. He viewed talent differently. He wondered, who are their peers? He should have fired his geology department, but he didn't. You know, some of the best submissions didn't come from geologists. They came from computer scientists, engineers. The winner was a computer graphics company that built a three dimensional model of the mine where you can helicopter underground and see where the gold is.
He helped us understand that social media's becoming social production. It's not about hooking up online. This is a new means of production in the making. And this Ideagora that he created, an open market, agora, for uniquely qualified minds, was part of a change, a profound change in the deep structure and architecture of our organizations, and how we sort of orchestrate capability to innovate, to create goods and services, to engage with the rest of the world, in terms of government, how we create public value. Openness is about collaboration.
Now secondly, openness is about transparency. This is different. Here, we're talking about the communication of pertinent information to stakeholders of organizations: employees, customers, business partners, shareholders, and so on.
And everywhere, our institutions are becoming naked. People are all bent out of shape about WikiLeaks, but that's just the tip of the iceberg. You see, people at their fingertips now, everybody, not just Julian Assange, have these powerful tools for finding out what's going on, scrutinizing, informing others, and even organizing collective responses. Institutions are becoming naked, and if you're going to be naked, well, there's some corollaries that flow from that. I mean, one is, fitness is no longer optional. (Laughter) You know? Or if you're going to be naked, you'd better get buff.
Now, by buff I mean, you need to have good value, because value is evidenced like never before. You say you have good products. They'd better be good. But you also need to have values. You need to have integrity as part of your bones and your DNA as an organization, because if you don't, you'll be unable to build trust, and trust is a sine equation of this new network world.
So this is good. It's not bad. Sunlight is the best disinfectant. And we need a lot of sunlight in this troubled world.
Now, the third meaning and corresponding principle of openness is about sharing. Now this is different than transparency. Transparency is about the communication of information. Sharing is about giving up assets, intellectual property.
And there are all kinds of famous stories about this. IBM gave away 400 million dollars of software to the Linux movement, and that gave them a multi-billion dollar payoff.
Now, conventional wisdom says, "Well, hey, our intellectual property belongs to us, and if someone tries to infringe it, we're going to get out our lawyers and we're going to sue them." Well, it didn't work so well for the record labels, did it? I mean, they took — They had a technology disruption, and rather than taking a business model innovation to correspond to that, they took and sought a legal solution and the industry that brought you Elvis and the Beatles is now suing children and is in danger of collapse.
So we need to think differently about intellectual property.
I'll give you an example. The pharmaceutical industry is in deep trouble. First of all, there aren't a lot of big inventions in the pipeline, and this is a big problem for human health, and the pharmaceutical industry has got a bigger problem, that they're about to fall off something called the patent cliff. Do you know about this? They're going to lose 20 to 35 percent of their revenue in the next 12 months. And what are you going to do, like, cut back on paper clips or something? No.
We need to reinvent the whole model of scientific research. The pharmaceutical industry needs to place assets in a commons. They need to start sharing precompetitive research. They need to start sharing clinical trial data, and in doing so, create a rising tide that could lift all boats, not just for the industry but for humanity.
Now, the fourth meaning of openness, and corresponding principle, is about empowerment. And I'm not talking about the motherhood sense here. Knowledge and intelligence is power, and as it becomes more distributed, there's a concomitant distribution and decentralization and disaggregation of power that's underway in the world today. The open world is bringing freedom.
Now, take the Arab Spring. The debate about the role of social media and social change has been settled. You know, one word: Tunisia. And then it ended up having a whole bunch of other words too. But in the Tunisian revolution, the new media didn't cause the revolution; it was caused by injustice. Social media didn't create the revolution; it was created by a new generation of young people who wanted jobs and hope and who didn't want to be treated as subjects anymore.
But just as the Internet drops transaction and collaboration costs in business and government, it also drops the cost of dissent, of rebellion, and even insurrection in ways that people didn't understand.
You know, during the Tunisian revolution, snipers associated with the regime were killing unarmed students in the street. So the students would take their mobile devices, take a picture, triangulate the location, send that picture to friendly military units, who'd come in and take out the snipers. You think that social media is about hooking up online? For these kids, it was a military tool to defend unarmed people from murderers. It was a tool of self-defense.
You know, as we speak today, young people are being killed in Syria, and up until three months ago, if you were injured on the street, an ambulance would pick you up, take you to the hospital, you'd go in, say, with a broken leg, and you'd come out with a bullet in your head.
So these 20-somethings created an alternative health care system, where what they did is they used Twitter and basic publicly available tools that when someone's injured, a car would show up, it would pick them up, take them to a makeshift medical clinic, where you'd get medical treatment, as opposed to being executed. So this is a time of great change.
Now, it's not without its problems. Up until two years ago, all revolutions in human history had a leadership, and when the old regime fell, the leadership and the organization would take power. Well, these wiki revolutions happen so fast they create a vacuum, and politics abhors a vacuum, and unsavory forces can fill that, typically the old regime, or extremists, or fundamentalist forces. You can see this playing out today in Egypt.
But that doesn't matter, because this is moving forward. The train has left the station. The cat is out of the bag. The horse is out of the barn. Help me out here, okay? (Laughter) The toothpaste is out of the tube. I mean, we're not putting this one back. The open world is bringing empowerment and freedom.
I think, at the end of these four days, that you'll come to conclude that the arc of history is a positive one, and it's towards openness.
If you go back a few hundred years, all around the world it was a very closed society. It was agrarian, and the means of production and political system was called feudalism, and knowledge was concentrated in the church and the nobility. People didn't know about things. There was no concept of progress. You were born, you lived your life and you died.
But then Johannes Gutenberg came along with his great invention, and, over time, the society opened up. People started to learn about things, and when they did, the institutions of feudal society appeared to be stalled, or frozen, or failing. It didn't make sense for the church to be responsible for medicine when people had knowledge.
So we saw the Protestant Reformation. Martin Luther called the printing press "God's highest act of grace." The creation of a corporation, science, the university, eventually the Industrial Revolution, and it was all good.
But it came with a cost.
And now, once again, the technology genie is out of the bottle, but this time it's different. The printing press gave us access to the written word. The Internet enables each of us to be a producer. The printing press gave us access to recorded knowledge. The Internet gives us access, not just to information and knowledge, but to the intelligence contained in the crania of other people on a global basis.
To me, this is not an information age, it's an age of networked intelligence. It's an age of vast promise, an age of collaboration, where the boundaries of our organizations are changing, of transparency, where sunlight is disinfecting civilization, an age of sharing and understanding the new power of the commons, and it's an age of empowerment and of freedom.
Now, what I'd like to do is, to close, to share with you some research that I've been doing. I've tried to study all kinds of organizations to understand what the future might look like, but I've been studying nature recently.
You know, bees come in swarms and fish come in schools. Starlings, in the area around Edinburgh, in the moors of England, come in something called a murmuration, and the murmuration refers to the murmuring of the wings of the birds, and throughout the day the starlings are out over a 20-mile radius sort of doing their starling thing. And at night they come together and they create one of the most spectacular things in all of nature, and it's called a murmuration. And scientists that have studied this have said they've never seen an accident. Now, this thing has a function. It protects the birds. You can see on the right here, there's a predator being chased away by the collective power of the birds, and apparently this is a frightening thing if you're a predator of starlings. And there's leadership, but there's no one leader.
Now, is this some kind of fanciful analogy, or could we actually learn something from this? Well, the murmuration functions to record a number of principles, and they're basically the principles that I have described to you today. This is a huge collaboration. It's an openness, it's a sharing of all kinds of information, not just about location and trajectory and danger and so on, but about food sources. And there's a real sense of interdependence, that the individual birds somehow understand that their interests are in the interest of the collective.
Perhaps like we should understand that business can't succeed in a world that's failing.
Well, I look at this thing, and I get a lot of hope. Think about the kids today in the Arab Spring, and you see something like this that's underway.
And imagine, just consider this idea, if you would: What if we could connect ourselves in this world through a vast network of air and glass? Could we go beyond just sharing information and knowledge? Could we start to share our intelligence? Could we create some kind of collective intelligence that goes beyond an individual or a group or a team to create, perhaps, some kind of consciousness on a global basis? Well, if we could do this, we could attack some big problems in the world.
And I look at this thing, and, I don't know, I get a lot of hope that maybe this smaller, networked, open world that our kids inherit might be a better one, and that this new age of networked intelligence could be an age of promise fulfilled and of peril unrequited.
Have you ever asked yourselves why it is that companies, the really cool companies, the innovative ones, the creative, new economy-type companies -- Apple, Google, Facebook -- are coming out of one particular country, the United States of America? Usually when I say this, someone says, "Spotify! That's Europe." But, yeah. It has not had the impact that these other companies have had.
Now what I do is I'm an economist, and I actually study the relationship between innovation and economic growth at the level of the company, the industry and the nation, and I work with policymakers worldwide, especially in the European Commission, but recently also in interesting places like China, and I can tell you that that question is on the tip of all of their tongues: Where are the European Googles? What is the secret behind the Silicon Valley growth model, which they understand is different from this old economy growth model? And what is interesting is that often, even if we're in the 21st century, we kind of come down in the end to these ideas of market versus state. It's talked about in these modern ways, but the idea is that somehow, behind places like Silicon Valley, the secret have been different types of market-making mechanisms, the private initiative, whether this be about a dynamic venture capital sector that's actually able to provide that high-risk finance to these innovative companies, the gazelles as we often call them, which traditional banks are scared of, or different types of really successful commercialization policies which actually allow these companies to bring these great inventions, their products, to the market and actually get over this really scary Death Valley period in which many companies instead fail.
But what really interests me, especially nowadays and because of what's happening politically around the world, is the language that's used, the narrative, the discourse, the images, the actual words. So we often are presented with the kind of words like that the private sector is also much more innovative because it's able to think out of the box. They are more dynamic. Think of Steve Jobs' really inspirational speech to the 2005 graduating class at Stanford, where he said to be innovative, you've got to stay hungry, stay foolish. Right? So these guys are kind of the hungry and foolish and colorful guys, right? And in places like Europe, it might be more equitable, we might even be a bit better dressed and eat better than the U.S., but the problem is this damn public sector. It's a bit too big, and it hasn't actually allowed these things like dynamic venture capital and commercialization to actually be able to really be as fruitful as it could. And even really respectable newspapers, some that I'm actually subscribed to, the words they use are, you know, the state as this Leviathan. Right? This monster with big tentacles. They're very explicit in these editorials. They say, "You know, the state, it's necessary to fix these little market failures when you have public goods or different types of negative externalities like pollution, but you know what, what is the next big revolution going to be after the Internet? We all hope it might be something green, or all of this nanotech stuff, and in order for that stuff to happen," they say -- this was a special issue on the next industrial revolution -- they say, "the state, just stick to the basics, right? Fund the infrastructure. Fund the schools. Even fund the basic research, because this is popularly recognized, in fact, as a big public good which private companies don't want to invest in, do that, but you know what? Leave the rest to the revolutionaries." Those colorful, out-of-the-box kind of thinkers. They're often called garage tinkerers, because some of them actually did some things in garages, even though that's partly a myth. And so what I want to do with you in, oh God, only 10 minutes, is to really think again this juxtaposition, because it actually has massive, massive implications beyond innovation policy, which just happens to be the area that I often talk with with policymakers. It has huge implications, even with this whole notion that we have on where, when and why we should actually be cutting back on public spending and different types of public services which, of course, as we know, are increasingly being outsourced because of this juxtaposition. Right? I mean, the reason that we need to maybe have free schools or charter schools is in order to make them more innovative without being emburdened by this heavy hand of the state curriculum, or something. So these kind of words are constantly, these juxtapositions come up everywhere, not just with innovation policy.
And so to think again, there's no reason that you should believe me, so just think of some of the smartest revolutionary things that you have in your pockets and do not turn it on, but you might want to take it out, your iPhone. Ask who actually funded the really cool, revolutionary thinking-out-of-the-box things in the iPhone. What actually makes your phone a smartphone, basically, instead of a stupid phone? So the Internet, which you can surf the web anywhere you are in the world; GPS, where you can actually know where you are anywhere in the world; the touchscreen display, which makes it also a really easy-to-use phone for anybody. These are the very smart, revolutionary bits about the iPhone, and they're all government-funded. And the point is that the Internet was funded by DARPA, U.S. Department of Defense. GPS was funded by the military's Navstar program. Even Siri was actually funded by DARPA. The touchscreen display was funded by two public grants by the CIA and the NSF to two public university researchers at the University of Delaware. Now, you might be thinking, "Well, she's just said the word 'defense' and 'military' an awful lot," but what's really interesting is that this is actually true in sector after sector and department after department. So the pharmaceutical industry, which I am personally very interested in because I've actually had the fortune to study it in quite some depth, is wonderful to be asking this question about the revolutionary versus non-revolutionary bits, because each and every medicine can actually be divided up on whether it really is revolutionary or incremental. So the new molecular entities with priority rating are the revolutionary new drugs, whereas the slight variations of existing drugs -- Viagra, different color, different dosage -- are the less revolutionary ones. And it turns out that a full 75 percent of the new molecular entities with priority rating are actually funded in boring, Kafka-ian public sector labs. This doesn't mean that Big Pharma is not spending on innovation. They do. They spend on the marketing part. They spend on the D part of R&D. They spend an awful lot on buying back their stock, which is quite problematic. In fact, companies like Pfizer and Amgen recently have spent more money in buying back their shares to boost their stock price than on R&D, but that's a whole different TED Talk which one day I'd be fascinated to tell you about.
Now, what's interesting in all of this is the state, in all these examples, was doing so much more than just fixing market failures. It was actually shaping and creating markets. It was funding not only the basic research, which again is a typical public good, but even the applied research. It was even, God forbid, being a venture capitalist. So these SBIR and SDTR programs, which give small companies early-stage finance have not only been extremely important compared to private venture capital, but also have become increasingly important. Why? Because, as many of us know, V.C. is actually quite short-term. They want their returns in three to five years. Innovation takes a much longer time than that, 15 to 20 years. And so this whole notion -- I mean, this is the point, right? Who's actually funding the hard stuff? Of course, it's not just the state. The private sector does a lot. But the narrative that we've always been told is the state is important for the basics, but not really providing that sort of high-risk, revolutionary thinking out of the box. In all these sectors, from funding the Internet to doing the spending, but also the envisioning, the strategic vision, for these investments, it was actually coming within the state. The nanotechnology sector is actually fascinating to study this, because the word itself, nanotechnology, came from within government.
And so there's huge implications of this. First of all, of course I'm not someone, this old-fashioned person, market versus state. What we all know in dynamic capitalism is that what we actually need are public-private partnerships. But the point is, by constantly depicting the state part as necessary but actually -- pffff -- a bit boring and often a bit dangerous kind of Leviathan, I think we've actually really stunted the possibility to build these public-private partnerships in a really dynamic way. Even the words that we often use to justify the "P" part, the public part -- well, they're both P's -- with public-private partnerships is in terms of de-risking. What the public sector did in all these examples I just gave you, and there's many more, which myself and other colleagues have been looking at, is doing much more than de-risking. It's kind of been taking on that risk. Bring it on. It's actually been the one thinking out of the box. But also, I'm sure you all have had experience with local, regional, national governments, and you're kind of like, "You know what, that Kafka-ian bureaucrat, I've met him." That whole juxtaposition thing, it's kind of there. Well, there's a self-fulfilling prophecy. By talking about the state as kind of irrelevant, boring, it's sometimes that we actually create those organizations in that way. So what we have to actually do is build these entrepreneurial state organizations. DARPA, that funded the Internet and Siri, actually thought really hard about this, how to welcome failure, because you will fail. You will fail when you innovative. One out of 10 experiments has any success. And the V.C. guys know this, and they're able to actually fund the other losses from that one success.
And this brings me, actually, probably, to the biggest implication, and this has huge implications beyond innovation. If the state is more than just a market fixer, if it actually is a market shaper, and in doing that has had to take on this massive risk, what happened to the reward? We all know, if you've ever taken a finance course, the first thing you're taught is sort of the risk-reward relationship, and so some people are foolish enough or probably smart enough if they have time to wait, to actually invest in stocks, because they're higher risk which over time will make a greater reward than bonds, that whole risk-reward thing. Well, where's the reward for the state of having taken on these massive risks and actually been foolish enough to have done the Internet? The Internet was crazy. It really was. I mean, the probability of failure was massive. You had to be completely nuts to do it, and luckily, they were. Now, we don't even get to this question about rewards unless you actually depict the state as this risk-taker. And the problem is that economists often think, well, there is a reward back to the state. It's tax. You know, the companies will pay tax, the jobs they create will create growth so people who get those jobs and their incomes rise will come back to the state through the tax mechanism. Well, unfortunately, that's not true. Okay, it's not true because many of the jobs that are created go abroad. Globalization, and that's fine. We shouldn't be nationalistic. Let the jobs go where they have to go, perhaps. I mean, one can take a position on that. But also these companies that have actually had this massive benefit from the state -- Apple's a great example. They even got the first -- well, not the first, but 500,000 dollars actually went to Apple, the company, through this SBIC program, which predated the SBIR program, as well as, as I said before, all the technologies behind the iPhone. And yet we know they legally, as many other companies, pay very little tax back.
So what we really need to actually rethink is should there perhaps be a return-generating mechanism that's much more direct than tax. Why not? It could happen perhaps through equity. This, by the way, in the countries that are actually thinking about this strategically, countries like Finland in Scandinavia, but also in China and Brazil, they're retaining equity in these investments. Sitra funded Nokia, kept equity, made a lot of money, it's a public funding agency in Finland, which then funded the next round of Nokias. The Brazilian Development Bank, which is providing huge amounts of funds today to clean technology, they just announced a $56 billion program for the future on this, is retaining equity in these investments. So to put it provocatively, had the U.S. government thought about this, and maybe just brought back just something called an innovation fund, you can bet that, you know, if even just .05 percent of the profits from what the Internet produced had come back to that innovation fund, there would be so much more money to spend today on green technology. Instead, many of the state budgets which in theory are trying to do that are being constrained. But perhaps even more important, we heard before about the one percent, the 99 percent. If the state is thought about in this more strategic way, as one of the lead players in the value creation mechanism, because that's what we're talking about, right? Who are the different players in creating value in the economy, and is the state's role, has it been sort of dismissed as being a backseat player? If we can actually have a broader theory of value creation and allow us to actually admit what the state has been doing and reap something back, it might just be that in the next round, and I hope that we all hope that the next big revolution will in fact be green, that that period of growth will not only be smart, innovation-led, not only green, but also more inclusive, so that the public schools in places like Silicon Valley can actually also benefit from that growth, because they have not.
I want to share some personal friends and stories with you that I've actually never talked about in public before to help illustrate the idea and the need and the hope for us to reinvent our health care system around the world. Twenty-four years ago, I had -- a sophomore in college, I had a series of fainting spells. No alcohol was involved. And I ended up in student health, and they ran some labwork and came back right away, and said, "Kidney problems." And before I knew it, I was involved and thrown into this six months of tests and trials and tribulations with six doctors across two hospitals in this clash of medical titans to figure out which one of them was right about what was wrong with me. And I'm sitting in a waiting room some time later for an ultrasound, and all six of these doctors actually show up in the room at once, and I'm like, "Uh oh, this is bad news." And their diagnosis was this: They said, "You have two rare kidney diseases that are going to actually destroy your kidneys eventually, you have cancer-like cells in your immune system that we need to start treatment right away, and you'll never be eligible for a kidney transplant, and you're not likely to live more than two or three years."
Now, with the gravity of this doomsday diagnosis, it just sucked me in immediately, as if I began preparing myself as a patient to die according to the schedule that they had just given to me, until I met a patient named Verna in a waiting room, who became a dear friend, and she grabbed me one day and took me off to the medical library and did a bunch of research on these diagnoses and these diseases, and said, "Eric, these people who get this are normally in their '70s and '80s. They don't know anything about you. Wake up. Take control of your health and get on with your life." And I did.
Now, these people making these proclamations to me were not bad people. In fact, these professionals were miracle workers, but they're working in a flawed, expensive system that's set up the wrong way. It's dependent on hospitals and clinics for our every care need. It's dependent on specialists who just look at parts of us. It's dependent on guesswork of diagnoses and drug cocktails, and so something either works or you die. And it's dependent on passive patients who just take it and don't ask any questions.
Now the problem with this model is that it's unsustainable globally. It's unaffordable globally. We need to invent what I call a personal health system. So what does this personal health system look like, and what new technologies and roles is it going to entail?
Now, I'm going to start by actually sharing with you a new friend of mine, Libby, somebody I've become quite attached to over the last six months. This is Libby, or actually, this is an ultrasound image of Libby. This is the kidney transplant I was never supposed to have. Now, this is an image that we shot a couple of weeks ago for today, and you'll notice, on the edge of this image, there's some dark spots there, which was really concerning to me. So we're going to actually do a live exam to sort of see how Libby's doing. This is not a wardrobe malfunction. I have to take my belt off here. Don't you in the front row worry or anything. (Laughter) I'm going to use a device from a company called Mobisante. This is a portable ultrasound. It can plug into a smartphone. It can plug into a tablet. Mobisante is up in Redmond, Washington, and they kindly trained me to actually do this on myself. They're not approved to do this. Patients are not approved to do this. This is a concept demo, so I want to make that clear. All right, I gotta gel up. Now the people in the front row are very nervous. (Laughter)
And I want to actually introduce you to Dr. Batiuk, who's another friend of mine. He's up in Legacy Good Samaritan Hospital in Portland, Oregon. So let me just make sure. Hey, Dr. Batiuk. Can you hear me okay? And actually, can you see Libby?
Thomas Batuik: Hi there, Eric. You look busy. How are you?
Eric Dishman: I'm good. I'm just taking my clothes off in front of a few hundred people. It's wonderful. So I just wanted to see, is this the image you need to get? And I know you want to look and see if those spots are still there.
TB: Okay. Well let's scan around a little bit here, give me a lay of the land.
ED: All right.TB: Okay. Turn it a little bit inside, a little bit toward the middle for me. Okay, that's good. How about up a little bit? Okay, freeze that image. That's a good one for me.
ED: All right. Now last week, when I did this, you had me measure that spot to the right. Should I do that again?
TB: Yeah, let's do that.
ED: All right. This is kind of hard to do with one hand on your belly and one hand on measuring, but I've got it, I think, and I'll save that image and send it to you. So tell me a little bit about what this dark spot means. It's not something I was very happy about.
TB: Many people after a kidney transplant will develop a little fluid collection around the kidney. Most of the time it doesn't create any kind of mischief, but it does warrant looking at, so I'm happy we've got an opportunity to look at it today, make sure that it's not growing, it's not creating any problems. Based on the other images we have, I'm really happy how it looks today.
ED: All right. Well, I guess we'll double check it when I come in. I've got my six month biopsy in a couple of weeks, and I'm going to let you do that in the clinic, because I don't think I can do that one on myself.
TB: Good choice.ED: All right, thanks, Dr. Batiuk. All right. So what you're sort of seeing here is an example of disruptive technologies, of mobile, social and analytic technologies. These are the foundations of what's going to make personal health possible.
Now there's really three pillars of this personal health I want to talk to you about now, and it's care anywhere, care networking and care customization. And you just saw a little bit of the first two with my interaction with Dr. Batiuk.
So let's start with care anywhere. Humans invented the idea of hospitals and clinics in the 1780s. It is time to update our thinking. We have got to untether clinicians and patients from the notion of traveling to a special bricks-and-mortar place for all of our care, because these places are often the wrong tool, and the most expensive tool, for the job. And these are sometimes unsafe places to send our sickest patients, especially in an era of superbugs and hospital-acquired infections. And many countries are going to go brickless from the start because they're never going to be able to afford the mega-medicalplexes that a lot of the rest of the world has built. Now I personally learned that hospitals can be a very dangerous place at a young age. This was me in third grade. I broke my elbow very seriously, had to have surgery, worried that they were going to actually lose the arm. Recovering from the surgery in the hospital, I get bedsores. Those bedsores become infected, and they give me an antibiotic which I end up being allergic to, and now my whole body breaks out, and now all of those become infected. The longer I stayed in the hospital, the sicker I became, and the more expensive it became, and this happens to millions of people around the world every year. The future of personal health that I'm talking about says care must occur at home as the default model, not in a hospital or clinic. You have to earn your way into those places by being sick enough to use that tool for the job. Now the smartphones that we're already carrying can clearly have diagnostic devices like ultrasounds plugged into them, and a whole array of others, today, and as sensing is built into these, we'll be able to do vital signs monitor and behavioral monitoring like we've never had before. Many of us will have implantables that will actually look real-time at what's going on with our blood chemistry and in our proteins right now. Now the software is also getting smarter, right? Think about a coach, an agent online, that's going to help me do safe self-care. That same interaction that we just did with the ultrasound will likely have real-time image processing, and the device will say, "Up, down, left, right, ah, Eric, that's the perfect spot to send that image off to your doctor."
Now, if we've got all these networked devices that are helping us to do care anywhere, it stands to reason that we also need a team to be able to interact with all of that stuff, and that leads to the second pillar I want to talk about, care networking. We have got to go beyond this paradigm of isolated specialists doing parts care to multidisciplinary teams doing person care. Uncoordinated care today is expensive at best, and it is deadly at worst. Eighty percent of medical errors are actually caused by communication and coordination problems amongst medical team members. I had my own heart scare years ago in graduate school, when we're under treatment for the kidney, and suddenly, they're like, "Oh, we think you have a heart problem." And I have these palpitations that are showing up. They put me through five weeks of tests -- very expensive, very scary -- before the nurse finally notices the piece of the paper, my meds list that I've been carrying to every single appointment, and says, "Oh my gosh." Three different specialists had prescribed three different versions of the same drug to me. I did not have a heart problem. I had an overdose problem. I had a care coordination problem. And this happens to millions of people every year. I want to use technology that we're all working on and making happen to make health care a coordinated team sport. Now this is the most frightening thing to me. Out of all the care I've had in hospitals and clinics around the world, the first time I've ever had a true team-based care experience was at Legacy Good Sam these last six months for me to go get this. And this is a picture of my graduation team from Legacy. There's a couple of the folks here. You'll recognize Dr. Batiuk. We just talked to him. Here's Jenny, one of the nurses, Allison, who helped manage the transplant list, and a dozen other people who aren't pictured, a pharmacist, a psychologist, a nutritionist, even a financial counselor, Lisa, who helped us deal with all the insurance hassles. I wept the day I graduated. I should have been happy, because I was so well that I could go back to my normal doctors, but I wept because I was so actually connected to this team.
And here's the most important part. The other people in this picture are me and my wife, Ashley. Legacy trained us on how to do care for me at home so that they could offload the hospitals and clinics. That's the only way that the model works. My team is actually working in China on one of these self-care models for a project we called Age-Friendly Cities. We're trying to help build a social network that can help track and train the care of seniors caring for themselves as well as the care provided by their family members or volunteer community health workers, as well as have an exchange network online, where, for example, I can donate three hours of care a day to your mom, if somebody else can help me with transportation to meals, and we exchange all of that online. The most important point I want to make to you about this is the sacred and somewhat over-romanticized doctor-patient one-on-one is a relic of the past. The future of health care is smart teams, and you'd better be on that team for yourself.
Now, the last thing that I want to talk to you about is care customization, because if you've got care anywhere and you've got care networking, those are going to go a long way towards improving our health care system, but there's still too much guesswork. Randomized clinical trials were actually invented in 1948 to help invent the drugs that cured tuberculosis, and those are important things, don't get me wrong. These population studies that we've done have created tons of miracle drugs that have saved millions of lives, but the problem is that health care is treating us as averages, not unique individuals, because at the end of the day, the patient is not the same thing as the population who are studied. That's what's leading to the guesswork. The technologies that are coming, high-performance computing, analytics, big data that everyone's talking about, will allow us to build predictive models for each of us as individual patients. And the magic here is, experiment on my avatar in software, not my body in suffering.
Now, I've had two examples I want to quickly share with you of this kind of care customization on my own journey. The first was quite simple. I finally realized some years ago that all my medical teams were optimizing my treatment for longevity. It's like a badge of honor to see how long they can get the patient to live. I was optimizing my life for quality of life, and quality of life for me means time in snow. So on my chart, I forced them to put, "Patient goal: low doses of drugs over longer periods of time, side effects friendly to skiing." And I think that's why I achieved longevity. I think that time-in-snow therapy was as important as the pharmaceuticals that I had. Now the second example of customization -- and by the way, you can't customize care if you don't know your own goals, so health care can't know those until you know your own health care goals. But the second example I want to give you is, I happened to be an early guinea pig, and I got very lucky to have my whole genome sequenced. Now it took about two weeks of processing on Intel's highest-end servers to make this happen, and another six months of human and computing labor to make sense of all of that data. And at the end of all of that, they said, "Yes, those diagnoses of that clash of medical titans all of those years ago were wrong, and we have a better path forward." The future that Intel's working on now is to figure out how to make that computing for personalized medicine go from months and weeks to even hours, and make this kind of tool available, not just in the mainframes of tier-one research hospitals around the world, but in the mainstream -- every patient, every clinic with access to whole genome sequencing. And I tell you, this kind of care customization for everything from your goals to your genetics will be the most game-changing transformation that we witness in health care during our lifetime.
So these three pillars of personal health, care anywhere, care networking, care customization, are happening in pieces now, but this vision will completely fail if we don't step up as caregivers and as patients to take on new roles. It's what my friend Verna said: Wake up and take control of your health. Because at the end of the day these technologies are simply about people caring for other people and ourselves in some powerful new ways.
And it's in that spirit that I want to introduce you to one last friend, very quickly. Tracey Gamley stepped up to give me the impossible kidney that I was never supposed to have.
So Tracey, just tell us a little bit quickly about what the donor experience was like with you.
Tracey Gamley: For me, it was really easy. I only had one night in the hospital. The surgery was done laparoscopically, so I have just five very small scars on my abdomen, and I had four weeks away from work and went back to doing everything I'd done before without any changes.
ED: Well, I probably will never get a chance to say this to you in such a large audience ever again. So "thank you" feel likes a really trite word, but thank you from the bottom of my heart for saving my life.
This TED stage and all of the TED stages are often about celebrating innovation and celebrating new technologies, and I've done that here today, and I've seen amazing things coming from TED speakers, I mean, my gosh, artificial kidneys, even printable kidneys, that are coming. But until such time that these amazing technologies are available to all of us, and even when they are, it's up to us to care for, and even save, one another. I hope you will go out and make personal health happen for yourselves and for everyone. Thanks so much.
So in 1885, Karl Benz invented the automobile. Later that year, he took it out for the first public test drive, and -- true story -- crashed into a wall. For the last 130 years, we've been working around that least reliable part of the car, the driver. We've made the car stronger. We've added seat belts, we've added air bags, and in the last decade, we've actually started trying to make the car smarter to fix that bug, the driver.
Now, today I'm going to talk to you a little bit about the difference between patching around the problem with driver assistance systems and actually having fully self-driving cars and what they can do for the world. I'm also going to talk to you a little bit about our car and allow you to see how it sees the world and how it reacts and what it does, but first I'm going to talk a little bit about the problem. And it's a big problem: 1.2 million people are killed on the world's roads every year. In America alone, 33,000 people are killed each year. To put that in perspective, that's the same as a 737 falling out of the sky every working day. It's kind of unbelievable. Cars are sold to us like this, but really, this is what driving's like. Right? It's not sunny, it's rainy, and you want to do anything other than drive. And the reason why is this: Traffic is getting worse. In America, between 1990 and 2010, the vehicle miles traveled increased by 38 percent. We grew by six percent of roads, so it's not in your brains. Traffic really is substantially worse than it was not very long ago.
And all of this has a very human cost. So if you take the average commute time in America, which is about 50 minutes, you multiply that by the 120 million workers we have, that turns out to be about six billion minutes wasted in commuting every day. Now, that's a big number, so let's put it in perspective. You take that six billion minutes and you divide it by the average life expectancy of a person, that turns out to be 162 lifetimes spent every day, wasted, just getting from A to B. It's unbelievable. And then, there are those of us who don't have the privilege of sitting in traffic. So this is Steve. He's an incredibly capable guy, but he just happens to be blind, and that means instead of a 30-minute drive to work in the morning, it's a two-hour ordeal of piecing together bits of public transit or asking friends and family for a ride. He doesn't have that same freedom that you and I have to get around. We should do something about that.
Now, conventional wisdom would say that we'll just take these driver assistance systems and we'll kind of push them and incrementally improve them, and over time, they'll turn into self-driving cars. Well, I'm here to tell you that's like me saying that if I work really hard at jumping, one day I'll be able to fly. We actually need to do something a little different. And so I'm going to talk to you about three different ways that self-driving systems are different than driver assistance systems. And I'm going to start with some of our own experience.
So back in 2013, we had the first test of a self-driving car where we let regular people use it. Well, almost regular -- they were 100 Googlers, but they weren't working on the project. And we gave them the car and we allowed them to use it in their daily lives. But unlike a real self-driving car, this one had a big asterisk with it: They had to pay attention, because this was an experimental vehicle. We tested it a lot, but it could still fail. And so we gave them two hours of training, we put them in the car, we let them use it, and what we heard back was something awesome, as someone trying to bring a product into the world. Every one of them told us they loved it. In fact, we had a Porsche driver who came in and told us on the first day, "This is completely stupid. What are we thinking?" But at the end of it, he said, "Not only should I have it, everyone else should have it, because people are terrible drivers." So this was music to our ears, but then we started to look at what the people inside the car were doing, and this was eye-opening. Now, my favorite story is this gentleman who looks down at his phone and realizes the battery is low, so he turns around like this in the car and digs around in his backpack, pulls out his laptop, puts it on the seat, goes in the back again, digs around, pulls out the charging cable for his phone, futzes around, puts it into the laptop, puts it on the phone. Sure enough, the phone is charging. All the time he's been doing 65 miles per hour down the freeway. Right? Unbelievable. So we thought about this and we said, it's kind of obvious, right? The better the technology gets, the less reliable the driver is going to get. So by just making the cars incrementally smarter, we're probably not going to see the wins we really need.
Let me talk about something a little technical for a moment here. So we're looking at this graph, and along the bottom is how often does the car apply the brakes when it shouldn't. You can ignore most of that axis, because if you're driving around town, and the car starts stopping randomly, you're never going to buy that car. And the vertical axis is how often the car is going to apply the brakes when it's supposed to to help you avoid an accident. Now, if we look at the bottom left corner here, this is your classic car. It doesn't apply the brakes for you, it doesn't do anything goofy, but it also doesn't get you out of an accident. Now, if we want to bring a driver assistance system into a car, say with collision mitigation braking, we're going to put some package of technology on there, and that's this curve, and it's going to have some operating properties, but it's never going to avoid all of the accidents, because it doesn't have that capability. But we'll pick some place along the curve here, and maybe it avoids half of accidents that the human driver misses, and that's amazing, right? We just reduced accidents on our roads by a factor of two. There are now 17,000 less people dying every year in America.
But if we want a self-driving car, we need a technology curve that looks like this. We're going to have to put more sensors in the vehicle, and we'll pick some operating point up here where it basically never gets into a crash. They'll happen, but very low frequency. Now you and I could look at this and we could argue about whether it's incremental, and I could say something like "80-20 rule," and it's really hard to move up to that new curve. But let's look at it from a different direction for a moment. So let's look at how often the technology has to do the right thing. And so this green dot up here is a driver assistance system. It turns out that human drivers make mistakes that lead to traffic accidents about once every 100,000 miles in America. In contrast, a self-driving system is probably making decisions about 10 times per second, so order of magnitude, that's about 1,000 times per mile. So if you compare the distance between these two, it's about 10 to the eighth, right? Eight orders of magnitude. That's like comparing how fast I run to the speed of light. It doesn't matter how hard I train, I'm never actually going to get there. So there's a pretty big gap there.
And then finally, there's how the system can handle uncertainty. So this pedestrian here might be stepping into the road, might not be. I can't tell, nor can any of our algorithms, but in the case of a driver assistance system, that means it can't take action, because again, if it presses the brakes unexpectedly, that's completely unacceptable. Whereas a self-driving system can look at that pedestrian and say, I don't know what they're about to do, slow down, take a better look, and then react appropriately after that.
So it can be much safer than a driver assistance system can ever be. So that's enough about the differences between the two. Let's spend some time talking about how the car sees the world.
So this is our vehicle. It starts by understanding where it is in the world, by taking a map and its sensor data and aligning the two, and then we layer on top of that what it sees in the moment. So here, all the purple boxes you can see are other vehicles on the road, and the red thing on the side over there is a cyclist, and up in the distance, if you look really closely, you can see some cones. Then we know where the car is in the moment, but we have to do better than that: we have to predict what's going to happen. So here the pickup truck in top right is about to make a left lane change because the road in front of it is closed, so it needs to get out of the way. Knowing that one pickup truck is great, but we really need to know what everybody's thinking, so it becomes quite a complicated problem. And then given that, we can figure out how the car should respond in the moment, so what trajectory it should follow, how quickly it should slow down or speed up. And then that all turns into just following a path: turning the steering wheel left or right, pressing the brake or gas. It's really just two numbers at the end of the day. So how hard can it really be?
Back when we started in 2009, this is what our system looked like. So you can see our car in the middle and the other boxes on the road, driving down the highway. The car needs to understand where it is and roughly where the other vehicles are. It's really a geometric understanding of the world. Once we started driving on neighborhood and city streets, the problem becomes a whole new level of difficulty. You see pedestrians crossing in front of us, cars crossing in front of us, going every which way, the traffic lights, crosswalks. It's an incredibly complicated problem by comparison. And then once you have that problem solved, the vehicle has to be able to deal with construction. So here are the cones on the left forcing it to drive to the right, but not just construction in isolation, of course. It has to deal with other people moving through that construction zone as well. And of course, if anyone's breaking the rules, the police are there and the car has to understand that that flashing light on the top of the car means that it's not just a car, it's actually a police officer. Similarly, the orange box on the side here, it's a school bus, and we have to treat that differently as well.
When we're out on the road, other people have expectations: So, when a cyclist puts up their arm, it means they're expecting the car to yield to them and make room for them to make a lane change. And when a police officer stood in the road, our vehicle should understand that this means stop, and when they signal to go, we should continue.
Now, the way we accomplish this is by sharing data between the vehicles. The first, most crude model of this is when one vehicle sees a construction zone, having another know about it so it can be in the correct lane to avoid some of the difficulty. But we actually have a much deeper understanding of this. We could take all of the data that the cars have seen over time, the hundreds of thousands of pedestrians, cyclists, and vehicles that have been out there and understand what they look like and use that to infer what other vehicles should look like and other pedestrians should look like. And then, even more importantly, we could take from that a model of how we expect them to move through the world. So here the yellow box is a pedestrian crossing in front of us. Here the blue box is a cyclist and we anticipate that they're going to nudge out and around the car to the right. Here there's a cyclist coming down the road and we know they're going to continue to drive down the shape of the road. Here somebody makes a right turn, and in a moment here, somebody's going to make a U-turn in front of us, and we can anticipate that behavior and respond safely.
Now, that's all well and good for things that we've seen, but of course, you encounter lots of things that you haven't seen in the world before. And so just a couple of months ago, our vehicles were driving through Mountain View, and this is what we encountered. This is a woman in an electric wheelchair chasing a duck in circles on the road. (Laughter) Now it turns out, there is nowhere in the DMV handbook that tells you how to deal with that, but our vehicles were able to encounter that, slow down, and drive safely. Now, we don't have to deal with just ducks. Watch this bird fly across in front of us. The car reacts to that. Here we're dealing with a cyclist that you would never expect to see anywhere other than Mountain View. And of course, we have to deal with drivers, even the very small ones. Watch to the right as someone jumps out of this truck at us. And now, watch the left as the car with the green box decides he needs to make a right turn at the last possible moment. Here, as we make a lane change, the car to our left decides it wants to as well. And here, we watch a car blow through a red light and yield to it. And similarly, here, a cyclist blowing through that light as well. And of course, the vehicle responds safely. And of course, we have people who do I don't know what sometimes on the road, like this guy pulling out between two self-driving cars. You have to ask, "What are you thinking?" (Laughter)
Now, I just fire-hosed you with a lot of stuff there, so I'm going to break one of these down pretty quickly. So what we're looking at is the scene with the cyclist again, and you might notice in the bottom, we can't actually see the cyclist yet, but the car can: it's that little blue box up there, and that comes from the laser data. And that's not actually really easy to understand, so what I'm going to do is I'm going to turn that laser data and look at it, and if you're really good at looking at laser data, you can see a few dots on the curve there, right there, and that blue box is that cyclist. Now as our light is red, the cyclist's light has turned yellow already, and if you squint, you can see that in the imagery. But the cyclist, we see, is going to proceed through the intersection. Our light has now turned green, his is solidly red, and we now anticipate that this bike is going to come all the way across. Unfortunately the other drivers next to us were not paying as much attention. They started to pull forward, and fortunately for everyone, this cyclists reacts, avoids, and makes it through the intersection. And off we go.
Now, as you can see, we've made some pretty exciting progress, and at this point we're pretty convinced this technology is going to come to market. We do three million miles of testing in our simulators every single day, so you can imagine the experience that our vehicles have. We are looking forward to having this technology on the road, and we think the right path is to go through the self-driving rather than driver assistance approach because the urgency is so large. In the time I have given this talk today, 34 people have died on America's roads.
How soon can we bring it out? Well, it's hard to say because it's a really complicated problem, but these are my two boys. My oldest son is 11, and that means in four and a half years, he's going to be able to get his driver's license. My team and I are committed to making sure that doesn't happen.
 (Laughter) (Applause) Chris Anderson: Chris, I've got a question for you.
Chris Urmson: Sure.
CA: So certainly, the mind of your cars is pretty mind-boggling. On this debate between driver-assisted and fully driverless -- I mean, there's a real debate going on out there right now. So some of the companies, for example, Tesla, are going the driver-assisted route. What you're saying is that that's kind of going to be a dead end because you can't just keep improving that route and get to fully driverless at some point, and then a driver is going to say, "This feels safe," and climb into the back, and something ugly will happen.
CU: Right. No, that's exactly right, and it's not to say that the driver assistance systems aren't going to be incredibly valuable. They can save a lot of lives in the interim, but to see the transformative opportunity to help someone like Steve get around, to really get to the end case in safety, to have the opportunity to change our cities and move parking out and get rid of these urban craters we call parking lots, it's the only way to go.
CA: We will be tracking your progress with huge interest. Thanks so much, Chris. CU: Thank you.
I have to say that I'm very glad to be here. I understand we have over 80 countries here, so that's a whole new paradigm for me to speak to all of these countries.
In each country, I'm sure you have this thing called the parent-teacher conference. Do you know about the parent-teacher conference? Not the ones for your kids, but the one you had as a child, where your parents come to school and your teacher talks to your parents, and it's a little bit awkward. Well, I remember in third grade, I had this moment where my father, who never takes off from work, he's a classical blue collar, a working-class immigrant person, going to school to see his son, how he's doing, and the teacher said to him, he said, "You know, John is good at math and art." And he kind of nodded, you know? The next day I saw him talking to a customer at our tofu store, and he said, "You know, John's good at math." 
And that always stuck with me all my life. Why didn't Dad say art? Why wasn't it okay? Why? It became a question my entire life, and that's all right, because being good at math meant he bought me a computer, and some of you remember this computer, this was my first computer. Who had an Apple II? Apple II users, very cool. (Applause) As you remember, the Apple II did nothing at all. (Laughter) You'd plug it in, you'd type in it and green text would come out. It would say you're wrong most of the time. That was the computer we knew. That computer is a computer that I learned about going to MIT, my father's dream. And at MIT, however, I learned about the computer at all levels, and after, I went to art school to get away from computers, and I began to think about the computer as more of a spiritual space of thinking. And I was influenced by performance art -- so this is 20 years ago. I made a computer out of people. It was called the Human Powered Computer Experiment. I have a power manager, mouse driver, memory, etc., and I built this in Kyoto, the old capital of Japan. It's a room broken in two halves. I've turned the computer on, and these assistants are placing a giant floppy disk built out of cardboard, and it's put into the computer. And the floppy disk drive person wears it. (Laughter) She finds the first sector on the disk, and takes data off the disk and passes it off to, of course, the bus. So the bus diligently carries the data into the computer to the memory, to the CPU, the VRAM, etc., and it's an actual working computer. That's a bus, really. (Laughter) And it looks kind of fast. That's a mouse driver, where it's XY. (Laughter) It looks like it's happening kind of quickly, but it's actually a very slow computer, and when I realized how slow this computer was compared to how fast a computer is, it made me wonder about computers and technology in general.
And so I'm going to talk today about four things, really. The first three things are about how I've been curious about technology, design and art, and how they intersect, how they overlap, and also a topic that I've taken on since four years ago I became the President of Rhode Island School of Design: leadership. And I'll talk about how I've looked to combine these four areas into a kind of a synthesis, a kind of experiment.
So starting from technology, technology is a wonderful thing. When that Apple II came out, it really could do nothing. It could show text and after we waited a bit, we had these things called images. Remember when images were first possible with a computer, those gorgeous, full-color images? And then after a few years, we got CD-quality sound. It was incredible. You could listen to sound on the computer. And then movies, via CD-ROM. It was amazing. Remember that excitement? And then the browser appeared. The browser was great, but the browser was very primitive, very narrow bandwidth. Text first, then images, we waited, CD-quality sound over the Net, then movies over the Internet. Kind of incredible. And then the mobile phone occurred, text, images, audio, video. And now we have iPhone, iPad, Android, with text, video, audio, etc. You see this little pattern here? We're kind of stuck in a loop, perhaps, and this sense of possibility from computing is something I've been questioning for the last 10 or so years, and have looked to design, as we understand most things, and to understand design with our technology has been a passion of mine. And I have a small experiment to give you a quick design lesson.
Designers talk about the relationship between form and content, content and form. Now what does that mean? Well, content is the word up there: fear. It's a four-letter word. It's a kind of a bad feeling word, fear. Fear is set in Light Helvetica, so it's not too stressful, and if you set it in Ultra Light Helvetica, it's like, "Oh, fear, who cares?" Right? (Laughter) You take the same Ultra Light Helvetica and make it big, and like, whoa, that hurts. Fear. So you can see how you change the scale, you change the form. Content is the same, but you feel differently. You change the typeface to, like, this typeface, and it's kind of funny. It's like pirate typeface, like Captain Jack Sparrow typeface. Arr! Fear! Like, aww, that's not fearful. That's actually funny. Or fear like this, kind of a nightclub typeface. (Laughter) Like, we gotta go to Fear. (Laughter) It's, like, amazing, right? (Laughter) (Applause) It just changes the same content. Or you make it -- The letters are separated apart, they're huddled together like on the deck of the Titanic, and you feel sorry for the letters, like, I feel the fear. You feel for them. Or you change the typeface to something like this. It's very classy. It's like that expensive restaurant, Fear. I can never get in there. (Laughter) It's just amazing, Fear. But that's form, content.
If you just change one letter in that content, you get a much better word, much better content: free. "Free" is a great word. You can serve it almost any way. Free bold feels like Mandela free. It's like, yes, I can be free. Free even light feels kind of like, ah, I can breathe in free. It feels great. Or even free spread out, it's like, ah, I can breathe in free, so easily. And I can add in a blue gradient and a dove, and I have, like, Don Draper free. (Laughter) So you see that -- form, content, design, it works that way. It's a powerful thing. It's like magic, almost, like the magicians we've seen at TED. It's magic. Design does that.
And I've been curious about how design and technology intersect, and I'm going to show you some old work I never really show anymore, to give you a sense of what I used to do. So -- yeah. So I made a lot of work in the '90s. This was a square that responds to sound. People ask me why I made that. It's not clear. (Laughter) But I thought it'd be neat for the square to respond to me, and my kids were small then, and my kids would play with these things, like, "Aaah," you know, they would say, "Daddy, aaah, aaah." You know, like that. We'd go to a computer store, and they'd do the same thing. And they'd say, "Daddy, why doesn't the computer respond to sound?" And it was really at the time I was wondering why doesn't the computer respond to sound? So I made this as a kind of an experiment at the time.
And then I spent a lot of time in the space of interactive graphics and things like this, and I stopped doing it because my students at MIT got so much better than myself, so I had to hang up my mouse. But in '96, I made my last piece. It was in black and white, monochrome, fully monochrome, all in integer mathematics. It's called "Tap, Type, Write." It's paying a tribute to the wonderful typewriter that my mother used to type on all the time as a legal secretary. It has 10 variations. (Typing noise) (Typing noise) There's a shift. Ten variations. This is, like, spin the letter around. (Typing noises) This is, like, a ring of letters. (Typing noises) This is 20 years old, so it's kind of a -- Let's see, this is — I love the French film "The Red Balloon." Great movie, right? I love that movie. So, this is sort of like a play on that. (Typing noises) (Typewriter bell) It's peaceful, like that. (Laughter) I'll show this last one. This is about balance, you know. It's kind of stressful typing out, so if you type on this keyboard, you can, like, balance it out. (Laughter) If you hit G, life's okay, so I always say, "Hit G, and it's going to be all right. Thank you. 
So that was 20 years ago, and I was always on the periphery of art. By being President of RISD I've gone deep into art, and art is a wonderful thing, fine art, pure art. You know, when people say, "I don't get art. I don't get it at all." That means art is working, you know? It's like, art is supposed to be enigmatic, so when you say, like, "I don't get it," like, oh, that's great. (Laughter) Art does that, because art is about asking questions, questions that may not be answerable.
At RISD, we have this amazing facility called the Edna Lawrence Nature Lab. It has 80,000 samples of animal, bone, mineral, plants. You know, in Rhode Island, if an animal gets hit on the road, they call us up and we pick it up and stuff it.
And why do we have this facility? Because at RISD, you have to look at the actual animal, the object, to understand its volume, to perceive it. At RISD, you're not allowed to draw from an image. And many people ask me, John, couldn't you just digitize all this? Make it all digital? Wouldn't it be better? And I often say, well, there's something good to how things used to be done. There's something very different about it, something we should figure out what is good about how we did it, even in this new era. And I have a good friend, he's a new media artist named Tota Hasegawa. He's based in London, no, actually it's in Tokyo, but when he was based in London, he had a game with his wife. He would go to antique shops, and the game was as such: When we look at an antique we want, we'll ask the shopkeeper for the story behind the antique, and if it's a good story, we'll buy it. So they'd go to an antique shop, and they'd look at this cup, and they'd say, "Tell us about this cup." And the shopkeeper would say, "It's old." (Laughter) "Tell us more." "Oh, it's really old." (Laughter) And he saw, over and over, the antique's value was all about it being old. And as a new media artist, he reflected, and said, you know, I've spent my whole career making new media art. People say, "Wow, your art, what is it?" It's new media. And he realized, it isn't about old or new. It's about something in between. It isn't about "old," the dirt, "new," the cloud. It's about what is good. A combination of the cloud and the dirt is where the action is at. You see it in all interesting art today, in all interesting businesses today. How we combine those two together to make good is very interesting.
So art makes questions, and leadership is something that is asking a lot of questions. We aren't functioning so easily anymore. We aren't a simple authoritarian regime anymore. As an example of authoritarianism, I was in Russia one time traveling in St. Petersburg, at a national monument, and I saw this sign that says, "Do Not Walk On The Grass," and I thought, oh, I mean, I speak English, and you're trying to single me out. That's not fair. But I found a sign for Russian-speaking people, and it was the best sign ever to say no. It was like, "No swimming, no hiking, no anything." My favorite ones are "no plants." Why would you bring a plant to a national monument? I'm not sure. And also "no love." (Laughter) So that is authoritarianism. And what is that, structurally? It's a hierarchy. We all know that a hierarchy is how we run many systems today, but as we know, it's been disrupted. It is now a network instead of a perfect tree. It's a heterarchy instead of a hierarchy. And that's kind of awkward.
And so today, leaders are faced with how to lead differently, I believe. This is work I did with my colleague Becky Bermont on creative leadership. What can we learn from artists and designers for how to lead? Because in many senses, a regular leader loves to avoid mistakes. Someone who's creative actually loves to learn from mistakes. A traditional leader is always wanting to be right, whereas a creative leader hopes to be right. And this frame is important today, in this complex, ambiguous space, and artists and designers have a lot to teach us, I believe.
And I had a show in London recently where my friends invited me to come to London for four days to sit in a sandbox, and I said great. And so I sat in a sandbox for four days straight, six hours every day, six-minute appointments with anyone in London, and that was really bad. But I would listen to people, hear their issues, draw in the sand, try to figure things out, and it was kind of hard to figure out what I was doing. You know? It's all these one-on-one meetings for like four days. And it felt kind of like being president, actually. I was like, "Oh, this my job. President. I do a lot of meetings, you know?" And by the end of the experience, I realized why I was doing this. It's because leaders, what we do is we connect improbable connections and hope something will happen, and in that room I found so many connections between people across all of London, and so leadership, connecting people, is the great question today. Whether you're in the hierarchy or the heterarchy, it's a wonderful design challenge.
And one thing I've been doing is doing some research on systems that can combine technology and leadership with an art and design perspective. Let me show you something I haven't shown anywhere, actually. So what this is, is a kind of a sketch, an application sketch I wrote in Python. You know how there's Photoshop? This is called Powershop, and the way it works is imagine an organization. You know, the CEO isn't ever at the top. The CEO's at the center of the organization. There may be different subdivisions in the organization, and you might want to look into different areas. For instance, green are areas doing well, red are areas doing poorly. You know, how do you, as the leader, scan, connect, make things happen? So for instance, you might open up a distribution here and find the different subdivisions in there, and know that you know someone in Eco, over here, and these people here are in Eco, the people you might engage with as CEO, people going across the hierarchy. And part of the challenge of the CEO is to find connections across areas, and so you might look in R&D, and here you see one person who crosses the two areas of interest, and it's a person important to engage. So you might want to, for instance, get a heads-up display on how you're interacting with them. How many coffees do you have? How often are you calling them, emailing them? What is the tenor of their email? How is it working out? Leaders might be able to use these systems to better regulate how they work inside the heterarchy. You can also imagine using technology like from Luminoso, the guys from Cambridge who were looking at deep text analysis. What is the tenor of your communications?
So these kind of systems, I believe, are important. They're targeted social media systems around leaders. And I believe that this kind of perspective will only begin to grow as more leaders enter the space of art and design, because art and design lets you think like this, find different systems like this, and I've just begun thinking like this, so I'm glad to share that with you. So in closing, I want to thank all of you for your attention. Thanks very much.
What do you think of when I say the word "design"? You probably think of things like this, finely crafted objects that you can hold in your hand, or maybe logos and posters and maps that visually explain things, classic icons of timeless design. But I'm not here to talk about that kind of design. I want to talk about the kind that you probably use every day and may not give much thought to, designs that change all the time and that live inside your pocket. I'm talking about the design of digital experiences and specifically the design of systems that are so big that their scale can be hard to comprehend. Consider the fact that Google processes over one billion search queries every day, that every minute, over 100 hours of footage are uploaded to YouTube. That's more in a single day than all three major U.S. networks broadcast in the last five years combined. And Facebook transmitting the photos, messages and stories of over 1.23 billion people. That's almost half of the Internet population, and a sixth of humanity.
These are some of the products that I've helped design over the course of my career, and their scale is so massive that they've produced unprecedented design challenges. But what is really hard about designing at scale is this: It's hard in part because it requires a combination of two things, audacity and humility — audacity to believe that the thing that you're making is something that the entire world wants and needs, and humility to understand that as a designer, it's not about you or your portfolio, it's about the people that you're designing for, and how your work just might help them live better lives. Now, unfortunately, there's no school that offers the course Designing for Humanity 101. I and the other designers who work on these kinds of products have had to invent it as we go along, and we are teaching ourselves the emerging best practices of designing at scale, and today I'd like share some of the things that we've learned over the years.
Now, the first thing that you need to know about designing at scale is that the little things really matter. Here's a really good example of how a very tiny design element can make a big impact. The team at Facebook that manages the Facebook "Like" button decided that it needed to be redesigned. The button had kind of gotten out of sync with the evolution of our brand and it needed to be modernized. Now you might think, well, it's a tiny little button, it probably is a pretty straightforward, easy design assignment, but it wasn't. Turns out, there were all kinds of constraints for the design of this button. You had to work within specific height and width parameters. You had to be careful to make it work in a bunch of different languages, and be careful about using fancy gradients or borders because it has to degrade gracefully in old web browsers. The truth is, designing this tiny little button was a huge pain in the butt.
Now, this is the new version of the button, and the designer who led this project estimates that he spent over 280 hours redesigning this button over the course of months. Now, why would we spend so much time on something so small? It's because when you're designing at scale, there's no such thing as a small detail. This innocent little button is seen on average 22 billion times a day and on over 7.5 million websites. It's one of the single most viewed design elements ever created. Now that's a lot of pressure for a little button and the designer behind it, but with these kinds of products, you need to get even the tiny things right.
Now, the next thing that you need to understand is how to design with data. Now, when you're working on products like this, you have incredible amounts of information about how people are using your product that you can then use to influence your design decisions, but it's not just as simple as following the numbers. Let me give you an example so that you can understand what I mean. Facebook has had a tool for a long time that allowed people to report photos that may be in violation of our community standards, things like spam and abuse. And there were a ton of photos reported, but as it turns out, only a small percentage were actually in violation of those community standards. Most of them were just your typical party photo. Now, to give you a specific hypothetical example, let's say my friend Laura hypothetically uploads a picture of me from a drunken night of karaoke. This is purely hypothetical, I can assure you. (Laughter) Now, incidentally, you know how some people are kind of worried that their boss or employee is going to discover embarrassing photos of them on Facebook? Do you know how hard that is to avoid when you actually work at Facebook? So anyway, there are lots of these photos being erroneously reported as spam and abuse, and one of the engineers on the team had a hunch. He really thought there was something else going on and he was right, because when he looked through a bunch of the cases, he found that most of them were from people who were requesting the takedown of a photo of themselves. Now this was a scenario that the team never even took into account before. So they added a new feature that allowed people to message their friend to ask them to take the photo down. But it didn't work. Only 20 percent of people sent the message to their friend. So the team went back at it. They consulted with experts in conflict resolution. They even studied the universal principles of polite language, which I didn't even actually know existed until this research happened. And they found something really interesting. They had to go beyond just helping people ask their friend to take the photo down. They had to help people express to their friend how the photo made them feel.
Here's how the experience works today. So I find this hypothetical photo of myself, and it's not spam, it's not abuse, but I really wish it weren't on the site. So I report it and I say, "I'm in this photo and I don't like it," and then we dig deeper. Why don't you like this photo of yourself? And I select "It's embarrassing." And then I'm encouraged to message my friend, but here's the critical difference. I'm provided specific suggested language that helps me communicate to Laura how the photo makes me feel. Now the team found that this relatively small change had a huge impact. Before, only 20 percent of people were sending the message, and now 60 percent were, and surveys showed that people on both sides of the conversation felt better as a result. That same survey showed that 90 percent of your friends want to know if they've done something to upset you. Now I don't know who the other 10 percent are, but maybe that's where our "Unfriend" feature can come in handy.
So as you can see, these decisions are highly nuanced. Of course we use a lot of data to inform our decisions, but we also rely very heavily on iteration, research, testing, intuition, human empathy. It's both art and science. Now, sometimes the designers who work on these products are called "data-driven," which is a term that totally drives us bonkers. The fact is, it would be irresponsible of us not to rigorously test our designs when so many people are counting on us to get it right, but data analytics will never be a substitute for design intuition. Data can help you make a good design great, but it will never made a bad design good.
The next thing that you need to understand as a principle is that when you introduce change, you need to do it extraordinarily carefully. Now I often have joked that I spend almost as much time designing the introduction of change as I do the change itself, and I'm sure that we can all relate to that when something that we use a lot changes and then we have to adjust. The fact is, people can become very efficient at using bad design, and so even if the change is good for them in the long run, it's still incredibly frustrating when it happens, and this is particularly true with user-generated content platforms, because people can rightfully claim a sense of ownership. It is, after all, their content.
Now, years ago, when I was working at YouTube, we were looking for ways to encourage more people to rate videos, and it was interesting because when we looked into the data, we found that almost everyone was exclusively using the highest five-star rating, a handful of people were using the lowest one-star, and virtually no one was using two, three or four stars. So we decided to simplify into an up-down kind of voting binary model. It's going to be much easier for people to engage with. But people were very attached to the five-star rating system. Video creators really loved their ratings. Millions and millions of people were accustomed to the old design. So in order to help people prepare themselves for change and acclimate to the new design more quickly, we actually published the data graph sharing with the community the rationale for what we were going to do, and it even engaged the larger industry in a conversation, which resulted in my favorite TechCrunch headline of all time: "YouTube Comes to a 5-Star Realization: Its Ratings Are Useless."
Now, it's impossible to completely avoid change aversion when you're making changes to products that so many people use. Even though we tried to do all the right things, we still received our customary flood of video protests and angry emails and even a package that had to be scanned by security, but we have to remember people care intensely about this stuff, and it's because these products, this work, really, really matters to them.
Now, we know that we have to be careful about paying attention to the details, we have to be cognizant about how we use data in our design process, and we have to introduce change very, very carefully. Now, these things are all really useful. They're good best practices for designing at scale. But they don't mean anything if you don't understand something much more fundamental. You have to understand who you are designing for.
Now, when you set a goal to design for the entire human race, and you start to engage in that goal in earnest, at some point you run into the walls of the bubble that you're living in. Now, in San Francisco, we get a little miffed when we hit a dead cell zone because we can't use our phones to navigate to the new hipster coffee shop. But what if you had to drive four hours to charge your phone because you had no reliable source of electricity? What if you had no access to public libraries? What if your country had no free press? What would these products start to mean to you? This is what Google, YouTube and Facebook look like to most of the world, and it's what they'll look like to most of the next five billion people to come online. Designing for low-end cell phones is not glamorous design work, but if you want to design for the whole world, you have to design for where people are, and not where you are.
So how do we keep this big, big picture in mind? We try to travel outside of our bubble to see, hear and understand the people we're designing for. We use our products in non-English languages to make sure that they work just as well. And we try to use one of these phones from time to time to keep in touch with their reality.
So what does it mean to design at a global scale? It means difficult and sometimes exasperating work to try to improve and evolve products. Finding the audacity and the humility to do right by them can be pretty exhausting, and the humility part, it's a little tough on the design ego. Because these products are always changing, everything that I've designed in my career is pretty much gone, and everything that I will design will fade away. But here's what remains: the never-ending thrill of being a part of something that is so big, you can hardly get your head around it, and the promise that it just might change the world. 
When I was about three or four years old, I remember my mum reading a story to me and my two big brothers, and I remember putting up my hands to feel the page of the book, to feel the picture they were discussing.
And my mum said, "Darling, remember that you can't see and you can't feel the picture and you can't feel the print on the page."
And I thought to myself, "But that's what I want to do. I love stories. I want to read." Little did I know that I would be part of a technological revolution that would make that dream come true.
I was born premature by about 10 weeks, which resulted in my blindness, some 64 years ago. The condition is known as retrolental fibroplasia, and it's now very rare in the developed world. Little did I know, lying curled up in my prim baby humidicrib in 1948 that I'd been born at the right place and the right time, that I was in a country where I could participate in the technological revolution.
There are 37 million totally blind people on our planet, but those of us who've shared in the technological changes mainly come from North America, Europe, Japan and other developed parts of the world. Computers have changed the lives of us all in this room and around the world, but I think they've changed the lives of we blind people more than any other group. And so I want to tell you about the interaction between computer-based adaptive technology and the many volunteers who helped me over the years to become the person I am today. It's an interaction between volunteers, passionate inventors and technology, and it's a story that many other blind people could tell. But let me tell you a bit about it today.
When I was five, I went to school and I learned braille. It's an ingenious system of six dots that are punched into paper, and I can feel them with my fingers. In fact, I think they're putting up my grade six report. I don't know where Julian Morrow got that from. (Laughter) I was pretty good in reading, but religion and musical appreciation needed more work. (Laughter)
When you leave the opera house, you'll find there's braille signage in the lifts. Look for it. Have you noticed it? I do. I look for it all the time.
When I was at school, the books were transcribed by transcribers, voluntary people who punched one dot at a time so I'd have volumes to read, and that had been going on, mainly by women, since the late 19th century in this country, but it was the only way I could read. When I was in high school, I got my first Philips reel-to-reel tape recorder, and tape recorders became my sort of pre-computer medium of learning. I could have family and friends read me material, and I could then read it back as many times as I needed. And it brought me into contact with volunteers and helpers. For example, when I studied at graduate school at Queen's University in Canada, the prisoners at the Collins Bay jail agreed to help me. I gave them a tape recorder, and they read into it. As one of them said to me, "Ron, we ain't going anywhere at the moment."
But think of it. These men, who hadn't had the educational opportunities I'd had, helped me gain post-graduate qualifications in law by their dedicated help.
Well, I went back and became an academic at Melbourne's Monash University, and for those 25 years, tape recorders were everything to me. In fact, in my office in 1990, I had 18 miles of tape. Students, family and friends all read me material. Mrs. Lois Doery, whom I later came to call my surrogate mum, read me many thousands of hours onto tape. One of the reasons I agreed to give this talk today was that I was hoping that Lois would be here so I could introduce you to her and publicly thank her. But sadly, her health hasn't permitted her to come today. But I thank you here, Lois, from this platform.
I saw my first Apple computer in 1984, and I thought to myself, "This thing's got a glass screen, not much use to me." How very wrong I was. In 1987, in the month our eldest son Gerard was born, I got my first blind computer, and it's actually here. See it up there? And you see it has no, what do you call it, no screen. (Laughter) It's a blind computer. (Laughter) It's a Keynote Gold 84k, and the 84k stands for it had 84 kilobytes of memory. (Laughter) Don't laugh, it cost me 4,000 dollars at the time. (Laughter) I think there's more memory in my watch.
It was invented by Russell Smith, a passionate inventor in New Zealand who was trying to help blind people. Sadly, he died in a light plane crash in 2005, but his memory lives on in my heart. It meant, for the first time, I could read back what I had typed into it. It had a speech synthesizer. I'd written my first coauthored labor law book on a typewriter in 1979 purely from memory. This now allowed me to read back what I'd written and to enter the computer world, even with its 84k of memory.
In 1974, the great Ray Kurzweil, the American inventor, worked on building a machine that would scan books and read them out in synthetic speech. Optical character recognition units then only operated usually on one font, but by using charge-coupled device flatbed scanners and speech synthesizers, he developed a machine that could read any font. And his machine, which was as big as a washing machine, was launched on the 13th of January, 1976. I saw my first commercially available Kurzweil in March 1989, and it blew me away, and in September 1989, the month that my associate professorship at Monash University was announced, the law school got one, and I could use it. For the first time, I could read what I wanted to read by putting a book on the scanner. I didn't have to be nice to people!
I no longer would be censored. For example, I was too shy then, and I'm actually too shy now, to ask anybody to read me out loud sexually explicit material. (Laughter) But, you know, I could pop a book on in the middle of the night, and -- (Laughter) (Applause)
Now, the Kurzweil reader is simply a program on my laptop. That's what it's shrunk to. And now I can scan the latest novel and not wait to get it into talking book libraries. I can keep up with my friends.
There are many people who have helped me in my life, and many that I haven't met. One is another American inventor Ted Henter. Ted was a motorcycle racer, but in 1978 he had a car accident and lost his sight, which is devastating if you're trying to ride motorbikes. He then turned to being a waterskier and was a champion disabled waterskier. But in 1989, he teamed up with Bill Joyce to develop a program that would read out what was on the computer screen from the Net or from what was on the computer. It's called JAWS, Job Access With Speech, and it sounds like this.
Ron McCallum: Isn't that slow?
 (Laughter) You see, if I read like that, I'd fall asleep. I slowed it down for you. I'm going to ask that we play it at the speed I read it. Can we play that one?
RM: You know, when you're marking student essays, you want to get through them fairly quickly.
This technology that fascinated me in 1987 is now on my iPhone and on yours as well. But, you know, I find reading with machines a very lonely process. I grew up with family, friends, reading to me, and I loved the warmth and the breath and the closeness of people reading. Do you love being read to? And one of my most enduring memories is in 1999, Mary reading to me and the children down near Manly Beach "Harry Potter and the Philosopher's Stone." Isn't that a great book? I still love being close to someone reading to me. But I wouldn't give up the technology, because it's allowed me to lead a great life.
Of course, talking books for the blind predated all this technology. After all, the long-playing record was developed in the early 1930s, and now we put talking books on CDs using the digital access system known as DAISY. But when I'm reading with synthetic voices, I love to come home and read a racy novel with a real voice.
Now there are still barriers in front of we people with disabilities. Many websites we can't read using JAWS and the other technologies. Websites are often very visual, and there are all these sorts of graphs that aren't labeled and buttons that aren't labeled, and that's why the World Wide Web Consortium 3, known as W3C, has developed worldwide standards for the Internet. And we want all Internet users or Internet site owners to make their sites compatible so that we persons without vision can have a level playing field. There are other barriers brought about by our laws. For example, Australia, like about one third of the world's countries, has copyright exceptions which allow books to be brailled or read for we blind persons. But those books can't travel across borders. For example, in Spain, there are a 100,000 accessible books in Spanish. In Argentina, there are 50,000. In no other Latin American country are there more than a couple of thousand. But it's not legal to transport the books from Spain to Latin America. There are hundreds of thousands of accessible books in the United States, Britain, Canada, Australia, etc., but they can't be transported to the 60 countries in our world where English is the first and the second language. And remember I was telling you about Harry Potter. Well, because we can't transport books across borders, there had to be separate versions read in all the different English-speaking countries: Britain, United States, Canada, Australia, and New Zealand all had to have separate readings of Harry Potter.
And that's why, next month in Morocco, a meeting is taking place between all the countries. It's something that a group of countries and the World Blind Union are advocating, a cross-border treaty so that if books are available under a copyright exception and the other country has a copyright exception, we can transport those books across borders and give life to people, particularly in developing countries, blind people who don't have the books to read. I want that to happen.
My life has been extraordinarily blessed with marriage and children and certainly interesting work to do, whether it be at the University of Sydney Law School, where I served a term as dean, or now as I sit on the United Nations Committee on the Rights of Persons with Disabilities, in Geneva. I've indeed been a very fortunate human being.
I wonder what the future will hold. The technology will advance even further, but I can still remember my mum saying, 60 years ago, "Remember, darling, you'll never be able to read the print with your fingers." I'm so glad that the interaction between braille transcribers, volunteer readers and passionate inventors, has allowed this dream of reading to come true for me and for blind people throughout the world.
I'd like to thank my researcher Hannah Martin, who is my slide clicker, who clicks the slides, and my wife, Professor Mary Crock, who's the light of my life, is coming on to collect me. I want to thank her too.
I think I have to say goodbye now. Bless you. Thank you very much.
I'd like to take you on the epic quest of the Rosetta spacecraft. To escort and land the probe on a comet, this has been my passion for the past two years. In order to do that, I need to explain to you something about the origin of the solar system.
When we go back four and a half billion years, there was a cloud of gas and dust. In the center of this cloud, our sun formed and ignited. Along with that, what we now know as planets, comets and asteroids formed. What then happened, according to theory, is that when the Earth had cooled down a bit after its formation, comets massively impacted the Earth and delivered water to Earth. They probably also delivered complex organic material to Earth, and that may have bootstrapped the emergence of life. You can compare this to having to solve a 250-piece puzzle and not a 2,000-piece puzzle.
Afterwards, the big planets like Jupiter and Saturn, they were not in their place where they are now, and they interacted gravitationally, and they swept the whole interior of the solar system clean, and what we now know as comets ended up in something called the Kuiper Belt, which is a belt of objects beyond the orbit of Neptune. And sometimes these objects run into each other, and they gravitationally deflect, and then the gravity of Jupiter pulls them back into the solar system. And they then become the comets as we see them in the sky.
The important thing here to note is that in the meantime, the four and a half billion years, these comets have been sitting on the outside of the solar system, and haven't changed -- deep, frozen versions of our solar system.
In the sky, they look like this. We know them for their tails. There are actually two tails. One is a dust tail, which is blown away by the solar wind. The other one is an ion tail, which is charged particles, and they follow the magnetic field in the solar system. There's the coma, and then there is the nucleus, which here is too small to see, and you have to remember that in the case of Rosetta, the spacecraft is in that center pixel. We are only 20, 30, 40 kilometers away from the comet.
So what's important to remember? Comets contain the original material from which our solar system was formed, so they're ideal to study the components that were present at the time when Earth, and life, started. Comets are also suspected of having brought the elements which may have bootstrapped life. In 1983, ESA set up its long-term Horizon 2000 program, which contained one cornerstone, which would be a mission to a comet. In parallel, a small mission to a comet, what you see here, Giotto, was launched, and in 1986, flew by the comet of Halley with an armada of other spacecraft. From the results of that mission, it became immediately clear that comets were ideal bodies to study to understand our solar system. And thus, the Rosetta mission was approved in 1993, and originally it was supposed to be launched in 2003, but a problem arose with an Ariane rocket. However, our P.R. department, in its enthusiasm, had already made 1,000 Delft Blue plates with the name of the wrong comets. So I've never had to buy any china since. That's the positive part. (Laughter)
Once the whole problem was solved, we left Earth in 2004 to the newly selected comet, Churyumov-Gerasimenko. This comet had to be specially selected because A, you have to be able to get to it, and B, it shouldn't have been in the solar system too long. This particular comet has been in the solar system since 1959. That's the first time when it was deflected by Jupiter, and it got close enough to the sun to start changing. So it's a very fresh comet.
Rosetta made a few historic firsts. It's the first satellite to orbit a comet, and to escort it throughout its whole tour through the solar system -- closest approach to the sun, as we will see in August, and then away again to the exterior. It's the first ever landing on a comet. We actually orbit the comet using something which is not normally done with spacecraft. Normally, you look at the sky and you know where you point and where you are. In this case, that's not enough. We navigated by looking at landmarks on the comet. We recognized features -- boulders, craters -- and that's how we know where we are respective to the comet.
And, of course, it's the first satellite to go beyond the orbit of Jupiter on solar cells. Now, this sounds more heroic than it actually is, because the technology to use radio isotope thermal generators wasn't available in Europe at that time, so there was no choice. But these solar arrays are big. This is one wing, and these are not specially selected small people. They're just like you and me. (Laughter) We have two of these wings, 65 square meters. Now later on, of course, when we got to the comet, you find out that 65 square meters of sail close to a body which is outgassing is not always a very handy choice.
Now, how did we get to the comet? Because we had to go there for the Rosetta scientific objectives very far away -- four times the distance of the Earth to the sun -- and also at a much higher velocity than we could achieve with fuel, because we'd have to take six times as much fuel as the whole spacecraft weighed. So what do you do? You use gravitational flybys, slingshots, where you pass by a planet at very low altitude, a few thousand kilometers, and then you get the velocity of that planet around the sun for free. We did that a few times. We did Earth, we did Mars, we did twice Earth again, and we also flew by two asteroids, Lutetia and Steins. Then in 2011, we got so far from the sun that if the spacecraft got into trouble, we couldn't actually save the spacecraft anymore, so we went into hibernation. Everything was switched off except for one clock. Here you see in white the trajectory, and the way this works. You see that from the circle where we started, the white line, actually you get more and more and more elliptical, and then finally we approached the comet in May 2014, and we had to start doing the rendezvous maneuvers.
On the way there, we flew by Earth and we took a few pictures to test our cameras. This is the moon rising over Earth, and this is what we now call a selfie, which at that time, by the way, that word didn't exist. (Laughter) It's at Mars. It was taken by the CIVA camera. That's one of the cameras on the lander, and it just looks under the solar arrays, and you see the planet Mars and the solar array in the distance.
Now, when we got out of hibernation in January 2014, we started arriving at a distance of two million kilometers from the comet in May. However, the velocity the spacecraft had was much too fast. We were going 2,800 kilometers an hour faster than the comet, so we had to brake. We had to do eight maneuvers, and you see here, some of them were really big. We had to brake the first one by a few hundred kilometers per hour, and actually, the duration of that was seven hours, and it used 218 kilos of fuel, and those were seven nerve-wracking hours, because in 2007, there was a leak in the system of the propulsion of Rosetta, and we had to close off a branch, so the system was actually operating at a pressure which it was never designed or qualified for.
Then we got in the vicinity of the comet, and these were the first pictures we saw. The true comet rotation period is 12 and a half hours, so this is accelerated, but you will understand that our flight dynamics engineers thought, this is not going to be an easy thing to land on. We had hoped for some kind of spud-like thing where you could easily land. But we had one hope: maybe it was smooth. No. That didn't work either. (Laughter)
So at that point in time, it was clearly unavoidable: we had to map this body in all the detail you could get, because we had to find an area which is 500 meters in diameter and flat. Why 500 meters? That's the error we have on landing the probe. So we went through this process, and we mapped the comet. We used a technique called photoclinometry. You use shadows thrown by the sun. What you see here is a rock sitting on the surface of the comet, and the sun shines from above. From the shadow, we, with our brain, can immediately determine roughly what the shape of that rock is. You can program that in a computer, you then cover the whole comet, and you can map the comet. For that, we flew special trajectories starting in August. First, a triangle of 100 kilometers on a side at 100 kilometers' distance, and we repeated the whole thing at 50 kilometers. At that time, we had seen the comet at all kinds of angles, and we could use this technique to map the whole thing.
Now, this led to a selection of landing sites. This whole process we had to do, to go from the mapping of the comet to actually finding the final landing site, was 60 days. We didn't have more. To give you an idea, the average Mars mission takes hundreds of scientists for years to meet about where shall we go? We had 60 days, and that was it.
We finally selected the final landing site and the commands were prepared for Rosetta to launch Philae. The way this works is that Rosetta has to be at the right point in space, and aiming towards the comet, because the lander is passive. The lander is then pushed out and moves towards the comet. Rosetta had to turn around to get its cameras to actually look at Philae while it was departing and to be able to communicate with it.
Now, the landing duration of the whole trajectory was seven hours. Now do a simple calculation: if the velocity of Rosetta is off by one centimeter per second, seven hours is 25,000 seconds. That means 252 meters wrong on the comet. So we had to know the velocity of Rosetta much better than one centimeter per second, and its location in space better than 100 meters at 500 million kilometers from Earth. That's no mean feat.
Let me quickly take you through some of the science and the instruments. I won't bore you with all the details of all the instruments, but it's got everything. We can sniff gas, we can measure dust particles, the shape of them, the composition, there are magnetometers, everything. This is one of the results from an instrument which measures gas density at the position of Rosetta, so it's gas which has left the comet. The bottom graph is September of last year. There is a long-term variation, which in itself is not surprising, but you see the sharp peaks. This is a comet day. You can see the effect of the sun on the evaporation of gas and the fact that the comet is rotating. So there is one spot, apparently, where there is a lot of stuff coming from, it gets heated in the Sun, and then cools down on the back side. And we can see the density variations of this.
These are the gases and the organic compounds that we already have measured. You will see it's an impressive list, and there is much, much, much more to come, because there are more measurements. Actually, there is a conference going on in Houston at the moment where many of these results are presented.
Also, we measured dust particles. Now, for you, this will not look very impressive, but the scientists were thrilled when they saw this. Two dust particles: the right one they call Boris, and they shot it with tantalum in order to be able to analyze it. Now, we found sodium and magnesium. What this tells you is this is the concentration of these two materials at the time the solar system was formed, so we learned things about which materials were there when the planet was made.
Of course, one of the important elements is the imaging. This is one of the cameras of Rosetta, the OSIRIS camera, and this actually was the cover of Science magazine on January 23 of this year. Nobody had expected this body to look like this. Boulders, rocks -- if anything, it looks more like the Half Dome in Yosemite than anything else. We also saw things like this: dunes, and what look to be, on the righthand side, wind-blown shadows. Now we know these from Mars, but this comet doesn't have an atmosphere, so it's a bit difficult to create a wind-blown shadow. It may be local outgassing, stuff which goes up and comes back. We don't know, so there is a lot to investigate. Here, you see the same image twice. On the left-hand side, you see in the middle a pit. On the right-hand side, if you carefully look, there are three jets coming out of the bottom of that pit. So this is the activity of the comet. Apparently, at the bottom of these pits is where the active regions are, and where the material evaporates into space. There is a very intriguing crack in the neck of the comet. You see it on the right-hand side. It's a kilometer long, and it's two and a half meters wide. Some people suggest that actually, when we get close to the sun, the comet may split in two, and then we'll have to choose, which comet do we go for? The lander -- again, lots of instruments, mostly comparable except for the things which hammer in the ground and drill, etc. But much the same as Rosetta, and that is because you want to compare what you find in space with what you find on the comet. These are called ground truth measurements.
These are the landing descent images that were taken by the OSIRIS camera. You see the lander getting further and further away from Rosetta. On the top right, you see an image taken at 60 meters by the lander, 60 meters above the surface of the comet. The boulder there is some 10 meters. So this is one of the last images we took before we landed on the comet. Here, you see the whole sequence again, but from a different perspective, and you see three blown-ups from the bottom-left to the middle of the lander traveling over the surface of the comet. Then, at the top, there is a before and an after image of the landing. The only problem with the after image is, there is no lander. But if you carefully look at the right-hand side of this image, we saw the lander still there, but it had bounced. It had departed again.
Now, on a bit of a comical note here is that originally Rosetta was designed to have a lander which would bounce. That was discarded because it was way too expensive. Now, we forgot, but the lander knew. (Laughter) During the first bounce, in the magnetometers, you see here the data from them, from the three axes, x, y and z. Halfway through, you see a red line. At that red line, there is a change. What happened, apparently, is during the first bounce, somewhere, we hit the edge of a crater with one of the legs of the lander, and the rotation velocity of the lander changed. So we've been rather lucky that we are where we are.
This is one of the iconic images of Rosetta. It's a man-made object, a leg of the lander, standing on a comet. This, for me, is one of the very best images of space science I have ever seen.
One of the things we still have to do is to actually find the lander. The blue area here is where we know it must be. We haven't been able to find it yet, but the search is continuing, as are our efforts to start getting the lander to work again. We listen every day, and we hope that between now and somewhere in April, the lander will wake up again.
The findings of what we found on the comet: This thing would float in water. It's half the density of water. So it looks like a very big rock, but it's not. The activity increase we saw in June, July, August last year was a four-fold activity increase. By the time we will be at the sun, there will be 100 kilos a second leaving this comet: gas, dust, whatever. That's 100 million kilos a day.
Then, finally, the landing day. I will never forget -- absolute madness, 250 TV crews in Germany. The BBC was interviewing me, and another TV crew who was following me all day were filming me being interviewed, and it went on like that for the whole day. The Discovery Channel crew actually caught me when leaving the control room, and they asked the right question, and man, I got into tears, and I still feel this. For a month and a half, I couldn't think about landing day without crying, and I still have the emotion in me.
With this image of the comet, I would like to leave you.
I have the feeling that we can all agree that we're moving towards a new model of the state and society. But, we're absolutely clueless as to what this is or what it should be. It seems like we need to have a conversation about democracy in our day and age. Let's think about it this way: We are 21st-century citizens, doing our very, very best to interact with 19th century-designed institutions that are based on an information technology of the 15th century. Let's have a look at some of the characteristics of this system. First of all, it's designed for an information technology that's over 500 years old. And the best possible system that could be designed for it is one where the few make daily decisions in the name of the many. And the many get to vote once every couple of years. In the second place, the costs of participating in this system are incredibly high. You either have to have a fair bit of money and influence, or you have to devote your entire life to politics. You have to become a party member and slowly start working up the ranks until maybe, one day, you'll get to sit at a table where a decision is being made. And last but not least, the language of the system — it's incredibly cryptic. It's done for lawyers, by lawyers, and no one else can understand. So, it's a system where we can choose our authorities, but we are completely left out on how those authorities reach their decisions. So, in a day where a new information technology allows us to participate globally in any conversation, our barriers of information are completely lowered and we can, more than ever before, express our desires and our concerns. Our political system remains the same for the past 200 years and expects us to be contented with being simply passive recipients of a monologue. So, it's really not surprising that this kind of system is only able to produce two kinds of results: silence or noise. Silence, in terms of citizens not engaging, simply not wanting to participate. There's this commonplace [idea] that I truly, truly dislike, and it's this idea that we citizens are naturally apathetic. That we shun commitment. But, can you really blame us for not jumping at the opportunity of going to the middle of the city in the middle of a working day to attend, physically, a public hearing that has no impact whatsoever? Conflict is bound to happen between a system that no longer represents, nor has any dialogue capacity, and citizens that are increasingly used to representing themselves. And, then we find noise: Chile, Argentina, Brazil, Mexico Italy, France, Spain, the United States, they're all democracies. Their citizens have access to the ballot boxes. But they still feel the need, they need to take to the streets in order to be heard. To me, it seems like the 18th-century slogan that was the basis for the formation of our modern democracies, "No taxation without representation," can now be updated to "No representation without a conversation." We want our seat at the table.
And rightly so. But in order to be part of this conversation, we need to know what we want to do next, because political action is being able to move from agitation to construction. My generation has been incredibly good at using new networks and technologies to organize protests, protests that were able to successfully impose agendas, roll back extremely pernicious legislation, and even overthrow authoritarian governments. And we should be immensely proud of this. But, we also must admit that we haven't been good at using those same networks and technologies to successfully articulate an alternative to what we're seeing and find the consensus and build the alliances that are needed
to make it happen. And so the risk that we face is that we can create these huge power vacuums that will very quickly get filled up by de facto powers, like the military or highly motivated and already organized groups
that generally lie on the extremes. But our democracy is neither just a matter of voting once every couple of years. But it's not either the ability to bring millions onto the streets. So the question I'd like to raise here, and I do believe it's the most important question we need to answer, is this one: If Internet is the new printing press, then what is democracy for the Internet era? What institutions do we want to build for the 21st-century society? I don't have the answer, just in case. I don't think anyone does. But I truly believe we can't afford to ignore this question anymore. So, I'd like to share our experience and what we've learned so far and hopefully contribute two cents to this conversation. Two years ago, with a group of friends from Argentina, we started thinking, "how can we get our representatives, our elected representatives, to represent us?" Marshall McLuhan once said that politics is solving today's problems with yesterday's tools. So the question that motivated us was, can we try and solve some of today's problems with the tools that we use every single day of our lives? Our first approach was to design and develop a piece of software called DemocracyOS. DemocracyOS is an open-source web application that is designed to become a bridge between citizens and their elected representatives to make it easier for us to participate from our everyday lives. So first of all, you can get informed so every new project that gets introduced in Congress gets immediately translated and explained in plain language on this platform. But we all know that social change is not going to come from just knowing more information, but from doing something with it. So better access to information should lead to a conversation about what we're going to do next, and DemocracyOS allows for that. Because we believe that democracy is not just a matter of stacking up preferences, one on top of each other, but that our healthy and robust public debate should be, once again, one of its fundamental values. So DemocracyOS is about persuading and being persuaded. It's about reaching a consensus as much as finding a proper way of channeling our disagreement. And finally, you can vote how you would like your elected representative to vote. And if you do not feel comfortable voting on a certain issue, you can always delegate your vote to someone else, allowing for a dynamic and emerging social leadership. It suddenly became very easy for us to simply compare these results with how our representatives were voting in Congress. But, it also became very evident that technology was not going to do the trick. What we needed to do to was to find actors that were able to grab this distributed knowledge in society and use it to make better and more fair decisions. So we reached out to traditional political parties and we offered them DemocracyOS. We said, "Look, here you have a platform that you can use to build a two-way conversation with your constituencies." And yes, we failed. We failed big time. We were sent to play outside like little kids. Amongst other things, we were called naive. And I must be honest: I think, in hindsight, we were. Because the challenges that we face, they're not technological, they're cultural. Political parties were never willing to change the way they make their decisions. So it suddenly became a bit obvious that if we wanted to move forward with this idea, we needed to do it ourselves. And so we took quite a leap of faith, and in August last year, we founded our own political party, El Partido de la Red, or the Net Party, in the city of Buenos Aires. And taking an even bigger leap of faith, we ran for elections in October last year with this idea: if we want a seat in Congress, our candidate, our representatives were always going to vote according to what citizens decided on DemocracyOS. Every single project that got introduced in Congress, we were going vote according to what citizens decided on an online platform. It was our way of hacking the political system. We understood that if we wanted to become part of the conversation, to have a seat at the table, we needed to become valid stakeholders, and the only way of doing it is to play by the system rules. But we were hacking it in the sense that we were radically changing the way a political party makes its decisions. For the first time, we were making our decisions together with those who we were affecting directly by those decisions. It was a very, very bold move for a two-month-old party in the city of Buenos Aires. But it got attention. We got 22,000 votes, that's 1.2 percent of the votes, and we came in second for the local options. So, even if that wasn't enough to win a seat in Congress, it was enough for us to become part of the conversation, to the extent that next month, Congress, as an institution, is launching for the first time in Argentina's history, a DemocracyOS to discuss, with the citizens, three pieces of legislation: two on urban transportation and one on the use of public space. Of course, our elected representatives are not saying, "Yes, we're going to vote according to what citizens decide," but they're willing to try. They're willing to open up a new space for citizen engagement and hopefully they'll be willing to listen as well. Our political system can be transformed, and not by subverting it, by destroying it, but by rewiring it with the tools that Internet affords us now. But a real challenge is to find, to design to create, to empower those connectors that are able to innovate, to transform noise and silence into signal and finally bring our democracies to the 21st century. I'm not saying it's easy. But in our experience, we actually stand a chance of making it work. And in my heart, it's most definitely worth trying. Thank you. 
Let me show you something.
 (Video) Girl: Okay, that's a cat sitting in a bed. The boy is petting the elephant. Those are people that are going on an airplane. That's a big airplane.
Fei-Fei Li: This is a three-year-old child describing what she sees in a series of photos. She might still have a lot to learn about this world, but she's already an expert at one very important task: to make sense of what she sees. Our society is more technologically advanced than ever. We send people to the moon, we make phones that talk to us or customize radio stations that can play only music we like. Yet, our most advanced machines and computers still struggle at this task. So I'm here today to give you a progress report on the latest advances in our research in computer vision, one of the most frontier and potentially revolutionary technologies in computer science.
Yes, we have prototyped cars that can drive by themselves, but without smart vision, they cannot really tell the difference between a crumpled paper bag on the road, which can be run over, and a rock that size, which should be avoided. We have made fabulous megapixel cameras, but we have not delivered sight to the blind. Drones can fly over massive land, but don't have enough vision technology to help us to track the changes of the rainforests. Security cameras are everywhere, but they do not alert us when a child is drowning in a swimming pool. Photos and videos are becoming an integral part of global life. They're being generated at a pace that's far beyond what any human, or teams of humans, could hope to view, and you and I are contributing to that at this TED. Yet our most advanced software is still struggling at understanding and managing this enormous content. So in other words, collectively as a society, we're very much blind, because our smartest machines are still blind.
"Why is this so hard?" you may ask. Cameras can take pictures like this one by converting lights into a two-dimensional array of numbers known as pixels, but these are just lifeless numbers. They do not carry meaning in themselves. Just like to hear is not the same as to listen, to take pictures is not the same as to see, and by seeing, we really mean understanding. In fact, it took Mother Nature 540 million years of hard work to do this task, and much of that effort went into developing the visual processing apparatus of our brains, not the eyes themselves. So vision begins with the eyes, but it truly takes place in the brain.
So for 15 years now, starting from my Ph.D. at Caltech and then leading Stanford's Vision Lab, I've been working with my mentors, collaborators and students to teach computers to see. Our research field is called computer vision and machine learning. It's part of the general field of artificial intelligence. So ultimately, we want to teach the machines to see just like we do: naming objects, identifying people, inferring 3D geometry of things, understanding relations, emotions, actions and intentions. You and I weave together entire stories of people, places and things the moment we lay our gaze on them.
The first step towards this goal is to teach a computer to see objects, the building block of the visual world. In its simplest terms, imagine this teaching process as showing the computers some training images of a particular object, let's say cats, and designing a model that learns from these training images. How hard can this be? After all, a cat is just a collection of shapes and colors, and this is what we did in the early days of object modeling. We'd tell the computer algorithm in a mathematical language that a cat has a round face, a chubby body, two pointy ears, and a long tail, and that looked all fine. But what about this cat? (Laughter) It's all curled up. Now you have to add another shape and viewpoint to the object model. But what if cats are hidden? What about these silly cats? Now you get my point. Even something as simple as a household pet can present an infinite number of variations to the object model, and that's just one object.
So about eight years ago, a very simple and profound observation changed my thinking. No one tells a child how to see, especially in the early years. They learn this through real-world experiences and examples. If you consider a child's eyes as a pair of biological cameras, they take one picture about every 200 milliseconds, the average time an eye movement is made. So by age three, a child would have seen hundreds of millions of pictures of the real world. That's a lot of training examples. So instead of focusing solely on better and better algorithms, my insight was to give the algorithms the kind of training data that a child was given through experiences in both quantity and quality.
Once we know this, we knew we needed to collect a data set that has far more images than we have ever had before, perhaps thousands of times more, and together with Professor Kai Li at Princeton University, we launched the ImageNet project in 2007. Luckily, we didn't have to mount a camera on our head and wait for many years. We went to the Internet, the biggest treasure trove of pictures that humans have ever created. We downloaded nearly a billion images and used crowdsourcing technology like the Amazon Mechanical Turk platform to help us to label these images. At its peak, ImageNet was one of the biggest employers of the Amazon Mechanical Turk workers: together, almost 50,000 workers from 167 countries around the world helped us to clean, sort and label nearly a billion candidate images. That was how much effort it took to capture even a fraction of the imagery a child's mind takes in in the early developmental years.
In hindsight, this idea of using big data to train computer algorithms may seem obvious now, but back in 2007, it was not so obvious. We were fairly alone on this journey for quite a while. Some very friendly colleagues advised me to do something more useful for my tenure, and we were constantly struggling for research funding. Once, I even joked to my graduate students that I would just reopen my dry cleaner's shop to fund ImageNet. After all, that's how I funded my college years.
So we carried on. In 2009, the ImageNet project delivered a database of 15 million images across 22,000 classes of objects and things organized by everyday English words. In both quantity and quality, this was an unprecedented scale. As an example, in the case of cats, we have more than 62,000 cats of all kinds of looks and poses and across all species of domestic and wild cats. We were thrilled to have put together ImageNet, and we wanted the whole research world to benefit from it, so in the TED fashion, we opened up the entire data set to the worldwide research community for free. (Applause)
Now that we have the data to nourish our computer brain, we're ready to come back to the algorithms themselves. As it turned out, the wealth of information provided by ImageNet was a perfect match to a particular class of machine learning algorithms called convolutional neural network, pioneered by Kunihiko Fukushima, Geoff Hinton, and Yann LeCun back in the 1970s and '80s. Just like the brain consists of billions of highly connected neurons, a basic operating unit in a neural network is a neuron-like node. It takes input from other nodes and sends output to others. Moreover, these hundreds of thousands or even millions of nodes are organized in hierarchical layers, also similar to the brain. In a typical neural network we use to train our object recognition model, it has 24 million nodes, 140 million parameters, and 15 billion connections. That's an enormous model. Powered by the massive data from ImageNet and the modern CPUs and GPUs to train such a humongous model, the convolutional neural network blossomed in a way that no one expected. It became the winning architecture to generate exciting new results in object recognition. This is a computer telling us this picture contains a cat and where the cat is. Of course there are more things than cats, so here's a computer algorithm telling us the picture contains a boy and a teddy bear; a dog, a person, and a small kite in the background; or a picture of very busy things like a man, a skateboard, railings, a lampost, and so on. Sometimes, when the computer is not so confident about what it sees, we have taught it to be smart enough to give us a safe answer instead of committing too much, just like we would do, but other times our computer algorithm is remarkable at telling us what exactly the objects are, like the make, model, year of the cars.
We applied this algorithm to millions of Google Street View images across hundreds of American cities, and we have learned something really interesting: first, it confirmed our common wisdom that car prices correlate very well with household incomes. But surprisingly, car prices also correlate well with crime rates in cities, or voting patterns by zip codes.
So wait a minute. Is that it? Has the computer already matched or even surpassed human capabilities? Not so fast. So far, we have just taught the computer to see objects. This is like a small child learning to utter a few nouns. It's an incredible accomplishment, but it's only the first step. Soon, another developmental milestone will be hit, and children begin to communicate in sentences. So instead of saying this is a cat in the picture, you already heard the little girl telling us this is a cat lying on a bed.
So to teach a computer to see a picture and generate sentences, the marriage between big data and machine learning algorithm has to take another step. Now, the computer has to learn from both pictures as well as natural language sentences generated by humans. Just like the brain integrates vision and language, we developed a model that connects parts of visual things like visual snippets with words and phrases in sentences.
About four months ago, we finally tied all this together and produced one of the first computer vision models that is capable of generating a human-like sentence when it sees a picture for the first time. Now, I'm ready to show you what the computer says when it sees the picture that the little girl saw at the beginning of this talk.
 (Video) Computer: A man is standing next to an elephant. A large airplane sitting on top of an airport runway.
FFL: Of course, we're still working hard to improve our algorithms, and it still has a lot to learn. 
And the computer still makes mistakes.
 (Video) Computer: A cat lying on a bed in a blanket.
FFL: So of course, when it sees too many cats, it thinks everything might look like a cat.
 (Video) Computer: A young boy is holding a baseball bat. (Laughter)
FFL: Or, if it hasn't seen a toothbrush, it confuses it with a baseball bat.
 (Video) Computer: A man riding a horse down a street next to a building. (Laughter)
FFL: We haven't taught Art 101 to the computers.
 (Video) Computer: A zebra standing in a field of grass.
FFL: And it hasn't learned to appreciate the stunning beauty of nature like you and I do.
So it has been a long journey. To get from age zero to three was hard. The real challenge is to go from three to 13 and far beyond. Let me remind you with this picture of the boy and the cake again. So far, we have taught the computer to see objects or even tell us a simple story when seeing a picture.
 (Video) Computer: A person sitting at a table with a cake.
FFL: But there's so much more to this picture than just a person and a cake. What the computer doesn't see is that this is a special Italian cake that's only served during Easter time. The boy is wearing his favorite t-shirt given to him as a gift by his father after a trip to Sydney, and you and I can all tell how happy he is and what's exactly on his mind at that moment.
This is my son Leo. On my quest for visual intelligence, I think of Leo constantly and the future world he will live in. When machines can see, doctors and nurses will have extra pairs of tireless eyes to help them to diagnose and take care of patients. Cars will run smarter and safer on the road. Robots, not just humans, will help us to brave the disaster zones to save the trapped and wounded. We will discover new species, better materials, and explore unseen frontiers with the help of the machines.
Little by little, we're giving sight to the machines. First, we teach them to see. Then, they help us to see better. For the first time, human eyes won't be the only ones pondering and exploring our world. We will not only use the machines for their intelligence, we will also collaborate with them in ways that we cannot even imagine.
This is my quest: to give computers visual intelligence and to create a better future for Leo and for the world.
Fifty-four percent of the world's population lives in our cities. In developing countries, one third of that population is living in slums. Seventy-five percent of global energy consumption occurs in our cities, and 80 percent of gas emissions that cause global warming come from our cities. So things that you and I might think about as global problems, like climate change, the energy crisis or poverty, are really, in many ways, city problems. They will not be solved unless people who live in cities, like most of us, actually start doing a better job, because right now, we are not doing a very good one. And that becomes very clear when we look into three aspects of city life: first, our citizens' willingness to engage with democratic institutions; second, our cities' ability to really include all of their residents; and lastly, our own ability to live fulfilling and happy lives.
When it comes to engagement, the data is very clear. Voter turnout around the world peaked in the late '80s, and it has been declining at a pace that we have never seen before, and if those numbers are bad at the national level, at the level of our cities, they are just dismal. In the last two years, two of the world's most consolidated, oldest democracies, the U.S. and France, held nationwide municipal elections. In France, voter turnout hit a record low. Almost 40 percent of voters decided not to show up. In the U.S., the numbers were even scarier. In some American cities, voter turnout was close to five percent. I'll let that sink in for a second. We're talking about democratic cities in which 95 percent of people decided that it was not important to elect their leaders. The city of L.A., a city of four million people, elected its mayor with just a bit over 200,000 votes. That was the lowest turnout the city had seen in 100 years. Right here, in my city of Rio, in spite of mandatory voting, almost 30 percent of the voting population chose to either annul their votes or stay home and pay a fine in the last mayoral elections.
When it comes to inclusiveness, our cities are not the best cases of success either, and again, you don't need to look very far in order to find proof of that. The city of Rio is incredibly unequal. This is Leblon. Leblon is the city's richest neighborhood. And this is Complexo do Alemão. This is where over 70,000 of the city's poorest residents live. Leblon has an HDI, a Human Development Index, of .967. That is higher than Norway, Switzerland or Sweden. Complexo do Alemão has an HDI of .711. It sits somewhere in between the HDI of Algeria and Gabon. So Rio, like so many cities across the global South, is a place where you can go from northern Europe to sub-Saharan Africa in the space of 30 minutes. If you drive, that is. If you take public transit, it's about two hours.
And lastly, perhaps most importantly, cities, with the incredible wealth of relations that they enable, could be the ideal places for human happiness to flourish. We like being around people. We are social animals. Instead, countries where urbanization has already peaked seem to be the very countries in which cities have stopped making us happy. The United States population has suffered from a general decrease in happiness for the past three decades, and the main reason is this. The American way of building cities has caused good quality public spaces to virtually disappear in many, many American cities, and as a result, they have seen a decline of relations, of the things that make us happy. Many studies show an increase in solitude and a decrease in solidarity, honesty, and social and civic participation.
So how do we start building cities that make us care? Cities that value their most important asset: the incredible diversity of the people who live in them? Cities that make us happy? Well, I believe that if we want to change what our cities look like, then we really have to change the decision-making processes that have given us the results that we have right now. We need a participation revolution, and we need it fast. The idea of voting as our only exercise in citizenship does not make sense anymore. People are tired of only being treated as empowered individuals every few years when it's time to delegate that power to someone else. If the protests that swept Brazil in June 2013 have taught us anything, it's that every time we try to exercise our power outside of an electoral context, we are beaten up, humiliated or arrested. And this needs to change, because when it does, not only will people re-engage with the structures of representation, but also complement these structures with direct, effective, and collective decision making, decision making of the kind that attacks inequality by its very inclusive nature, decision making of the kind that can change our cities into better places for us to live.
But there is a catch, obviously: Enabling widespread participation and redistributing power can be a logistical nightmare, and there's where technology can play an incredibly helpful role, by making it easier for people to organize, communicate and make decisions without having to be in the same room at the same time.
Unfortunately for us, when it comes to fostering democratic processes, our city governments have not used technology to its full potential. So far, most city governments have been effective at using tech to turn citizens into human sensors who serve authorities with data on the city: potholes, fallen trees or broken lamps. They have also, to a lesser extent, invited people to participate in improving the outcome of decisions that were already made for them, just like my mom when I was eight and she told me that I had a choice: I had to be in bed by 8 p.m., but I could choose my pink pajamas or my blue pajamas. That's not participation, and in fact, governments have not been very good at using technology to enable participation on what matters — the way we allocate our budget, the way we occupy our land, and the way we manage our natural resources. Those are the kinds of decisions that can actually impact global problems that manifest themselves in our cities.
The good news is, and I do have good news to share with you, we don't need to wait for governments to do this. I have reason to believe that it's possible for citizens to build their own structures of participation. Three years ago, I cofounded an organization called Meu Rio, and we make it easier for people in the city of Rio to organize around causes and places that they care about in their own city, and have an impact on those causes and places every day. In these past three years, Meu Rio grew to a network of 160,000 citizens of Rio. About 40 percent of those members are young people aged 20 to 29. That is one in every 15 young people of that age in Rio today.
Amongst our members is this adorable little girl, Bia, to your right, and Bia was just 11 years old when she started a campaign using one of our tools to save her model public school from demolition. Her school actually ranks among the best public schools in the country, and it was going to be demolished by the Rio de Janeiro state government to build, I kid you not, a parking lot for the World Cup right before the event happened. Bia started a campaign, and we even watched her school 24/7 through webcam monitoring, and many months afterwards, the government changed their minds. Bia's school stayed in place.
There's also Jovita. She's an amazing woman whose daughter went missing about 10 years ago, and since then, she has been looking for her daughter. In that process, she found out that first, she was not alone. In the last year alone, 2013, 6,000 people disappeared in the state of Rio. But she also found out that in spite of that, Rio had no centralized intelligence system for solving missing persons cases. In other Brazilian cities, those systems have helped solve up to 80 percent of missing persons cases. She started a campaign, and after the secretary of security got 16,000 emails from people asking him to do this, he responded, and started to build a police unit specializing in those cases. It was open to the public at the end of last month, and Jovita was there giving interviews and being very fancy.
And then, there is Leandro. Leandro is an amazing guy in a slum in Rio, and he created a recycling project in the slum. At the end of last year, December 16, he received an eviction order by the Rio de Janeiro state government giving him two weeks to leave the space that he had been using for two years. The plan was to hand it over to a developer, who planned to turn it into a construction site. Leandro started a campaign using one of our tools, the Pressure Cooker, the same one that Bia and Jovita used, and the state government changed their minds before Christmas Eve.
These stories make me happy, but not just because they have happy endings. They make me happy because they are happy beginnings. The teacher and parent community at Bia's school is looking for other ways they could improve that space even further. Leandro has ambitious plans to take his model to other low-income communities in Rio, and Jovita is volunteering at the police unit that she helped created. Bia, Jovita and Leandro are living examples of something that citizens and city governments around the world need to know: We are ready. As citizens, we are ready to decide on our common destinies, because we know that the way we distribute power says a lot about how we actually value everyone, and because we know that enabling and participating in local politics is a sign that we truly care about our relations to one another, and we are ready to do this in cities around the world right now. With the Our Cities network, the Meu Rio team hopes to share what we have learned with other people who want to create similar initiatives in their own cities. We have already started doing it in São Paulo with incredible results, and want to take it to cities around the world through a network of citizen-centric, citizen-led organizations that can inspire us, challenge us, and remind us to demand real participation in our city lives.
It is up to us to decide whether we want schools or parking lots, community-driven recycling projects or construction sites, loneliness or solidarity, cars or buses, and it is our responsibility to do that now, for ourselves, for our families, for the people who make our lives worth living, and for the incredible creativity, beauty, and wonder that make our cities, in spite of all of their problems, the greatest invention of our time.
I was in New York during Hurricane Sandy, and this little white dog called Maui was staying with me. Half the city was dark because of a power cut, and I was living on the dark side. Now, Maui was terrified of the dark, so I had to carry him up the stairs, actually down the stairs first, for his walk, and then bring him back up. I was also hauling gallons of bottles of water up to the seventh floor every day. And through all of this, I had to hold a torch between my teeth. The stores nearby were out of flashlights and batteries and bread. For a shower, I walked 40 blocks to a branch of my gym.
But these were not the major preoccupations of my day. It was just as critical for me to be the first person in at a cafe nearby with extension cords and chargers to juice my multiple devices. I started to prospect under the benches of bakeries and the entrances of pastry shops for plug points. I wasn't the only one. Even in the rain, people stood between Madison and 5th Avenue under their umbrellas charging their cell phones from outlets on the street. Nature had just reminded us that it was stronger than all our technology, and yet here we were, obsessed about being wired.
I think there's nothing like a crisis to tell you what's really important and what's not, and Sandy made me realize that our devices and their connectivity matter to us right up there with food and shelter. The self as we once knew it no longer exists, and I think that an abstract, digital universe has become a part of our identity, and I want to talk to you about what I think that means.
I'm a novelist, and I'm interested in the self because the self and fiction have a lot in common. They're both stories, interpretations. You and I can experience things without a story. We might run up the stairs too quickly and we might get breathless. But the larger sense that we have of our lives, the slightly more abstract one, is indirect. Our story of our life is based on direct experience, but it's embellished. A novel needs scene after scene to build, and the story of our life needs an arc as well. It needs months and years. Discrete moments from our lives are its chapters. But the story is not about these chapters. It's the whole book. It's not only about the heartbreak and the happiness, the victories and the disappointments, but it's because how because of these, and sometimes, more importantly, in spite of these, we find our place in the world and we change it and we change ourselves. Our story, therefore, needs two dimensions of time: a long arc of time that is our lifespan, and the timeframe of direct experience that is the moment. Now the self that experiences directly can only exist in the moment, but the one that narrates needs several moments, a whole sequence of them, and that's why our full sense of self needs both immersive experience and the flow of time. Now, the flow of time is embedded in everything, in the erosion of a grain of sand, in the budding of a little bud into a rose. Without it, we would have no music. Our own emotions and state of mind often encode time, regret or nostalgia about the past, hope or dread about the future.
I think that technology has altered that flow of time. The overall time that we have for our narrative, our lifespan, has been increasing, but the smallest measure, the moment, has shrunk. It has shrunk because our instruments enable us in part to measure smaller and smaller units of time, and this in turn has given us a more granular understanding of the material world, and this granular understanding has generated reams of data that our brains can no longer comprehend and for which we need more and more complicated computers. All of this to say that the gap between what we can perceive and what we can measure is only going to widen. Science can do things with and in a picosecond, but you and I are never going to have the inner experience of a millionth of a millionth of a second. You and I answer only to nature's rhythm and flow, to the sun, the moon and the seasons, and this is why we need that long arc of time with the past, the present and the future to see things for what they are, to separate signal from noise and the self from sensations. We need time's arrow to understand cause and effect, not just in the material world, but in our own intentions and our motivations. What happens when that arrow goes awry? What happens when time warps?
So many of us today have the sensation that time's arrow is pointing everywhere and nowhere at once. This is because time doesn't flow in the digital world in the same way that it does in the natural one. We all know that the Internet has shrunk space as well as time. Far away over there is now here. News from India is a stream on my smartphone app whether I'm in New York or New Delhi. And that's not all. Your last job, your dinner reservations from last year, your former friends, lie on a flat plain with today's friends, because the Internet also archives, and it warps the past. With no distinction left between the past, the present and the future, and the here or there, we are left with this moment everywhere, this moment that I'll call the digital now.
Just how can we prioritize in the landscape of the digital now? This digital now is not the present, because it's always a few seconds ahead, with Twitter streams that are already trending and news from other time zones. This isn't the now of a shooting pain in your foot or the second that you bite into a pastry or the three hours that you lose yourself in a great book. This now bears very little physical or psychological reference to our own state. Its focus, instead, is to distract us at every turn on the road. Every digital landmark is an invitation to leave what you are doing now to go somewhere else and do something else. Are you reading an interview by an author? Why not buy his book? Tweet it. Share it. Like it. Find other books exactly like his. Find other people reading those books. Travel can be liberating, but when it is incessant, we become permanent exiles without repose. Choice is freedom, but not when it's constantly for its own sake.
Not just is the digital now far from the present, but it's in direct competition with it, and this is because not just am I absent from it, but so are you. Not just are we absent from it, but so is everyone else. And therein lies its greatest convenience and horror. I can order foreign language books in the middle of the night, shop for Parisian macarons, and leave video messages that get picked up later. At all times, I can operate at a different rhythm and pace from you, while I sustain the illusion that I'm tapped into you in real time.
Sandy was a reminder of how such an illusion can shatter. There were those with power and water, and those without. There are those who went back to their lives, and those who are still displaced after so many months. For some reason, technology seems to perpetuate the illusion for those who have it that everyone does, and then, like an ironic slap in the face, it makes it true. For example, it's said that there are more people in India with access to cell phones than toilets. Now if this rift, which is already so great in many parts of the world, between the lack of infrastructure and the spread of technology, isn't somehow bridged, there will be ruptures between the digital and the real. For us as individuals who live in the digital now and spend most of our waking moments in it, the challenge is to live in two streams of time that are parallel and almost simultaneous. How does one live inside distraction?
We might think that those younger than us, those who are born into this, will adapt more naturally. Possibly, but I remember my childhood. I remember my grandfather revising the capitals of the world with me. Buda and Pest were separated by the Danube, and Vienna had a Spanish riding school. If I were a child today, I could easily learn this information with apps and hyperlinks, but it really wouldn't be the same, because much later, I went to Vienna, and I went to the Spanish riding school, and I could feel my grandfather right beside me. Night after night, he took me up on the terrace, on his shoulders, and pointed out Jupiter and Saturn and the Great Bear to me. And even here, when I look at the Great Bear, I get back that feeling of being a child, hanging onto his head and trying to balance myself on his shoulder, and I can get back that feeling of being a child again. What I had with my grandfather was wrapped so often in information and knowledge and fact, but it was about so much more than information or knowledge or fact. Time-warping technology challenges our deepest core, because we are able to archive the past and some of it becomes hard to forget, even as the current moment is increasingly unmemorable. We want to clutch, and we are left instead clutching at a series of static moments. They're like soap bubbles that disappear when we touch them.
By archiving everything, we think that we can store it, but time is not data. It cannot be stored. You and I know exactly what it means like to be truly present in a moment. It might have happened while we were playing an instrument, or looking into the eyes of someone we've known for a very long time. At such moments, our selves are complete. The self that lives in the long narrative arc and the self that experiences the moment become one. The present encapsulates the past and a promise for the future. The present joins a flow of time from before and after.
I first experienced these feelings with my grandmother. I wanted to learn to skip, and she found an old rope and she tucked up her sari and she jumped over it. I wanted to learn to cook, and she kept me in the kitchen, cutting, cubing and chopping for a whole month. My grandmother taught me that things happen in the time they take, that time can't be fought, and because it will pass and it will move, we owe the present moment our full attention. Attention is time. One of my yoga instructors once said that love is attention, and definitely from my grandmother, love and attention were one and the same thing. The digital world cannibalizes time, and in doing so, I want to suggest that what it threatens is the completeness of ourselves. It threatens the flow of love. But we don't need to let it. We can choose otherwise. We've seen again and again just how creative technology can be, and in our lives and in our actions, we can choose those solutions and those innovations and those moments that restore the flow of time instead of fragmenting it. We can slow down and we can tune in to the ebb and flow of time. We can choose to take time back.
Well, I have a big announcement to make today, and I'm really excited about this. And this may be a little bit of a surprise to many of you who know my research and what I've done well. I've really tried to solve some big problems: counterterrorism, nuclear terrorism, and health care and diagnosing and treating cancer, but I started thinking about all these problems, and I realized that the really biggest problem we face, what all these other problems come down to, is energy, is electricity, the flow of electrons. And I decided that I was going to set out to try to solve this problem.
And this probably is not what you're expecting. You're probably expecting me to come up here and talk about fusion, because that's what I've done most of my life. But this is actually a talk about, okay -- (Laughter) — but this is actually a talk about fission. It's about perfecting something old, and bringing something old into the 21st century.
Let's talk a little bit about how nuclear fission works. In a nuclear power plant, you have a big pot of water that's under high pressure, and you have some fuel rods, and these fuel rods are encased in zirconium, and they're little pellets of uranium dioxide fuel, and a fission reaction is controlled and maintained at a proper level, and that reaction heats up water, the water turns to steam, steam turns the turbine, and you produce electricity from it. This is the same way we've been producing electricity, the steam turbine idea, for 100 years, and nuclear was a really big advancement in a way to heat the water, but you still boil water and that turns to steam and turns the turbine.
And I thought, you know, is this the best way to do it? Is fission kind of played out, or is there something left to innovate here? And I realized that I had hit upon something that I think has this huge potential to change the world. And this is what it is.
This is a small modular reactor. So it's not as big as the reactor you see in the diagram here. This is between 50 and 100 megawatts. But that's a ton of power. That's between, say at an average use, that's maybe 25,000 to 100,000 homes could run off that. Now the really interesting thing about these reactors is they're built in a factory. So they're modular reactors that are built essentially on an assembly line, and they're trucked anywhere in the world, you plop them down, and they produce electricity. This region right here is the reactor.
And this is buried below ground, which is really important. For someone who's done a lot of counterterrorism work, I can't extol to you how great having something buried below the ground is for proliferation and security concerns.
And inside this reactor is a molten salt, so anybody who's a fan of thorium, they're going to be really excited about this, because these reactors happen to be really good at breeding and burning the thorium fuel cycle, uranium-233.
But I'm not really concerned about the fuel. You can run these off -- they're really hungry, they really like down-blended weapons pits, so that's highly enriched uranium and weapons-grade plutonium that's been down-blended. It's made into a grade where it's not usable for a nuclear weapon, but they love this stuff. And we have a lot of it sitting around, because this is a big problem. You know, in the Cold War, we built up this huge arsenal of nuclear weapons, and that was great, and we don't need them anymore, and what are we doing with all the waste, essentially? What are we doing with all the pits of those nuclear weapons? Well, we're securing them, and it would be great if we could burn them, eat them up, and this reactor loves this stuff.
So it's a molten salt reactor. It has a core, and it has a heat exchanger from the hot salt, the radioactive salt, to a cold salt which isn't radioactive. It's still thermally hot but it's not radioactive. And then that's a heat exchanger to what makes this design really, really interesting, and that's a heat exchanger to a gas. So going back to what I was saying before about all power being produced -- well, other than photovoltaic -- being produced by this boiling of steam and turning a turbine, that's actually not that efficient, and in fact, in a nuclear power plant like this, it's only roughly 30 to 35 percent efficient. That's how much thermal energy the reactor's putting out to how much electricity it's producing. And the reason the efficiencies are so low is these reactors operate at pretty low temperature. They operate anywhere from, you know, maybe 200 to 300 degrees Celsius. And these reactors run at 600 to 700 degrees Celsius, which means the higher the temperature you go to, thermodynamics tells you that you will have higher efficiencies. And this reactor doesn't use water. It uses gas, so supercritical CO2 or helium, and that goes into a turbine, and this is called the Brayton cycle. This is the thermodynamic cycle that produces electricity, and this makes this almost 50 percent efficient, between 45 and 50 percent efficiency. And I'm really excited about this, because it's a very compact core. Molten salt reactors are very compact by nature, but what's also great is you get a lot more electricity out for how much uranium you're fissioning, not to mention the fact that these burn up. Their burn-up is much higher. So for a given amount of fuel you put in the reactor, a lot more of it's being used.
And the problem with a traditional nuclear power plant like this is, you've got these rods that are clad in zirconium, and inside them are uranium dioxide fuel pellets. Well, uranium dioxide's a ceramic, and ceramic doesn't like releasing what's inside of it. So you have what's called the xenon pit, and so some of these fission products love neutrons. They love the neutrons that are going on and helping this reaction take place. And they eat them up, which means that, combined with the fact that the cladding doesn't last very long, you can only run one of these reactors for roughly, say, 18 months without refueling it. So these reactors run for 30 years without refueling, which is, in my opinion, very, very amazing, because it means it's a sealed system. No refueling means you can seal them up and they're not going to be a proliferation risk, and they're not going to have either nuclear material or radiological material proliferated from their cores.
But let's go back to safety, because everybody after Fukushima had to reassess the safety of nuclear, and one of the things when I set out to design a power reactor was it had to be passively and intrinsically safe, and I'm really excited about this reactor for essentially two reasons. One, it doesn't operate at high pressure. So traditional reactors like a pressurized water reactor or boiling water reactor, they're very, very hot water at very high pressures, and this means, essentially, in the event of an accident, if you had any kind of breach of this stainless steel pressure vessel, the coolant would leave the core. These reactors operate at essentially atmospheric pressure, so there's no inclination for the fission products to leave the reactor in the event of an accident. Also, they operate at high temperatures, and the fuel is molten, so they can't melt down, but in the event that the reactor ever went out of tolerances, or you lost off-site power in the case of something like Fukushima, there's a dump tank. Because your fuel is liquid, and it's combined with your coolant, you could actually just drain the core into what's called a sub-critical setting, basically a tank underneath the reactor that has some neutrons absorbers. And this is really important, because the reaction stops. In this kind of reactor, you can't do that. The fuel, like I said, is ceramic inside zirconium fuel rods, and in the event of an accident in one of these type of reactors, Fukushima and Three Mile Island -- looking back at Three Mile Island, we didn't really see this for a while — but these zirconium claddings on these fuel rods, what happens is, when they see high pressure water, steam, in an oxidizing environment, they'll actually produce hydrogen, and that hydrogen has this explosive capability to release fission products. So the core of this reactor, since it's not under pressure and it doesn't have this chemical reactivity, means that there's no inclination for the fission products to leave this reactor. So even in the event of an accident, yeah, the reactor may be toast, which is, you know, sorry for the power company, but we're not going to contaminate large quantities of land. So I really think that in the, say, 20 years it's going to take us to get fusion and make fusion a reality, this could be the source of energy that provides carbon-free electricity. Carbon-free electricity.
And it's an amazing technology because not only does it combat climate change, but it's an innovation. It's a way to bring power to the developing world, because it's produced in a factory and it's cheap. You can put them anywhere in the world you want to.
And maybe something else. As a kid, I was obsessed with space. Well, I was obsessed with nuclear science too, to a point, but before that I was obsessed with space, and I was really excited about, you know, being an astronaut and designing rockets, which was something that was always exciting to me. But I think I get to come back to this, because imagine having a compact reactor in a rocket that produces 50 to 100 megawatts. That is the rocket designer's dream. That's someone who is designing a habitat on another planet's dream. Not only do you have 50 to 100 megawatts to power whatever you want to provide propulsion to get you there, but you have power once you get there. You know, rocket designers who use solar panels or fuel cells, I mean a few watts or kilowatts -- wow, that's a lot of power. I mean, now we're talking about 100 megawatts. That's a ton of power. That could power a Martian community. That could power a rocket there. And so I hope that maybe I'll have an opportunity to kind of explore my rocketry passion at the same time that I explore my nuclear passion.
And people say, "Oh, well, you've launched this thing, and it's radioactive, into space, and what about accidents?" But we launch plutonium batteries all the time. Everybody was really excited about Curiosity, and that had this big plutonium battery on board that has plutonium-238, which actually has a higher specific activity than the low-enriched uranium fuel of these molten salt reactors, which means that the effects would be negligible, because you launch it cold, and when it gets into space is where you actually activate this reactor.
So I'm really excited. I think that I've designed this reactor here that can be an innovative source of energy, provide power for all kinds of neat scientific applications, and I'm really prepared to do this. I graduated high school in May, and -- (Laughter) (Applause) — I graduated high school in May, and I decided that I was going to start up a company to commercialize these technologies that I've developed, these revolutionary detectors for scanning cargo containers and these systems to produce medical isotopes, but I want to do this, and I've slowly been building up a team of some of the most incredible people I've ever had the chance to work with, and I'm really prepared to make this a reality. And I think, I think, that looking at the technology, this will be cheaper than or the same price as natural gas, and you don't have to refuel it for 30 years, which is an advantage for the developing world.
And I'll just say one more maybe philosophical thing to end with, which is weird for a scientist. But I think there's something really poetic about using nuclear power to propel us to the stars, because the stars are giant fusion reactors. They're giant nuclear cauldrons in the sky. The energy that I'm able to talk to you today, while it was converted to chemical energy in my food, originally came from a nuclear reaction, and so there's something poetic about, in my opinion, perfecting nuclear fission and using it as a future source of innovative energy.
Most of us think of motion as a very visual thing. If I walk across this stage or gesture with my hands while I speak, that motion is something that you can see. But there's a world of important motion that's too subtle for the human eye, and over the past few years, we've started to find that cameras can often see this motion even when humans can't.
So let me show you what I mean. On the left here, you see video of a person's wrist, and on the right, you see video of a sleeping infant, but if I didn't tell you that these were videos, you might assume that you were looking at two regular images, because in both cases, these videos appear to be almost completely still. But there's actually a lot of subtle motion going on here, and if you were to touch the wrist on the left, you would feel a pulse, and if you were to hold the infant on the right, you would feel the rise and fall of her chest as she took each breath. And these motions carry a lot of significance, but they're usually too subtle for us to see, so instead, we have to observe them through direct contact, through touch.
But a few years ago, my colleagues at MIT developed what they call a motion microscope, which is software that finds these subtle motions in video and amplifies them so that they become large enough for us to see. And so, if we use their software on the left video, it lets us see the pulse in this wrist, and if we were to count that pulse, we could even figure out this person's heart rate. And if we used the same software on the right video, it lets us see each breath that this infant takes, and we can use this as a contact-free way to monitor her breathing.
And so this technology is really powerful because it takes these phenomena that we normally have to experience through touch and it lets us capture them visually and non-invasively.
So a couple years ago, I started working with the folks that created that software, and we decided to pursue a crazy idea. We thought, it's cool that we can use software to visualize tiny motions like this, and you can almost think of it as a way to extend our sense of touch. But what if we could do the same thing with our ability to hear? What if we could use video to capture the vibrations of sound, which are just another kind of motion, and turn everything that we see into a microphone?
Now, this is a bit of a strange idea, so let me try to put it in perspective for you. Traditional microphones work by converting the motion of an internal diaphragm into an electrical signal, and that diaphragm is designed to move readily with sound so that its motion can be recorded and interpreted as audio. But sound causes all objects to vibrate. Those vibrations are just usually too subtle and too fast for us to see.
So what if we record them with a high-speed camera and then use software to extract tiny motions from our high-speed video, and analyze those motions to figure out what sounds created them? This would let us turn visible objects into visual microphones from a distance. And so we tried this out, and here's one of our experiments, where we took this potted plant that you see on the right and we filmed it with a high-speed camera while a nearby loudspeaker played this sound.
And so here's the video that we recorded, and we recorded it at thousands of frames per second, but even if you look very closely, all you'll see are some leaves that are pretty much just sitting there doing nothing, because our sound only moved those leaves by about a micrometer. That's one ten-thousandth of a centimeter, which spans somewhere between a hundredth and a thousandth of a pixel in this image. So you can squint all you want, but motion that small is pretty much perceptually invisible. But it turns out that something can be perceptually invisible and still be numerically significant, because with the right algorithms, we can take this silent, seemingly still video and we can recover this sound.
So how is this possible? How can we get so much information out of so little motion? Well, let's say that those leaves move by just a single micrometer, and let's say that that shifts our image by just a thousandth of a pixel. That may not seem like much, but a single frame of video may have hundreds of thousands of pixels in it, and so if we combine all of the tiny motions that we see from across that entire image, then suddenly a thousandth of a pixel can start to add up to something pretty significant.
On a personal note, we were pretty psyched when we figured this out. (Laughter) But even with the right algorithm, we were still missing a pretty important piece of the puzzle. You see, there are a lot of factors that affect when and how well this technique will work. There's the object and how far away it is; there's the camera and the lens that you use; how much light is shining on the object and how loud your sound is. And even with the right algorithm, we had to be very careful with our early experiments, because if we got any of these factors wrong, there was no way to tell what the problem was. We would just get noise back. And so a lot of our early experiments looked like this. And so here I am, and on the bottom left, you can kind of see our high-speed camera, which is pointed at a bag of chips, and the whole thing is lit by these bright lamps. And like I said, we had to be very careful in these early experiments, so this is how it went down.
 (Video) Abe Davis: Three, two, one, go. Mary had a little lamb! Little lamb! Little lamb!
AD: So this experiment looks completely ridiculous. (Laughter) I mean, I'm screaming at a bag of chips -- (Laughter) -- and we're blasting it with so much light, we literally melted the first bag we tried this on. (Laughter) But ridiculous as this experiment looks, it was actually really important, because we were able to recover this sound.
 (Audio) Mary had a little lamb! Little lamb! Little lamb!
AD: And this was really significant, because it was the first time we recovered intelligible human speech from silent video of an object. And so it gave us this point of reference, and gradually we could start to modify the experiment, using different objects or moving the object further away, using less light or quieter sounds. And we analyzed all of these experiments until we really understood the limits of our technique, because once we understood those limits, we could figure out how to push them.
And that led to experiments like this one, where again, I'm going to speak to a bag of chips, but this time we've moved our camera about 15 feet away, outside, behind a soundproof window, and the whole thing is lit by only natural sunlight. And so here's the video that we captured. And this is what things sounded like from inside, next to the bag of chips.
 (Audio) Mary had a little lamb whose fleece was white as snow, and everywhere that Mary went, that lamb was sure to go.
AD: And here's what we were able to recover from our silent video captured outside behind that window.
 (Audio) Mary had a little lamb whose fleece was white as snow, and everywhere that Mary went, that lamb was sure to go.
AD: And there are other ways that we can push these limits as well. So here's a quieter experiment where we filmed some earphones plugged into a laptop computer, and in this case, our goal was to recover the music that was playing on that laptop from just silent video of these two little plastic earphones, and we were able to do this so well that I could even Shazam our results. (Laughter)
And we can also push things by changing the hardware that we use. Because the experiments I've shown you so far were done with a camera, a high-speed camera, that can record video about a 100 times faster than most cell phones, but we've also found a way to use this technique with more regular cameras, and we do that by taking advantage of what's called a rolling shutter. You see, most cameras record images one row at a time, and so if an object moves during the recording of a single image, there's a slight time delay between each row, and this causes slight artifacts that get coded into each frame of a video. And so what we found is that by analyzing these artifacts, we can actually recover sound using a modified version of our algorithm. So here's an experiment we did where we filmed a bag of candy while a nearby loudspeaker played the same "Mary Had a Little Lamb" music from before, but this time, we used just a regular store-bought camera, and so in a second, I'll play for you the sound that we recovered, and it's going to sound distorted this time, but listen and see if you can still recognize the music.
And so, again, that sounds distorted, but what's really amazing here is that we were able to do this with something that you could literally run out and pick up at a Best Buy.
So at this point, a lot of people see this work, and they immediately think about surveillance. And to be fair, it's not hard to imagine how you might use this technology to spy on someone. But keep in mind that there's already a lot of very mature technology out there for surveillance. In fact, people have been using lasers to eavesdrop on objects from a distance for decades. But what's really new here, what's really different, is that now we have a way to picture the vibrations of an object, which gives us a new lens through which to look at the world, and we can use that lens to learn not just about forces like sound that cause an object to vibrate, but also about the object itself.
And so I want to take a step back and think about how that might change the ways that we use video, because we usually use video to look at things, and I've just shown you how we can use it to listen to things. But there's another important way that we learn about the world: that's by interacting with it. We push and pull and poke and prod things. We shake things and see what happens. And that's something that video still won't let us do, at least not traditionally. So I want to show you some new work, and this is based on an idea I had just a few months ago, so this is actually the first time I've shown it to a public audience. And the basic idea is that we're going to use the vibrations in a video to capture objects in a way that will let us interact with them and see how they react to us.
So here's an object, and in this case, it's a wire figure in the shape of a human, and we're going to film that object with just a regular camera. So there's nothing special about this camera. In fact, I've actually done this with my cell phone before. But we do want to see the object vibrate, so to make that happen, we're just going to bang a little bit on the surface where it's resting while we record this video.
So that's it: just five seconds of regular video, while we bang on this surface, and we're going to use the vibrations in that video to learn about the structural and material properties of our object, and we're going to use that information to create something new and interactive. And so here's what we've created. And it looks like a regular image, but this isn't an image, and it's not a video, because now I can take my mouse and I can start interacting with the object. And so what you see here is a simulation of how this object would respond to new forces that we've never seen before, and we created it from just five seconds of regular video.
And so this is a really powerful way to look at the world, because it lets us predict how objects will respond to new situations, and you could imagine, for instance, looking at an old bridge and wondering what would happen, how would that bridge hold up if I were to drive my car across it. And that's a question that you probably want to answer before you start driving across that bridge. And of course, there are going to be limitations to this technique, just like there were with the visual microphone, but we found that it works in a lot of situations that you might not expect, especially if you give it longer videos.
So for example, here's a video that I captured of a bush outside of my apartment, and I didn't do anything to this bush, but by capturing a minute-long video, a gentle breeze caused enough vibrations that we could learn enough about this bush to create this simulation. (Applause) And so you could imagine giving this to a film director, and letting him control, say, the strength and direction of wind in a shot after it's been recorded. Or, in this case, we pointed our camera at a hanging curtain, and you can't even see any motion in this video, but by recording a two-minute-long video, natural air currents in this room created enough subtle, imperceptible motions and vibrations that we could learn enough to create this simulation.
And ironically, we're kind of used to having this kind of interactivity when it comes to virtual objects, when it comes to video games and 3D models, but to be able to capture this information from real objects in the real world using just simple, regular video, is something new that has a lot of potential.
So here are the amazing people who worked with me on these projects. (Applause)
And what I've shown you today is only the beginning. We've just started to scratch the surface of what you can do with this kind of imaging, because it gives us a new way to capture our surroundings with common, accessible technology. And so looking to the future, it's going to be really exciting to explore what this can tell us about the world.
So recently, we heard a lot about how social media helps empower protest, and that's true, but after more than a decade of studying and participating in multiple social movements, I've come to realize that the way technology empowers social movements can also paradoxically help weaken them. This is not inevitable, but overcoming it requires diving deep into what makes success possible over the long term. And the lessons apply in multiple domains.
Now, take Turkey's Gezi Park protests, July 2013, which I went back to study in the field. Twitter was key to its organizing. It was everywhere in the park -- well, along with a lot of tear gas. It wasn't all high tech. But the people in Turkey had already gotten used to the power of Twitter because of an unfortunate incident about a year before when military jets had bombed and killed 34 Kurdish smugglers near the border region, and Turkish media completely censored this news. Editors sat in their newsrooms and waited for the government to tell them what to do. One frustrated journalist could not take this anymore. He purchased his own plane ticket, and went to the village where this had occurred. And he was confronted by this scene: a line of coffins coming down a hill, relatives wailing. He later he told me how overwhelmed he felt, and didn't know what to do, so he took out his phone, like any one of us might, and snapped that picture and tweeted it out. And voila, that picture went viral and broke the censorship and forced mass media to cover it.
So when, a year later, Turkey's Gezi protests happened, it started as a protest about a park being razed, but became an anti-authoritarian protest. It wasn't surprising that media also censored it, but it got a little ridiculous at times. When things were so intense, when CNN International was broadcasting live from Istanbul, CNN Turkey instead was broadcasting a documentary on penguins. Now, I love penguin documentaries, but that wasn't the news of the day. An angry viewer put his two screens together and snapped that picture, and that one too went viral, and since then, people call Turkish media the penguin media. (Laughter)
But this time, people knew what to do. They just took out their phones and looked for actual news. Better, they knew to go to the park and take pictures and participate and share it more on social media. Digital connectivity was used for everything from food to donations. Everything was organized partially with the help of these new technologies.
And using Internet to mobilize and publicize protests actually goes back a long way. Remember the Zapatistas, the peasant uprising in the southern Chiapas region of Mexico led by the masked, pipe-smoking, charismatic Subcomandante Marcos? That was probably the first movement that got global attention thanks to the Internet. Or consider Seattle '99, when a multinational grassroots effort brought global attention to what was then an obscure organization, the World Trade Organization, by also utilizing these digital technologies to help them organize. And more recently, movement after movement has shaken country after country: the Arab uprisings from Bahrain to Tunisia to Egypt and more; indignados in Spain, Italy, Greece; the Gezi Park protests; Taiwan; Euromaidan in Ukraine; Hong Kong. And think of more recent initiatives, like the #BringBackOurGirls hashtags. Nowadays, a network of tweets can unleash a global awareness campaign. A Facebook page can become the hub of a massive mobilization. Amazing.
But think of the moments I just mentioned. The achievements they were able to have, their outcomes, are not really proportional to the size and energy they inspired. The hopes they rightfully raised are not really matched by what they were able to have as a result in the end. And this raises a question: As digital technology makes things easier for movements, why haven't successful outcomes become more likely as well? In embracing digital platforms for activism and politics, are we overlooking some of the benefits of doing things the hard way? Now, I believe so. I believe that the rule of thumb is: Easier to mobilize does not always mean easier to achieve gains.
Now, to be clear, technology does empower in multiple ways. It's very powerful. In Turkey, I watched four young college students organize a countrywide citizen journalism network called 140Journos that became the central hub for uncensored news in the country. In Egypt, I saw another four young people use digital connectivity to organize the supplies and logistics for 10 field hospitals, very large operations, during massive clashes near Tahrir Square in 2011. And I asked the founder of this effort, called Tahrir Supplies, how long it took him to go from when he had the idea to when he got started. "Five minutes," he said. Five minutes. And he had no training or background in logistics. Or think of the Occupy movement which rocked the world in 2011. It started with a single email from a magazine, Adbusters, to 90,000 subscribers in its list. About two months after that first email, there were in the United States 600 ongoing occupations and protests. Less than one month after the first physical occupation in Zuccotti Park, a global protest was held in about 82 countries, 950 cities. It was one of the largest global protests ever organized.
Now, compare that to what the Civil Rights Movement had to do in 1955 Alabama to protest the racially segregated bus system, which they wanted to boycott. They'd been preparing for many years and decided it was time to swing into action after Rosa Parks was arrested. But how do you get the word out -- tomorrow we're going to start the boycott -- when you don't have Facebook, texting, Twitter, none of that? So they had to mimeograph 52,000 leaflets by sneaking into a university duplicating room and working all night, secretly. They then used the 68 African-American organizations that criss-crossed the city to distribute those leaflets by hand. And the logistical tasks were daunting, because these were poor people. They had to get to work, boycott or no, so a massive carpool was organized, again by meeting. No texting, no Twitter, no Facebook. They had to meet almost all the time to keep this carpool going.
Today, it would be so much easier. We could create a database, available rides and what rides you need, have the database coordinate, and use texting. We wouldn't have to meet all that much. But again, consider this: the Civil Rights Movement in the United States navigated a minefield of political dangers, faced repression and overcame, won major policy concessions, navigated and innovated through risks. In contrast, three years after Occupy sparked that global conversation about inequality, the policies that fueled it are still in place. Europe was also rocked by anti-austerity protests, but the continent didn't shift its direction. In embracing these technologies, are we overlooking some of the benefits of slow and sustained? To understand this, I went back to Turkey about a year after the Gezi protests and I interviewed a range of people, from activists to politicians, from both the ruling party and the opposition party and movements. I found that the Gezi protesters were despairing. They were frustrated, and they had achieved much less than what they had hoped for. This echoed what I'd been hearing around the world from many other protesters that I'm in touch with. And I've come to realize that part of the problem is that today's protests have become a bit like climbing Mt. Everest with the help of 60 Sherpas, and the Internet is our Sherpa. What we're doing is taking the fast routes and not replacing the benefits of the slower work. Because, you see, the kind of work that went into organizing all those daunting, tedious logistical tasks did not just take care of those tasks, they also created the kind of organization that could think together collectively and make hard decisions together, create consensus and innovate, and maybe even more crucially, keep going together through differences. So when you see this March on Washington in 1963, when you look at that picture, where this is the march where Martin Luther King gave his famous "I have a dream" speech, 1963, you don't just see a march and you don't just hear a powerful speech, you also see the painstaking, long-term work that can put on that march. And if you're in power, you realize you have to take the capacity signaled by that march, not just the march, but the capacity signaled by that march, seriously. In contrast, when you look at Occupy's global marches that were organized in two weeks, you see a lot of discontent, but you don't necessarily see teeth that can bite over the long term. And crucially, the Civil Rights Movement innovated tactically from boycotts to lunch counter sit-ins to pickets to marches to freedom rides. Today's movements scale up very quickly without the organizational base that can see them through the challenges. They feel a little like startups that got very big without knowing what to do next, and they rarely manage to shift tactically because they don't have the depth of capacity to weather such transitions.
Now, I want to be clear: The magic is not in the mimeograph. It's in that capacity to work together, think together collectively, which can only be built over time with a lot of work. To understand all this, I interviewed a top official from the ruling party in Turkey, and I ask him, "How do you do it?" They too use digital technology extensively, so that's not it. So what's the secret? Well, he told me. He said the key is he never took sugar with his tea. I said, what has that got to do with anything? Well, he said, his party starts getting ready for the next election the day after the last one, and he spends all day every day meeting with voters in their homes, in their wedding parties, circumcision ceremonies, and then he meets with his colleagues to compare notes. With that many meetings every day, with tea offered at every one of them, which he could not refuse, because that would be rude, he could not take even one cube of sugar per cup of tea, because that would be many kilos of sugar, he can't even calculate how many kilos, and at that point I realized why he was speaking so fast. We had met in the afternoon, and he was already way over-caffeinated. But his party won two major elections within a year of the Gezi protests with comfortable margins. To be sure, governments have different resources to bring to the table. It's not the same game, but the differences are instructive. And like all such stories, this is not a story just of technology. It's what technology allows us to do converging with what we want to do. Today's social movements want to operate informally. They do not want institutional leadership. They want to stay out of politics because they fear corruption and cooptation. They have a point. Modern representative democracies are being strangled in many countries by powerful interests. But operating this way makes it hard for them to sustain over the long term and exert leverage over the system, which leads to frustrated protesters dropping out, and even more corrupt politics. And politics and democracy without an effective challenge hobbles, because the causes that have inspired the modern recent movements are crucial. Climate change is barreling towards us. Inequality is stifling human growth and potential and economies. Authoritarianism is choking many countries. We need movements to be more effective.
Now, some people have argued that the problem is today's movements are not formed of people who take as many risks as before, and that is not true. From Gezi to Tahrir to elsewhere, I've seen people put their lives and livelihoods on the line. It's also not true, as Malcolm Gladwell claimed, that today's protesters form weaker virtual ties. No, they come to these protests, just like before, with their friends, existing networks, and sometimes they do make new friends for life. I still see the friends that I made in those Zapatista-convened global protests more than a decade ago, and the bonds between strangers are not worthless. When I got tear-gassed in Gezi, people I didn't know helped me and one another instead of running away. In Tahrir, I saw people, protesters, working really hard to keep each other safe and protected. And digital awareness-raising is great, because changing minds is the bedrock of changing politics. But movements today have to move beyond participation at great scale very fast and figure out how to think together collectively, develop strong policy proposals, create consensus, figure out the political steps and relate them to leverage, because all these good intentions and bravery and sacrifice by itself are not going to be enough.
And there are many efforts. In New Zealand, a group of young people are developing a platform called Loomio for participatory decision making at scale. In Turkey, 140Journos are holding hack-a-thons so that they support communities as well as citizen journalism. In Argentina, an open-source platform called DemocracyOS is bringing participation to parliaments and political parties. These are all great, and we need more, but the answer won't just be better online decision-making, because to update democracy, we are going to need to innovate at every level, from the organizational to the political to the social. Because to succeed over the long term, sometimes you do need tea without sugar along with your Twitter. Thank you.
My job is to design, build and study robots that communicate with people. But this story doesn't start with robotics at all, it starts with animation. When I first saw Pixar's "Luxo Jr.," I was amazed by how much emotion they could put into something as trivial as a desk lamp. I mean, look at them -- at the end of this movie, you actually feel something for two pieces of furniture.
And I said, I have to learn how to do this. So I made a really bad career decision.
And that's what my mom was like when I did it.
I left a very cozy tech job in Israel at a nice software company and I moved to New York to study animation. And there I lived in a collapsing apartment building in Harlem with roommates. I'm not using this phrase metaphorically -- the ceiling actually collapsed one day in our living room. Whenever they did news stories about building violations in New York, they would put the report in front of our building, as kind of, like, a backdrop to show how bad things are.
Anyway, during the day, I went to school and at night I would sit and draw frame by frame of pencil animation. And I learned two surprising lessons. One of them was that when you want to arouse emotions, it doesn't matter so much how something looks; it's all in the motion, in the timing of how the thing moves. And the second was something one of our teachers told us. He actually did the weasel in "Ice Age." And he said, "As an animator, you're not a director -- you're an actor." So, if you want to find the right motion for a character, don't think about it -- go use your body to find it. Stand in front of a mirror, act it out in front of a camera -- whatever you need -- and then put it back in your character.
A year later I found myself at MIT in the Robotic Life Group. It was one of the first groups researching the relationships between humans and robots. And I still had this dream to make an actual, physical Luxo Jr. lamp. But I found that robots didn't move at all in this engaging way that I was used to from my animation studies. Instead, they were all -- how should I put it -- they were all kind of robotic. (Laughter) And I thought, what if I took whatever I learned in animation school, and used that to design my robotic desk lamp. So I went and designed frame by frame to try to make this robot as graceful and engaging as possible. And here when you see the robot interacting with me on a desktop -- and I'm actually redesigning the robot, so, unbeknownst to itself, it's kind of digging its own grave by helping me.
I wanted it to be less of a mechanical structure giving me light, and more of a helpful, kind of quiet apprentice that's always there when you need it and doesn't really interfere. And when, for example, I'm looking for a battery that I can't find, in a subtle way, it'll show me where the battery is. So you can see my confusion here. I'm not an actor. And I want you to notice how the same mechanical structure can, at one point, just by the way it moves, seem gentle and caring and in the other case, seem violent and confrontational. And it's the same structure, just the motion is different. Actor: "You want to know something? Well, you want to know something? He was already dead! Just laying there, eyes glazed over!"
But, moving in a graceful way is just one building block of this whole structure called human-robot interaction. I was, at the time, doing my PhD, I was working on human-robot teamwork, teams of humans and robots working together. I was studying the engineering, the psychology, the philosophy of teamwork, and at the same time, I found myself in my own kind of teamwork situation, with a good friend of mine, who's actually here. And in that situation, we can easily imagine robots in the near future being there with us. It was after a Passover Seder. We were folding up a lot of folding chairs, and I was amazed at how quickly we found our own rhythm. Everybody did their own part, we didn't have to divide our tasks. We didn't have to communicate verbally about this -- it all just happened.
And I thought, humans and robots don't look at all like this. When humans and robots interact, it's much more like a chess game: the human does a thing, the robot analyzes whatever the human did, the robot decides what to do next, plans it and does it. Then the human waits, until it's their turn again. So it's much more like a chess game, and that makes sense, because chess is great for mathematicians and computer scientists. It's all about information, analysis, decision-making and planning.
But I wanted my robot to be less of a chess player, and more like a doer that just clicks and works together. So I made my second horrible career choice: I decided to study acting for a semester. I took off from the PhD, I went to acting classes. I actually participated in a play -- I hope there’s no video of that around still.
And I got every book I could find about acting, including one from the 19th century that I got from the library. And I was really amazed, because my name was the second name on the list -- the previous name was in 1889.
And this book was kind of waiting for 100 years to be rediscovered for robotics. And this book shows actors how to move every muscle in the body to match every kind of emotion that they want to express.
But the real revelation was when I learned about method acting. It became very popular in the 20th century. And method acting said you don't have to plan every muscle in your body; instead, you have to use your body to find the right movement. You have to use your sense memory to reconstruct the emotions and kind of think with your body to find the right expression -- improvise, play off your scene partner. And this came at the same time as I was reading about this trend in cognitive psychology, called embodied cognition, which also talks about the same ideas. We use our bodies to think; we don't just think with our brains and use our bodies to move, but our bodies feed back into our brain to generate the way that we behave.
And it was like a lightning bolt. I went back to my office, I wrote this paper, which I never really published, called "Acting Lessons for Artificial Intelligence." And I even took another month to do what was then the first theater play with a human and a robot acting together. That's what you saw before with the actors. And I thought: How can we make an artificial intelligence model -- a computer, computational model -- that will model some of these ideas of improvisation, of taking risks, of taking chances, even of making mistakes? Maybe it can make for better robotic teammates. So I worked for quite a long time on these models and I implemented them on a number of robots.
Here you can see a very early example with the robots trying to use this embodied artificial intelligence to try to match my movements as closely as possible. It's sort of like a game. Let's look at it. You can see when I psych it out, it gets fooled. And it's a little bit like what you might see actors do when they try to mirror each other to find the right synchrony between them. And then, I did another experiment, and I got people off the street to use the robotic desk lamp, and try out this idea of embodied artificial intelligence. So, I actually used two kinds of brains for the same robot.
The robot is the same lamp that you saw, and I put two brains in it. For one half of the people, I put in a brain that's kind of the traditional, calculated robotic brain. It waits for its turn, it analyzes everything, it plans. Let's call it the calculated brain. The other got more the stage actor, risk-taker brain. Let's call it the adventurous brain. It sometimes acts without knowing everything it has to know. It sometimes makes mistakes and corrects them. And I had them do this very tedious task that took almost 20 minutes, and they had to work together, somehow simulating, like, a factory job of repetitively doing the same thing. What I found is that people actually loved the adventurous robot. They thought it was more intelligent, more committed, a better member of the team, contributed to the success of the team more. They even called it "he" and "she," whereas people with the calculated brain called it "it," and nobody ever called it "he" or "she." When they talked about it after the task, with the adventurous brain, they said, "By the end, we were good friends and high-fived mentally." Whatever that means.
Sounds painful. Whereas the people with the calculated brain said it was just like a lazy apprentice. It only did what it was supposed to do and nothing more, which is almost what people expect robots to do, so I was surprised that people had higher expectations of robots than what anybody in robotics thought robots should be doing. And in a way, I thought, maybe it's time -- just like method acting changed the way people thought about acting in the 19th century, from going from the very calculated, planned way of behaving, to a more intuitive, risk-taking, embodied way of behaving -- maybe it's time for robots to have the same kind of revolution.
A few years later, I was at my next research job at Georgia Tech in Atlanta, and I was working in a group dealing with robotic musicians. And I thought, music: that's the perfect place to look at teamwork, coordination, timing, improvisation -- and we just got this robot playing marimba. And the marimba, for everybody like me, it was this huge, wooden xylophone. And when I was looking at this, I looked at other works in human-robot improvisation -- yes, there are other works in human-robot improvisation -- and they were also a little bit like a chess game. The human would play, the robot analyzed what was played, and would improvise their own part. So, this is what musicians called a call-and-response interaction, and it also fits very well robots and artificial intelligence. But I thought, if I use the same ideas I used in the theater play and in the teamwork studies, maybe I can make the robots jam together like a band. Everybody's riffing off each other, nobody is stopping for a moment. And so I tried to do the same things, this time with music, where the robot doesn't really know what it's about to play, it just sort of moves its body and uses opportunities to play, and does what my jazz teacher when I was 17 taught me. She said, when you improvise, sometimes you don't know what you're doing, and you still do it. So I tried to make a robot that doesn't actually know what it's doing, but is still doing it. So let's look at a few seconds from this performance, where the robot listens to the human musician and improvises. And then, look how the human musician also responds to what the robot is doing and picking up from its behavior, and at some point can even be surprised by what the robot came up with.
Being a musician is not just about making notes, otherwise nobody would ever go see a live show. Musicians also communicate with their bodies, with other band members, with the audience, they use their bodies to express the music. And I thought, we already have a robot musician on stage, why not make it be a full-fledged musician? And I started designing a socially expressive head for the robot. The head doesn’t actually touch the marimba, it just expresses what the music is like. These are some napkin sketches from a bar in Atlanta that was dangerously located exactly halfway between my lab and my home. So I spent, I would say, on average, three to four hours a day there. I think.
And I went back to my animation tools and tried to figure out not just what a robotic musician would look like, but especially what a robotic musician would move like, to sort of show that it doesn't like what the other person is playing -- and maybe show whatever beat it's feeling at the moment.
So we ended up actually getting the money to build this robot, which was nice. I'm going to show you now the same kind of performance, this time with a socially expressive head. And notice one thing -- how the robot is really showing us the beat it's picking up from the human, while also giving the human a sense that the robot knows what it's doing. And also how it changes the way it moves as soon as it starts its own solo.
Now it's looking at me, showing that it's listening.
Now look at the final chord of the piece again. And this time the robot communicates with its body when it's busy doing its own thing, and when it's ready to coordinate the final chord with me.
Thanks. I hope you see how much this part of the body that doesn't touch the instrument actually helps with the musical performance. And at some point -- we are in Atlanta, so obviously some rapper will come into our lab at some point -- and we had this rapper come in and do a little jam with the robot. Here you can see the robot basically responding to the beat. Notice two things: one, how irresistible it is to join the robot while it's moving its head. You kind of want to move your own head when it does it. And second, even though the rapper is really focused on his iPhone, as soon as the robot turns to him, he turns back. So even though it's just in the periphery of his vision, in the corner of his eye, it's very powerful. And the reason is that we can't ignore physical things moving in our environment. We are wired for that. So if you have a problem -- maybe your partner is looking at their iPhone or smartphone too much -- you might want to have a robot there to get their attention.
Just to introduce the last robot that we've worked on, it came out of something surprising that we found: Some point people didn't care about the robot being intelligent, able to improvise and listen, and do all these embodied intelligence things that I spent years developing. They really liked that the robot was enjoying the music.
And they didn't say the robot was moving to the music, they said "enjoying" the music. And we thought, why don't we take this idea, and I designed a new piece of furniture. This time it wasn't a desk lamp, it was a speaker dock, one of those things you plug your smartphone in. And I thought, what would happen if your speaker dock didn't just play the music for you, but would actually enjoy it, too? And so again, here are some animation tests from an early stage.
And this is what the final product looked like.
So, a lot of bobbing heads.
A lot of bobbing heads in the audience, so we can still see robots influence people. And it's not just fun and games.
I think one of the reasons I care so much about robots that use their body to communicate and use their body to move is -- I'm going to let you in on a little secret we roboticists are hiding -- is that every one of you is going to be living with a robot at some point in your life. Somewhere in your future, there will be a robot in your life. If not in yours, your children's lives. And I want these robots to be more fluent, more engaging, more graceful than currently they seem to be. And for that I think maybe robots need to be less like chess players and more like stage actors and more like musicians. Maybe they should be able to take chances and improvise. Maybe they should be able to anticipate what you're about to do. Maybe they even need to be able to make mistakes and correct them, because in the end, we are human. And maybe as humans, robots that are a little less than perfect are just perfect for us.
So over the past few centuries, microscopes have revolutionized our world. They revealed to us a tiny world of objects, life and structures that are too small for us to see with our naked eyes. They are a tremendous contribution to science and technology. Today I'd like to introduce you to a new type of microscope, a microscope for changes. It doesn't use optics like a regular microscope to make small objects bigger, but instead it uses a video camera and image processing to reveal to us the tiniest motions and color changes in objects and people, changes that are impossible for us to see with our naked eyes. And it lets us look at our world in a completely new way.
So what do I mean by color changes? Our skin, for example, changes its color very slightly when the blood flows under it. That change is incredibly subtle, which is why, when you look at other people, when you look at the person sitting next to you, you don't see their skin or their face changing color. When we look at this video of Steve here, it appears to us like a static picture, but once we look at this video through our new, special microscope, suddenly we see a completely different image. What you see here are small changes in the color of Steve's skin, magnified 100 times so that they become visible. We can actually see a human pulse. We can see how fast Steve's heart is beating, but we can also see the actual way that the blood flows in his face. And we can do that not just to visualize the pulse, but also to actually recover our heart rates, and measure our heart rates. And we can do it with regular cameras and without touching the patients. So here you see the pulse and heart rate we extracted from a neonatal baby from a video we took with a regular DSLR camera, and the heart rate measurement we get is as accurate as the one you'd get with a standard monitor in a hospital. And it doesn't even have to be a video we recorded. We can do it essentially with other videos as well. So I just took a short clip from "Batman Begins" here just to show Christian Bale's pulse. (Laughter) And you know, presumably he's wearing makeup, the lighting here is kind of challenging, but still, just from the video, we're able to extract his pulse and show it quite well.
So how do we do all that? We basically analyze the changes in the light that are recorded at every pixel in the video over time, and then we crank up those changes. We make them bigger so that we can see them. The tricky part is that those signals, those changes that we're after, are extremely subtle, so we have to be very careful when you try to separate them from noise that always exists in videos. So we use some clever image processing techniques to get a very accurate measurement of the color at each pixel in the video, and then the way the color changes over time, and then we amplify those changes. We make them bigger to create those types of enhanced videos, or magnified videos, that actually show us those changes.
But it turns out we can do that not just to show tiny changes in color, but also tiny motions, and that's because the light that gets recorded in our cameras will change not only if the color of the object changes, but also if the object moves. So this is my daughter when she was about two months old. It's a video I recorded about three years ago. And as new parents, we all want to make sure our babies are healthy, that they're breathing, that they're alive, of course. So I too got one of those baby monitors so that I could see my daughter when she was asleep. And this is pretty much what you'll see with a standard baby monitor. You can see the baby's sleeping, but there's not too much information there. There's not too much we can see. Wouldn't it be better, or more informative, or more useful, if instead we could look at the view like this. So here I took the motions and I magnified them 30 times, and then I could clearly see that my daughter was indeed alive and breathing. (Laughter) Here is a side-by-side comparison. So again, in the source video, in the original video, there's not too much we can see, but once we magnify the motions, the breathing becomes much more visible. And it turns out, there's a lot of phenomena we can reveal and magnify with our new motion microscope. We can see how our veins and arteries are pulsing in our bodies. We can see that our eyes are constantly moving in this wobbly motion. And that's actually my eye, and again this video was taken right after my daughter was born, so you can see I wasn't getting too much sleep. (Laughter) Even when a person is sitting still, there's a lot of information we can extract about their breathing patterns, small facial expressions. Maybe we could use those motions to tell us something about our thoughts or our emotions. We can also magnify small mechanical movements, like vibrations in engines, that can help engineers detect and diagnose machinery problems, or see how our buildings and structures sway in the wind and react to forces. Those are all things that our society knows how to measure in various ways, but measuring those motions is one thing, and actually seeing those motions as they happen is a whole different thing.
And ever since we discovered this new technology, we made our code available online so that others could use and experiment with it. It's very simple to use. It can work on your own videos. Our collaborators at Quanta Research even created this nice website where you can upload your videos and process them online, so even if you don't have any experience in computer science or programming, you can still very easily experiment with this new microscope. And I'd like to show you just a couple of examples of what others have done with it.
So this video was made by a YouTube user called Tamez85. I don't know who that user is, but he, or she, used our code to magnify small belly movements during pregnancy. It's kind of creepy. (Laughter) People have used it to magnify pulsing veins in their hands. And you know it's not real science unless you use guinea pigs, and apparently this guinea pig is called Tiffany, and this YouTube user claims it is the first rodent on Earth that was motion-magnified.
You can also do some art with it. So this video was sent to me by a design student at Yale. She wanted to see if there's any difference in the way her classmates move. She made them all stand still, and then magnified their motions. It's like seeing still pictures come to life. And the nice thing with all those examples is that we had nothing to do with them. We just provided this new tool, a new way to look at the world, and then people find other interesting, new and creative ways of using it.
But we didn't stop there. This tool not only allows us to look at the world in a new way, it also redefines what we can do and pushes the limits of what we can do with our cameras. So as scientists, we started wondering, what other types of physical phenomena produce tiny motions that we could now use our cameras to measure? And one such phenomenon that we focused on recently is sound. Sound, as we all know, is basically changes in air pressure that travel through the air. Those pressure waves hit objects and they create small vibrations in them, which is how we hear and how we record sound. But it turns out that sound also produces visual motions. Those are motions that are not visible to us but are visible to a camera with the right processing. So here are two examples. This is me demonstrating my great singing skills. (Singing) (Laughter) And I took a high-speed video of my throat while I was humming. Again, if you stare at that video, there's not too much you'll be able to see, but once we magnify the motions 100 times, we can see all the motions and ripples in the neck that are involved in producing the sound. That signal is there in that video.
We also know that singers can break a wine glass if they hit the correct note. So here, we're going to play a note that's in the resonance frequency of that glass through a loudspeaker that's next to it. Once we play that note and magnify the motions 250 times, we can very clearly see how the glass vibrates and resonates in response to the sound. It's not something you're used to seeing every day. But this made us think. It gave us this crazy idea. Can we actually invert this process and recover sound from video by analyzing the tiny vibrations that sound waves create in objects, and essentially convert those back into the sounds that produced them. In this way, we can turn everyday objects into microphones.
So that's exactly what we did. So here's an empty bag of chips that was lying on a table, and we're going to turn that bag of chips into a microphone by filming it with a video camera and analyzing the tiny motions that sound waves create in it. So here's the sound that we played in the room.
And this is a high-speed video we recorded of that bag of chips. Again it's playing. There's no chance you'll be able to see anything going on in that video just by looking at it, but here's the sound we were able to recover just by analyzing the tiny motions in that video.
I call it -- Thank you. (Applause) I call it the visual microphone. We actually extract audio signals from video signals. And just to give you a sense of the scale of the motions here, a pretty loud sound will cause that bag of chips to move less than a micrometer. That's one thousandth of a millimeter. That's how tiny the motions are that we are now able to pull out just by observing how light bounces off objects and gets recorded by our cameras.
We can recover sounds from other objects, like plants.
And we can recover speech as well. So here's a person speaking in a room.
Voice: Mary had a little lamb whose fleece was white as snow, and everywhere that Mary went, that lamb was sure to go.
Michael Rubinstein: And here's that speech again recovered just from this video of that same bag of chips.
Voice: Mary had a little lamb whose fleece was white as snow, and everywhere that Mary went, that lamb was sure to go.
MR: We used "Mary Had a Little Lamb" because those are said to be the first words that Thomas Edison spoke into his phonograph in 1877. It was one of the first sound recording devices in history. It basically directed the sounds onto a diaphragm that vibrated a needle that essentially engraved the sound on tinfoil that was wrapped around the cylinder.
Here's a demonstration of recording and replaying sound with Edison's phonograph.
 (Video) Voice: Testing, testing, one two three. Mary had a little lamb whose fleece was white as snow, and everywhere that Mary went, the lamb was sure to go. Testing, testing, one two three. Mary had a little lamb whose fleece was white as snow, and everywhere that Mary went, the lamb was sure to go.
MR: And now, 137 years later, we're able to get sound in pretty much similar quality but by just watching objects vibrate to sound with cameras, and we can even do that when the camera is 15 feet away from the object, behind soundproof glass.
So this is the sound that we were able to recover in that case.
Voice: Mary had a little lamb whose fleece was white as snow, and everywhere that Mary went, the lamb was sure to go.
MR: And of course, surveillance is the first application that comes to mind. (Laughter) But it might actually be useful for other things as well. Maybe in the future, we'll be able to use it, for example, to recover sound across space, because sound can't travel in space, but light can.
We've only just begun exploring other possible uses for this new technology. It lets us see physical processes that we know are there but that we've never been able to see with our own eyes until now.
This is our team. Everything I showed you today is a result of a collaboration with this great group of people you see here, and I encourage you and welcome you to check out our website, try it out yourself, and join us in exploring this world of tiny motions.
This is a lot of ones and zeros. It's what we call binary information. This is how computers talk. It's how they store information. It's how computers think. It's how computers do everything it is that computers do. I'm a cybersecurity researcher, which means my job is to sit down with this information and try to make sense of it, to try to understand what all the ones and zeroes mean. Unfortunately for me, we're not just talking about the ones and zeros I have on the screen here. We're not just talking about a few pages of ones and zeros. We're talking about billions and billions of ones and zeros, more than anyone could possibly comprehend.
Now, as exciting as that sounds, when I first started doing cyber — (Laughter) — when I first started doing cyber, I wasn't sure that sifting through ones and zeros was what I wanted to do with the rest of my life, because in my mind, cyber was keeping viruses off of my grandma's computer, it was keeping people's Myspace pages from being hacked, and maybe, maybe on my most glorious day, it was keeping someone's credit card information from being stolen. Those are important things, but that's not how I wanted to spend my life.
But after 30 minutes of work as a defense contractor, I soon found out that my idea of cyber was a little bit off. In fact, in terms of national security, keeping viruses off of my grandma's computer was surprisingly low on their priority list. And the reason for that is cyber is so much bigger than any one of those things. Cyber is an integral part of all of our lives, because computers are an integral part of all of our lives, even if you don't own a computer. Computers control everything in your car, from your GPS to your airbags. They control your phone. They're the reason you can call 911 and get someone on the other line. They control our nation's entire infrastructure. They're the reason you have electricity, heat, clean water, food. Computers control our military equipment, everything from missile silos to satellites to nuclear defense networks. All of these things are made possible because of computers, and therefore because of cyber, and when something goes wrong, cyber can make all of these things impossible.
But that's where I step in. A big part of my job is defending all of these things, keeping them working, but once in a while, part of my job is to break one of these things, because cyber isn't just about defense, it's also about offense. We're entering an age where we talk about cyberweapons. In fact, so great is the potential for cyber offense that cyber is considered a new domain of warfare. Warfare. It's not necessarily a bad thing. On the one hand, it means we have whole new front on which we need to defend ourselves, but on the other hand, it means we have a whole new way to attack, a whole new way to stop evil people from doing evil things.
So let's consider an example of this that's completely theoretical. Suppose a terrorist wants to blow up a building, and he wants to do this again and again in the future. So he doesn't want to be in that building when it explodes. He's going to use a cell phone as a remote detonator. Now, it used to be the only way we had to stop this terrorist was with a hail of bullets and a car chase, but that's not necessarily true anymore. We're entering an age where we can stop him with the press of a button from 1,000 miles away, because whether he knew it or not, as soon as he decided to use his cell phone, he stepped into the realm of cyber. A well-crafted cyber attack could break into his phone, disable the overvoltage protections on his battery, drastically overload the circuit, cause the battery to overheat, and explode. No more phone, no more detonator, maybe no more terrorist, all with the press of a button from a thousand miles away.
So how does this work? It all comes back to those ones and zeros. Binary information makes your phone work, and used correctly, it can make your phone explode. So when you start to look at cyber from this perspective, spending your life sifting through binary information starts to seem kind of exciting.
But here's the catch: This is hard, really, really hard, and here's why. Think about everything you have on your cell phone. You've got the pictures you've taken. You've got the music you listen to. You've got your contacts list, your email, and probably 500 apps you've never used in your entire life, and behind all of this is the software, the code, that controls your phone, and somewhere, buried inside of that code, is a tiny piece that controls your battery, and that's what I'm really after, but all of this, just a bunch of ones and zeros, and it's all just mixed together. In cyber, we call this finding a needle in a stack of needles, because everything pretty much looks alike. I'm looking for one key piece, but it just blends in with everything else.
So let's step back from this theoretical situation of making a terrorist's phone explode, and look at something that actually happened to me. Pretty much no matter what I do, my job always starts with sitting down with a whole bunch of binary information, and I'm always looking for one key piece to do something specific. In this case, I was looking for a very advanced, very high-tech piece of code that I knew I could hack, but it was somewhere buried inside of a billion ones and zeroes. Unfortunately for me, I didn't know quite what I was looking for. I didn't know quite what it would look like, which makes finding it really, really hard. When I have to do that, what I have to do is basically look at various pieces of this binary information, try to decipher each piece, and see if it might be what I'm after. So after a while, I thought I had found the piece I was looking for. I thought maybe this was it. It seemed to be about right, but I couldn't quite tell. I couldn't tell what those ones and zeros represented. So I spent some time trying to put this together, but wasn't having a whole lot of luck, and finally I decided, I'm going to get through this, I'm going to come in on a weekend, and I'm not going to leave until I figure out what this represents. So that's what I did. I came in on a Saturday morning, and about 10 hours in, I sort of had all the pieces to the puzzle. I just didn't know how they fit together. I didn't know what these ones and zeros meant. At the 15-hour mark, I started to get a better picture of what was there, but I had a creeping suspicion that what I was looking at was not at all related to what I was looking for. By 20 hours, the pieces started to come together very slowly — (Laughter) — and I was pretty sure I was going down the wrong path at this point, but I wasn't going to give up. After 30 hours in the lab, I figured out exactly what I was looking at, and I was right, it wasn't what I was looking for. I spent 30 hours piecing together the ones and zeros that formed a picture of a kitten. (Laughter) I wasted 30 hours of my life searching for this kitten that had nothing at all to do with what I was trying to accomplish.
So I was frustrated, I was exhausted. After 30 hours in the lab, I probably smelled horrible. But instead of just going home and calling it quits, I took a step back and asked myself, what went wrong here? How could I make such a stupid mistake? I'm really pretty good at this. I do this for a living. So what happened? Well I thought, when you're looking at information at this level, it's so easy to lose track of what you're doing. It's easy to not see the forest through the trees. It's easy to go down the wrong rabbit hole and waste a tremendous amount of time doing the wrong thing. But I had this epiphany. We were looking at the data completely incorrectly since day one. This is how computers think, ones and zeros. It's not how people think, but we've been trying to adapt our minds to think more like computers so that we can understand this information. Instead of trying to make our minds fit the problem, we should have been making the problem fit our minds, because our brains have a tremendous potential for analyzing huge amounts of information, just not like this. So what if we could unlock that potential just by translating this to the right kind of information? So with these ideas in mind, I sprinted out of my basement lab at work to my basement lab at home, which looked pretty much the same. The main difference is, at work, I'm surrounded by cyber materials, and cyber seemed to be the problem in this situation. At home, I'm surrounded by everything else I've ever learned. So I poured through every book I could find, every idea I'd ever encountered, to see how could we translate a problem from one domain to something completely different?
The biggest question was, what do we want to translate it to? What do our brains do perfectly naturally that we could exploit? My answer was vision. We have a tremendous capability to analyze visual information. We can combine color gradients, depth cues, all sorts of these different signals into one coherent picture of the world around us. That's incredible. So if we could find a way to translate these binary patterns to visual signals, we could really unlock the power of our brains to process this stuff. So I started looking at the binary information, and I asked myself, what do I do when I first encounter something like this? And the very first thing I want to do, the very first question I want to answer, is what is this? I don't care what it does, how it works. All I want to know is, what is this? And the way I can figure that out is by looking at chunks, sequential chunks of binary information, and I look at the relationships between those chunks. When I gather up enough of these sequences, I begin to get an idea of exactly what this information must be. So let's go back to that blow up the terrorist's phone situation. This is what English text looks like at a binary level. This is what your contacts list would look like if I were examining it. It's really hard to analyze this at this level, but if we take those same binary chunks that I would be trying to find, and instead translate that to a visual representation, translate those relationships, this is what we get. This is what English text looks like from a visual abstraction perspective. All of a sudden, it shows us all the same information that was in the ones and zeros, but show us it in an entirely different way, a way that we can immediately comprehend. We can instantly see all of the patterns here. It takes me seconds to pick out patterns here, but hours, days, to pick them out in ones and zeros. It takes minutes for anybody to learn what these patterns represent here, but years of experience in cyber to learn what those same patterns represent in ones and zeros. So this piece is caused by lower case letters followed by lower case letters inside of that contact list. This is upper case by upper case, upper case by lower case, lower case by upper case. This is caused by spaces. This is caused by carriage returns. We can go through every little detail of the binary information in seconds, as opposed to weeks, months, at this level. This is what an image looks like from your cell phone. But this is what it looks like in a visual abstraction. This is what your music looks like, but here's its visual abstraction. Most importantly for me, this is what the code on your cell phone looks like. This is what I'm after in the end, but this is its visual abstraction. If I can find this, I can't make the phone explode. I could spend weeks trying to find this in ones and zeros, but it takes me seconds to pick out a visual abstraction like this.
One of those most remarkable parts about all of this is it gives us an entirely new way to understand new information, stuff that we haven't seen before. So I know what English looks like at a binary level, and I know what its visual abstraction looks like, but I've never seen Russian binary in my entire life. It would take me weeks just to figure out what I was looking at from raw ones and zeros, but because our brains can instantly pick up and recognize these subtle patterns inside of these visual abstractions, we can unconsciously apply those in new situations. So this is what Russian looks like in a visual abstraction. Because I know what one language looks like, I can recognize other languages even when I'm not familiar with them. This is what a photograph looks like, but this is what clip art looks like. This is what the code on your phone looks like, but this is what the code on your computer looks like. Our brains can pick up on these patterns in ways that we never could have from looking at raw ones and zeros. But we've really only scratched the surface of what we can do with this approach. We've only begun to unlock the capabilities of our minds to process visual information. If we take those same concepts and translate them into three dimensions instead, we find entirely new ways of making sense of information. In seconds, we can pick out every pattern here. we can see the cross associated with code. We can see cubes associated with text. We can even pick up the tiniest visual artifacts. Things that would take us weeks, months to find in ones and zeroes, are immediately apparent in some sort of visual abstraction, and as we continue to go through this and throw more and more information at it, what we find is that we're capable of processing billions of ones and zeros in a matter of seconds just by using our brain's built-in ability to analyze patterns.
So this is really nice and helpful, but all this tells me is what I'm looking at. So at this point, based on visual patterns, I can find the code on the phone. But that's not enough to blow up a battery. The next thing I need to find is the code that controls the battery, but we're back to the needle in a stack of needles problem. That code looks pretty much like all the other code on that system.
So I might not be able to find the code that controls the battery, but there's a lot of things that are very similar to that. You have code that controls your screen, that controls your buttons, that controls your microphones, so even if I can't find the code for the battery, I bet I can find one of those things. So the next step in my binary analysis process is to look at pieces of information that are similar to each other. It's really, really hard to do at a binary level, but if we translate those similarities to a visual abstraction instead, I don't even have to sift through the raw data. All I have to do is wait for the image to light up to see when I'm at similar pieces. I follow these strands of similarity like a trail of bread crumbs to find exactly what I'm looking for.
So at this point in the process, I've located the code responsible for controlling your battery, but that's still not enough to blow up a phone. The last piece of the puzzle is understanding how that code controls your battery. For this, I need to identify very subtle, very detailed relationships within that binary information, another very hard thing to do when looking at ones and zeros. But if we translate that information into a physical representation, we can sit back and let our visual cortex do all the hard work. It can find all the detailed patterns, all the important pieces, for us. It can find out exactly how the pieces of that code work together to control that battery. All of this can be done in a matter of hours, whereas the same process would have taken months in the past.
This is all well and good in a theoretical blow up a terrorist's phone situation. I wanted to find out if this would really work in the work I do every day. So I was playing around with these same concepts with some of the data I've looked at in the past, and yet again, I was trying to find a very detailed, specific piece of code inside of a massive piece of binary information. So I looked at it at this level, thinking I was looking at the right thing, only to see this doesn't have the connectivity I would have expected for the code I was looking for. In fact, I'm not really sure what this is, but when I stepped back a level and looked at the similarities within the code I saw, this doesn't have similarities like any code that exists out there. I can't even be looking at code. In fact, from this perspective, I could tell, this isn't code. This is an image of some sort. And from here, I can see, it's not just an image, this is a photograph. Now that I know it's a photograph, I've got dozens of other binary translation techniques to visualize and understand that information, so in a matter of seconds, we can take this information, shove it through a dozen other visual translation techniques in order to find out exactly what we were looking at. I saw — (Laughter) — it was that darn kitten again. All this is enabled because we were able to find a way to translate a very hard problem to something our brains do very naturally.
So what does this mean? Well, for kittens, it means no more hiding in ones and zeros. For me, it means no more wasted weekends. For cyber, it means we have a radical new way to tackle the most impossible problems. It means we have a new weapon in the evolving theater of cyber warfare, but for all of us, it means that cyber engineers now have the ability to become first responders in emergency situations. When seconds count, we've unlocked the means to stop the bad guys.
In my lab, we build autonomous aerial robots like the one you see flying here. Unlike the commercially available drones that you can buy today, this robot doesn't have any GPS on board. So without GPS, it's hard for robots like this to determine their position. This robot uses onboard sensors, cameras and laser scanners, to scan the environment. It detects features from the environment, and it determines where it is relative to those features, using a method of triangulation. And then it can assemble all these features into a map, like you see behind me. And this map then allows the robot to understand where the obstacles are and navigate in a collision-free manner.
What I want to show you next is a set of experiments we did inside our laboratory, where this robot was able to go for longer distances. So here you'll see, on the top right, what the robot sees with the camera. And on the main screen -- and of course this is sped up by a factor of four -- on the main screen you'll see the map that it's building. So this is a high-resolution map of the corridor around our laboratory. And in a minute you'll see it enter our lab, which is recognizable by the clutter that you see.
But the main point I want to convey to you is that these robots are capable of building high-resolution maps at five centimeters resolution, allowing somebody who is outside the lab, or outside the building to deploy these without actually going inside, and trying to infer what happens inside the building.
Now there's one problem with robots like this. The first problem is it's pretty big. Because it's big, it's heavy. And these robots consume about 100 watts per pound. And this makes for a very short mission life. The second problem is that these robots have onboard sensors that end up being very expensive -- a laser scanner, a camera and the processors. That drives up the cost of this robot.
So we asked ourselves a question: what consumer product can you buy in an electronics store that is inexpensive, that's lightweight, that has sensing onboard and computation? And we invented the flying phone.
So this robot uses a Samsung Galaxy smartphone that you can buy off the shelf, and all you need is an app that you can download from our app store. And you can see this robot reading the letters, "TED" in this case, looking at the corners of the "T" and the "E" and then triangulating off of that, flying autonomously. That joystick is just there to make sure if the robot goes crazy, Giuseppe can kill it.
In addition to building these small robots, we also experiment with aggressive behaviors, like you see here. So this robot is now traveling at two to three meters per second, pitching and rolling aggressively as it changes direction. The main point is we can have smaller robots that can go faster and then travel in these very unstructured environments.
And in this next video, just like you see this bird, an eagle, gracefully coordinating its wings, its eyes and feet to grab prey out of the water, our robot can go fishing, too.
In this case, this is a Philly cheesesteak hoagie that it's grabbing out of thin air.
So you can see this robot going at about three meters per second, which is faster than walking speed, coordinating its arms, its claws and its flight with split-second timing to achieve this maneuver. In another experiment, I want to show you how the robot adapts its flight to control its suspended payload, whose length is actually larger than the width of the window. So in order to accomplish this, it actually has to pitch and adjust the altitude and swing the payload through. But of course we want to make these even smaller, and we're inspired in particular by honeybees. So if you look at honeybees, and this is a slowed down video, they're so small, the inertia is so lightweight --
that they don't care -- they bounce off my hand, for example. This is a little robot that mimics the honeybee behavior. And smaller is better, because along with the small size you get lower inertia. Along with lower inertia --
along with lower inertia, you're resistant to collisions. And that makes you more robust. So just like these honeybees, we build small robots. And this particular one is only 25 grams in weight. It consumes only six watts of power. And it can travel up to six meters per second. So if I normalize that to its size, it's like a Boeing 787 traveling ten times the speed of sound.
And I want to show you an example. This is probably the first planned mid-air collision, at one-twentieth normal speed. These are going at a relative speed of two meters per second, and this illustrates the basic principle. The two-gram carbon fiber cage around it prevents the propellers from entangling, but essentially the collision is absorbed and the robot responds to the collisions. And so small also means safe. In my lab, as we developed these robots, we start off with these big robots and then now we're down to these small robots. And if you plot a histogram of the number of Band-Aids we've ordered in the past, that sort of tailed off now. Because these robots are really safe.
The small size has some disadvantages, and nature has found a number of ways to compensate for these disadvantages. The basic idea is they aggregate to form large groups, or swarms. So, similarly, in our lab, we try to create artificial robot swarms. And this is quite challenging because now you have to think about networks of robots. And within each robot, you have to think about the interplay of sensing, communication, computation -- and this network then becomes quite difficult to control and manage. So from nature we take away three organizing principles that essentially allow us to develop our algorithms. The first idea is that robots need to be aware of their neighbors. They need to be able to sense and communicate with their neighbors.
So this video illustrates the basic idea. You have four robots -- one of the robots has actually been hijacked by a human operator, literally. But because the robots interact with each other, they sense their neighbors, they essentially follow. And here there's a single person able to lead this network of followers. So again, it's not because all the robots know where they're supposed to go. It's because they're just reacting to the positions of their neighbors.
So the next experiment illustrates the second organizing principle. And this principle has to do with the principle of anonymity. Here the key idea is that the robots are agnostic to the identities of their neighbors. They're asked to form a circular shape, and no matter how many robots you introduce into the formation, or how many robots you pull out, each robot is simply reacting to its neighbor. It's aware of the fact that it needs to form the circular shape, but collaborating with its neighbors it forms the shape without central coordination. Now if you put these ideas together, the third idea is that we essentially give these robots mathematical descriptions of the shape they need to execute. And these shapes can be varying as a function of time, and you'll see these robots start from a circular formation, change into a rectangular formation, stretch into a straight line, back into an ellipse. And they do this with the same kind of split-second coordination that you see in natural swarms, in nature.
So why work with swarms? Let me tell you about two applications that we are very interested in. The first one has to do with agriculture, which is probably the biggest problem that we're facing worldwide. As you well know, one in every seven persons in this earth is malnourished. Most of the land that we can cultivate has already been cultivated. And the efficiency of most systems in the world is improving, but our production system efficiency is actually declining. And that's mostly because of water shortage, crop diseases, climate change and a couple of other things.
So what can robots do? Well, we adopt an approach that's called Precision Farming in the community. And the basic idea is that we fly aerial robots through orchards, and then we build precision models of individual plants. So just like personalized medicine, while you might imagine wanting to treat every patient individually, what we'd like to do is build models of individual plants and then tell the farmer what kind of inputs every plant needs -- the inputs in this case being water, fertilizer and pesticide. Here you'll see robots traveling through an apple orchard, and in a minute you'll see two of its companions doing the same thing on the left side. And what they're doing is essentially building a map of the orchard. Within the map is a map of every plant in this orchard.
Let's see what those maps look like. In the next video, you'll see the cameras that are being used on this robot. On the top-left is essentially a standout color camera. On the left-center is an infrared camera. And on the bottom-left is a thermal camera. And on the main panel, you're seeing a three-dimensional reconstruction of every tree in the orchard as the sensors fly right past the trees. Armed with information like this, we can do several things. The first and possibly the most important thing we can do is very simple: count the number of fruits on every tree. By doing this, you tell the farmer how many fruits she has in every tree and allow her to estimate the yield in the orchard, optimizing the production chain downstream.
The second thing we can do is take models of plants, construct three-dimensional reconstructions, and from that estimate the canopy size, and then correlate the canopy size to the amount of leaf area on every plant. And this is called the leaf area index. So if you know this leaf area index, you essentially have a measure of how much photosynthesis is possible in every plant, which again tells you how healthy each plant is. By combining visual and infrared information, we can also compute indices such as NDVI. And in this particular case, you can essentially see there are some crops that are not doing as well as other crops. This is easily discernible from imagery, not just visual imagery but combining both visual imagery and infrared imagery.
And then lastly, one thing we're interested in doing is detecting the early onset of chlorosis -- and this is an orange tree -- which is essentially seen by yellowing of leaves. But robots flying overhead can easily spot this autonomously and then report to the farmer that he or she has a problem in this section of the orchard.
Systems like this can really help, and we're projecting yields that can improve by about ten percent and, more importantly, decrease the amount of inputs such as water by 25 percent by using aerial robot swarms.
Lastly, I want you to applaud the people who actually create the future, Yash Mulgaonkar, Sikang Liu and Giuseppe Loianno, who are responsible for the three demonstrations that you saw.
I want to talk to you about one thing and just one thing only, and this has to do with when people ask me, what do you do? To which I usually respond, I do computer music.
Now, a number of people just stop talking to me right then and there, and the rest who are left usually have this blank look in their eye, as if to say, what does that mean? And I feel like I'm actually depriving them of information by telling them this, at which point I usually panic and spit out the first thing that comes to my mind, which is, I have no idea what I'm doing. Which is true. That's usually followed by a second thought, which is, whatever it is that I'm doing, I love it. And today, I want to, well, share with you something I love, and also why.
And I think we'll begin with just this question: What is computer music? And I'm going to try to do my best to provide a definition, maybe by telling you a story that goes through some of the stuff I've been working on.
And the first thing, I think, in our story is going to be something called ChucK. Now, ChucK is a programming language for music, and it's open-source, it's freely available, and I like to think that it crashes equally well on all modern operating systems. And instead of telling you more about it, I'm just going to give you a demo. By the way, I'm just going to nerd out for just a few minutes here, so I would say, don't freak out. In fact, I would invite all of you to join me in just geeking out. If you've never written a line of code before in your life, do not worry. I'll bet you'll be able to come along on this.
First thing I'm going to do is to make a sine wave oscillator, and we're going to called the sine wave generator "Ge." And then we're going to connect "Ge" to the DAC. Now this is kind of the abstraction for the sound output on my computer. Okay? So I've connected myself into the speaker. Next, I'm going to say my frequency is 440 hertz, and I'm going to let time advance by two seconds through this operation. All right, so if I were to play this -- (Tone) — you would hear a sine wave at 440 hertz for two seconds. Okay, great. Now I'm going to copy and paste this, and then just change some of these numbers, 220.5, 440 I shall leave it as that, and .5 and 880. By doubling the frequency, we're actually going up in successive octaves, and then we have this sequence -- (Tones) — of tones. Okay, great, now I can imagine creating all kinds of really horrible single sine wave pieces of music with this, but I'm going to do something that computers are really good at, which is repetition. I'm going to put this all in a while loop, and you actually don't need to indent, but this is purely for aesthetic reasons. It's good practice. And when we do this — (Tones) — that's going to go on for a while. In fact, it's probably not going to stop until this computer disintegrates. And I can't really empirically prove that to you, but I hope you'll believe me when I say that. Next, I'm going to replace this 220 by math.random2f. I'm going to generate a random number between 30 and 1,000 and send that to the frequency of me. And I'm going to do this every half a second. (Tones) Let's do this every 200 milliseconds. (Tones) One hundred. (Tones) All right. At this point, we've reached something that I would like to think of as the canonical computer music. This is, to me, the sound that mainframes are supposed to be making when they're thinking really hard. It's this sound, it's like, the square root of five million.
So is this computer music? Yeah, I guess by definition, it's kind of computer music. It's probably not the kind of music you would listen to cruising down the highway, but it's a foundation of computer-generated music, and using ChucK, we've actually been building instruments in the Stanford Laptop Orchestra, based right here at Stanford Center for Computer Research in Music and Acoustics.
Now the Laptop Orchestra is an ensemble of laptops, humans and special hemispherical speaker arrays. Now the reason we have these is so that for the instruments that we create out of the laptop, we want the sound to come out of somewhere near the instrument and the performer, kind of much like a traditional, acoustic instrument. Like, if I were to play a violin here, the sound would naturally not come out of the P.A. system, but from the artifact itself. So these speakers are meant to emulate that. In fact, I'm going to show you how we actually built them. The first step is to go to IKEA and buy a salad bowl. This is an 11-inch Blanda Matt. That's the actual name, and I actually use one of these to make salad at home as well, I kid you not. And the first step is you turn it upside down, and then you drill holes in them, six holes per hemi, and then make a base plate, put car speaker drivers in them along with amplifiers in the enclosure, and you put that all together and you have these hemispherical speaker arrays. Add people, add laptops, you have a laptop orchestra.
And what might a laptop orchestra sound like? Well, let me give you a demonstration of about 200 instruments we've created so far for the Laptop Orchestra. And what I'm going to do is actually come over to this thing. This thing I have in front of me actually used to be a commodity gaming controller called a Gametrak. This thing actually has a glove you can put on your hands. It's tethered to the base, and this will track the position of your hands in real time. It was originally designed as a golfing controller to detect the motion of your swing. That turned out to be a rather large commercial non-success, at which point they slashed prices to 10 dollars, at which point computer music researchers said, "This is awesome! We can prototype instruments out of this."
So let me show you one instrument we've created, one of many, and this instrument is called "Twilight," and it's meant to go with this metaphor of pulling a sound out of the ground. So let me see if this will work.
And put it back. And then if you go to the left, right, it sounds like an elephant in pain.
This is a slightly metallic sound. Turn it just a bit.
 (Music) It's like a hovering car.
Okay.
This third one is a ratchet-like interaction, so let me turn it up.
So it's a slightly different interaction.
The fourth one is a drone.
 (Music) And finally, let's see, this is a totally different interaction, and I think you have to imagine that there's this giant invisible drum sitting right here on stage, and I'm going to bang it. (Drum) (Laughter) So there we go, so that's one of many instruments in the Laptop Orchestra.
And when you put that together, you get something that sounds like this.
Okay, and so, I think from the experience of building a lot of instruments for the Laptop Orchestra, and I think from the curiosity of wondering, what if we took these hopefully expressive instruments and we brought it to a lot of people, plus then a healthy bout of insanity — put those three things together — led to me actually co-founding a startup company in 2008 called Smule.
Now Smule's mission is to create expressive, mobile music things, and one of the first musical instruments we created is called Ocarina. And I'm going to just demo this for you real quick. So Ocarina — (Music) — is based on this ancient flute-like instrument called the ocarina, and this one is the four-hole English pendant configuration, and you're literally blowing into the microphone to make the sound. And there's actually a little ChucK script running in here that's detecting the strength of your blowing and also synthesizing the sound. (Music) And vibrato is mapped to the accelerometer, so you can get — (Music) All right. So let me play a little ditty for you, a little Bach. And here, you'll hear a little accompaniment with the melody. The accompaniment actually follows the melody, not the other way around.
And this was designed to let you take your time and figure out where your expressive space is, and you can just hang out here for a while, for a really dramatic effect, if you want, and whenever you're ready -And on these longer notes, I'm going to use more vibrato towards the end of the notes to give it a little bit more of an expressive quality.
Huh, that's a nice chord to end this excerpt on.
So I think a good question to ask about Ocarina is, is this a toy or it an instrument? Maybe it's both, but for me, I think the more important question is, is it expressive? And at the same time, I think creating these types of instruments asks a question about the role of technology, and its place for how we make music. Apparently, for example, not that long ago, like only a hundred years ago — that's not that long in the course of human history — families back then used to make music together as a common form of entertainment. I don't think that's really happening that much anymore. You know, this is before radio, before recording. In the last hundred years, with all this technology, we now have more access to music as listeners and consumers, but somehow, I think we're making less music than ever before. I'm not sure why that would be. Maybe it's because it's too easy just to hit play. And while listening to music is wonderful, there's a special joy to making music that's all its own. And I think that's one part of the goal of why I do what I do is kind of to take us back to the past a little bit. Right?
Now, if that's one goal, the other goal is to look to the future and think about what kind of new musical things can we make that we don't perhaps yet have names for that's enabled by technology, but ultimately might change the way that humans make music. And I'll just give you one example here, and this is Ocarina's other feature. This is a globe, and here you're actually listening to other users of Ocarina blow into their iPhones to play something. This is "G.I.R." from Texas, "R.I.K." I don't know why it's these three-letter names today, Los Angeles. They're all playing pretty, somewhat minimal music here.
And the idea with this is that, well, technology should not be foregrounded here, and — (Laughter) — we've actually opened this up. The first thought is that, hey, you know there's somebody somewhere out there playing some music, and this is a small but I think important human connection to make that perhaps the technology affords.
As a final example, and perhaps my favorite example, is that in the wake of the 2011 earthquake and tsunami disaster in Japan, a woman reached out in one of our singing apps to try to get people to join in to sing with her on a version of "Lean on Me." Now, in these apps, there's this thing that allows any user to add their voice to an existing performance by any other user or group of users, so in some sense, she's created this kind of global ad hoc corral of strangers, and within weeks, thousands of people joined in on this, and you can kind of see people coming from all around the world and all these lines converging on the origin where the first rendition of the song was sung, and that's in Tokyo. And this is what it sounds like when there's 1,000 people. This is 1,000 voices.
 (Recording) ? Sometimes in our lives ?
? We all have pain, we all have sorrow ?
? But if we are wise ?
? We know that there's always tomorrow ?
? Lean on me ?
? When you're not strong ?
? And I'll be your friend ?
? I'll help you carry on ?
? For it won't be long ?
? Till I'm gonna need ?
? Somebody to lean on ?
? Just lean on — ?
Is this computer music?
Was that computer music? Yeah, I guess so; it's something that you really couldn't have done without computers. But at the same time, it's also just human, and I think what I've essentially answered so far is maybe why I do the stuff that I do, and let's just finally return to the first question: What is computer music? And I think that the catch here is that, at least to me, computer music isn't really about computers. It is about people. It's about how we can use technology to change the way we think and do and make music, and maybe even add to how we can connect with each other through music.
And with that, I want to say, this is computer music, and thank you for listening.
In the great 1980s movie "The Blues Brothers," there's a scene where John Belushi goes to visit Dan Aykroyd in his apartment in Chicago for the very first time. It's a cramped, tiny space and it's just three feet away from the train tracks. As John sits on Dan's bed, a train goes rushing by, rattling everything in the room. John asks, "How often does that train go by?" Dan replies, "So often, you won't even notice it." And then, something falls off the wall.
We all know what he's talking about. As human beings, we get used to everyday things really fast. As a product designer, it's my job to see those everyday things, to feel them, and try to improve upon them. For example, see this piece of fruit? See this little sticker? That sticker wasn't there when I was a kid. But somewhere as the years passed, someone had the bright idea to put that sticker on the fruit. Why? So it could be easier for us to check out at the grocery counter.
Well that's great, we can get in and out of the store quickly. But now, there's a new problem. When we get home and we're hungry and we see this ripe, juicy piece of fruit on the counter, we just want to pick it up and eat it. Except now, we have to look for this little sticker. And dig at it with our nails, damaging the flesh. Then rolling up that sticker -- you know what I mean. And then trying to flick it off your fingers. (Applause) It's not fun, not at all.
But something interesting happened. See the first time you did it, you probably felt those feelings. You just wanted to eat the piece of fruit. You felt upset. You just wanted to dive in. By the 10th time, you started to become less upset and you just started peeling the label off. By the 100th time, at least for me, I became numb to it. I simply picked up the piece of fruit, dug at it with my nails, tried to flick it off, and then wondered, "Was there another sticker?"
So why is that? Why do we get used to everyday things? Well as human beings, we have limited brain power. And so our brains encode the everyday things we do into habits so we can free up space to learn new things. It's a process called habituation and it's one of the most basic ways, as humans, we learn.
Now, habituation isn't always bad. Remember learning to drive? I sure do. Your hands clenched at 10 and 2 on the wheel, looking at every single object out there -- the cars, the lights, the pedestrians. It's a nerve-wracking experience. So much so, that I couldn't even talk to anyone else in the car and I couldn't even listen to music. But then something interesting happened. As the weeks went by, driving became easier and easier. You habituated it. It started to become fun and second nature. And then, you could talk to your friends again and listen to music.
So there's a good reason why our brains habituate things. If we didn't, we'd notice every little detail, all the time. It would be exhausting, and we'd have no time to learn about new things.
But sometimes, habituation isn't good. If it stops us from noticing the problems that are around us, well, that's bad. And if it stops us from noticing and fixing those problems, well, then that's really bad.
Comedians know all about this. Jerry Seinfeld's entire career was built on noticing those little details, those idiotic things we do every day that we don't even remember. He tells us about the time he visited his friends and he just wanted to take a comfortable shower. He'd reach out and grab the handle and turn it slightly one way, and it was 100 degrees too hot. And then he'd turn it the other way, and it was 100 degrees too cold. He just wanted a comfortable shower. Now, we've all been there, we just don't remember it. But Jerry did, and that's a comedian's job.
But designers, innovators and entrepreneurs, it's our job to not just notice those things, but to go one step further and try to fix them.
See this, this person, this is Mary Anderson. In 1902 in New York City, she was visiting. It was a cold, wet, snowy day and she was warm inside a streetcar. As she was going to her destination, she noticed the driver opening the window to clean off the excess snow so he could drive safely. When he opened the window, though, he let all this cold, wet air inside, making all the passengers miserable. Now probably, most of those passengers just thought, "It's a fact of life, he's got to open the window to clean it. That's just how it is." But Mary didn't. Mary thought, "What if the diver could actually clean the windshield from the inside so that he could stay safe and drive and the passengers could actually stay warm?" So she picked up her sketchbook right then and there, and began drawing what would become the world's first windshield wiper.
Now as a product designer, I try to learn from people like Mary to try to see the world the way it really is, not the way we think it is. Why? Because it's easy to solve a problem that almost everyone sees. But it's hard to solve a problem that almost no one sees.
Now some people think you're born with this ability or you're not, as if Mary Anderson was hardwired at birth to see the world more clearly. That wasn't the case for me. I had to work at it. During my years at Apple, Steve Jobs challenged us to come into work every day, to see our products through the eyes of the customer, the new customer, the one that has fears and possible frustrations and hopeful exhilaration that their new technology product could work straightaway for them. He called it staying beginners, and wanted to make sure that we focused on those tiny little details to make them faster, easier and seamless for the new customers.
So I remember this clearly in the very earliest days of the iPod. See, back in the '90s, being a gadget freak like I am, I would rush out to the store for the very, very latest gadget. I'd take all the time to get to the store, I'd check out, I'd come back home, I'd start to unbox it. And then, there was another little sticker: the one that said, "Charge before use."
What! I can't believe it! I just spent all this time buying this product and now I have to charge before use. I have to wait what felt like an eternity to use that coveted new toy. It was crazy.
But you know what? Almost every product back then did that. When it had batteries in it, you had to charge it before you used it. Well, Steve noticed that and he said, "We're not going to let that happen to our product." So what did we do? Typically, when you have a product that has a hard drive in it, you run it for about 30 minutes in the factory to make sure that hard drive's going to be working years later for the customer after they pull it out of the box. What did we do instead? We ran that product for over two hours. Why? Well, first off, we could make a higher quality product, be easy to test, and make sure it was great for the customer. But most importantly, the battery came fully charged right out of the box, ready to use. So that customer, with all that exhilaration, could just start using the product. It was great, and it worked. People liked it.
Today, almost every product that you get that's battery powered comes out of the box fully charged, even if it doesn't have a hard drive. But back then, we noticed that detail and we fixed it, and now everyone else does that as well. No more, "Charge before use."
So why am I telling you this? Well, it's seeing the invisible problem, not just the obvious problem, that's important, not just for product design, but for everything we do. You see, there are invisible problems all around us, ones we can solve. But first we need to see them, to feel them.
So, I'm hesitant to give you any tips about neuroscience or psychology. There's far too many experienced people in the TED community who would know much more about that than I ever will. But let me leave you with a few tips that I do, that we all can do, to fight habituation.
My first tip is to look broader. You see, when you're tackling a problem, sometimes, there are a lot of steps that lead up to that problem. And sometimes, a lot of steps after it. If you can take a step back and look broader, maybe you can change some of those boxes before the problem. Maybe you can combine them. Maybe you can remove them altogether to make that better.
Take thermostats, for instance. In the 1900s when they first came out, they were really simple to use. You could turn them up or turn them down. People understood them. But in the 1970s, the energy crisis struck, and customers started thinking about how to save energy. So what happened? Thermostat designers decided to add a new step. Instead of just turning up and down, you now had to program it. So you could tell it the temperature you wanted at a certain time. Now that seemed great. Every thermostat had started adding that feature. But it turned out that no one saved any energy. Now, why is that? Well, people couldn't predict the future. They just didn't know how their weeks would change season to season, year to year. So no one was saving energy, and what happened?
Thermostat designers went back to the drawing board and they focused on that programming step. They made better U.I.s, they made better documentation. But still, years later, people were not saving any energy because they just couldn't predict the future. So what did we do? We put a machine-learning algorithm in instead of the programming that would simply watch when you turned it up and down, when you liked a certain temperature when you got up, or when you went away. And you know what? It worked. People are saving energy without any programming.
So, it doesn't matter what you're doing. If you take a step back and look at all the boxes, maybe there's a way to remove one or combine them so that you can make that process much simpler. So that's my first tip: look broader.
For my second tip, it's to look closer. One of my greatest teachers was my grandfather. He taught me all about the world. He taught me how things were built and how they were repaired, the tools and techniques necessary to make a successful project. I remember one story he told me about screws, and about how you need to have the right screw for the right job. There are many different screws: wood screws, metal screws, anchors, concrete screws, the list went on and on. Our job is to make products that are easy to install for all of our customs themselves without professionals. So what did we do? I remembered that story that my grandfather told me, and so we thought, "How many different screws can we put in the box? Was it going to be two, three, four, five? Because there's so many different wall types." So we thought about it, we optimized it, and we came up with three different screws to put in the box. We thought that was going to solve the problem. But it turned out, it didn't.
So we shipped the product, and people weren't having a great experience. So what did we do? We went back to the drawing board just instantly after we figured out we didn't get it right. And we designed a special screw, a custom screw, much to the chagrin of our investors. They were like, "Why are you spending so much time on a little screw? Get out there and sell more!" And we said, "We will sell more if we get this right." And it turned out, we did. With that custom little screw, there was just one screw in the box, that was easy to mount and put on the wall.
So if we focus on those tiny details, the ones we may not see and we look at them as we say, "Are those important or is that the way we've always done it? Maybe there's a way to get rid of those."
So my last piece of advice is to think younger. Every day, I'm confronted with interesting questions from my three young kids. They come up with questions like, "Why can't cars fly around traffic?" Or, "Why don't my shoelaces have Velcro instead?" Sometimes, those questions are smart. My son came to me the other day and I asked him, "Go run out to the mailbox and check it." He looked at me, puzzled, and said, "Why doesn't the mailbox just check itself and tell us when it has mail?" (Laughter) I was like, "That's a pretty good question." So, they can ask tons of questions and sometimes we find out we just don't have the right answers. We say, "Son, that's just the way the world works." So the more we're exposed to something, the more we get used to it. But kids haven't been around long enough to get used to those things. And so when they run into problems, they immediately try to solve them, and sometimes they find a better way, and that way really is better.
So my advice that we take to heart is to have young people on your team, or people with young minds. Because if you have those young minds, they cause everyone in the room to think younger. Picasso once said, "Every child is an artist. The problem is when he or she grows up, is how to remain an artist." We all saw the world more clearly when we saw it for the first time, before a lifetime of habits got in the way. Our challenge is to get back there, to feel that frustration, to see those little details, to look broader, look closer, and to think younger so we can stay beginners.
It's not easy. It requires us pushing back against one of the most basic ways we make sense of the world. But if we do, we could do some pretty amazing things. For me, hopefully, that's better product design. For you, that could mean something else, something powerful.
Our challenge is to wake up each day and say, "How can I experience the world better?" And if we do, maybe, just maybe, we can get rid of these dumb little stickers.
Thank you very much.
I write fiction sci-fi thrillers, so if I say "killer robots," you'd probably think something like this. But I'm actually not here to talk about fiction. I'm here to talk about very real killer robots, autonomous combat drones.
Now, I'm not referring to Predator and Reaper drones, which have a human making targeting decisions. I'm talking about fully autonomous robotic weapons that make lethal decisions about human beings all on their own. There's actually a technical term for this: lethal autonomy.
Now, lethally autonomous killer robots would take many forms -- flying, driving, or just lying in wait. And actually, they're very quickly becoming a reality. These are two automatic sniper stations currently deployed in the DMZ between North and South Korea. Both of these machines are capable of automatically identifying a human target and firing on it, the one on the left at a distance of over a kilometer. Now, in both cases, there's still a human in the loop to make that lethal firing decision, but it's not a technological requirement. It's a choice. And it's that choice that I want to focus on, because as we migrate lethal decision-making from humans to software, we risk not only taking the humanity out of war, but also changing our social landscape entirely, far from the battlefield. That's because the way humans resolve conflict shapes our social landscape. And this has always been the case, throughout history.
For example, these were state-of-the-art weapons systems in 1400 A.D. Now they were both very expensive to build and maintain, but with these you could dominate the populace, and the distribution of political power in feudal society reflected that. Power was focused at the very top. And what changed? Technological innovation. Gunpowder, cannon. And pretty soon, armor and castles were obsolete, and it mattered less who you brought to the battlefield versus how many people you brought to the battlefield. And as armies grew in size, the nation-state arose as a political and logistical requirement of defense. And as leaders had to rely on more of their populace, they began to share power. Representative government began to form.
So again, the tools we use to resolve conflict shape our social landscape. Autonomous robotic weapons are such a tool, except that, by requiring very few people to go to war, they risk re-centralizing power into very few hands, possibly reversing a five-century trend toward democracy.
Now, I think, knowing this, we can take decisive steps to preserve our democratic institutions, to do what humans do best, which is adapt. But time is a factor. Seventy nations are developing remotely-piloted combat drones of their own, and as you'll see, remotely-piloted combat drones are the precursors to autonomous robotic weapons. That's because once you've deployed remotely-piloted drones, there are three powerful factors pushing decision-making away from humans and on to the weapon platform itself.
The first of these is the deluge of video that drones produce. For example, in 2004, the U.S. drone fleet produced a grand total of 71 hours of video surveillance for analysis. By 2011, this had gone up to 300,000 hours, outstripping human ability to review it all, but even that number is about to go up drastically. The Pentagon's Gorgon Stare and Argus programs will put up to 65 independently operated camera eyes on each drone platform, and this would vastly outstrip human ability to review it. And that means visual intelligence software will need to scan it for items of interest. And that means very soon drones will tell humans what to look at, not the other way around.
But there's a second powerful incentive pushing decision-making away from humans and onto machines, and that's electromagnetic jamming, severing the connection between the drone and its operator. Now we saw an example of this in 2011 when an American RQ-170 Sentinel drone got a bit confused over Iran due to a GPS spoofing attack, but any remotely-piloted drone is susceptible to this type of attack, and that means drones will have to shoulder more decision-making. They'll know their mission objective, and they'll react to new circumstances without human guidance. They'll ignore external radio signals and send very few of their own.
Which brings us to, really, the third and most powerful incentive pushing decision-making away from humans and onto weapons: plausible deniability. Now we live in a global economy. High-tech manufacturing is occurring on most continents. Cyber espionage is spiriting away advanced designs to parts unknown, and in that environment, it is very likely that a successful drone design will be knocked off in contract factories, proliferate in the gray market. And in that situation, sifting through the wreckage of a suicide drone attack, it will be very difficult to say who sent that weapon.
This raises the very real possibility of anonymous war. This could tilt the geopolitical balance on its head, make it very difficult for a nation to turn its firepower against an attacker, and that could shift the balance in the 21st century away from defense and toward offense. It could make military action a viable option not just for small nations, but criminal organizations, private enterprise, even powerful individuals. It could create a landscape of rival warlords undermining rule of law and civil society. Now if responsibility and transparency are two of the cornerstones of representative government, autonomous robotic weapons could undermine both.
Now you might be thinking that citizens of high-tech nations would have the advantage in any robotic war, that citizens of those nations would be less vulnerable, particularly against developing nations. But I think the truth is the exact opposite. I think citizens of high-tech societies are more vulnerable to robotic weapons, and the reason can be summed up in one word: data. Data powers high-tech societies. Cell phone geolocation, telecom metadata, social media, email, text, financial transaction data, transportation data, it's a wealth of real-time data on the movements and social interactions of people. In short, we are more visible to machines than any people in history, and this perfectly suits the targeting needs of autonomous weapons.
What you're looking at here is a link analysis map of a social group. Lines indicate social connectedness between individuals. And these types of maps can be automatically generated based on the data trail modern people leave behind. Now it's typically used to market goods and services to targeted demographics, but it's a dual-use technology, because targeting is used in another context. Notice that certain individuals are highlighted. These are the hubs of social networks. These are organizers, opinion-makers, leaders, and these people also can be automatically identified from their communication patterns. Now, if you're a marketer, you might then target them with product samples, try to spread your brand through their social group. But if you're a repressive government searching for political enemies, you might instead remove them, eliminate them, disrupt their social group, and those who remain behind lose social cohesion and organization. Now in a world of cheap, proliferating robotic weapons, borders would offer very little protection to critics of distant governments or trans-national criminal organizations. Popular movements agitating for change could be detected early and their leaders eliminated before their ideas achieve critical mass. And ideas achieving critical mass is what political activism in popular government is all about. Anonymous lethal weapons could make lethal action an easy choice for all sorts of competing interests. And this would put a chill on free speech and popular political action, the very heart of democracy.
And this is why we need an international treaty on robotic weapons, and in particular a global ban on the development and deployment of killer robots. Now we already have international treaties on nuclear and biological weapons, and, while imperfect, these have largely worked. But robotic weapons might be every bit as dangerous, because they will almost certainly be used, and they would also be corrosive to our democratic institutions.
Now in November 2012 the U.S. Department of Defense issued a directive requiring a human being be present in all lethal decisions. This temporarily effectively banned autonomous weapons in the U.S. military, but that directive needs to be made permanent. And it could set the stage for global action. Because we need an international legal framework for robotic weapons. And we need it now, before there's a devastating attack or a terrorist incident that causes nations of the world to rush to adopt these weapons before thinking through the consequences. Autonomous robotic weapons concentrate too much power in too few hands, and they would imperil democracy itself.
Now, don't get me wrong, I think there are tons of great uses for unarmed civilian drones: environmental monitoring, search and rescue, logistics. If we have an international treaty on robotic weapons, how do we gain the benefits of autonomous drones and vehicles while still protecting ourselves against illegal robotic weapons?
I think the secret will be transparency. No robot should have an expectation of privacy in a public place.
Each robot and drone should have a cryptographically signed I.D. burned in at the factory that can be used to track its movement through public spaces. We have license plates on cars, tail numbers on aircraft. This is no different. And every citizen should be able to download an app that shows the population of drones and autonomous vehicles moving through public spaces around them, both right now and historically. And civic leaders should deploy sensors and civic drones to detect rogue drones, and instead of sending killer drones of their own up to shoot them down, they should notify humans to their presence. And in certain very high-security areas, perhaps civic drones would snare them and drag them off to a bomb disposal facility.
But notice, this is more an immune system than a weapons system. It would allow us to avail ourselves of the use of autonomous vehicles and drones while still preserving our open, civil society.
We must ban the deployment and development of killer robots. Let's not succumb to the temptation to automate war. Autocratic governments and criminal organizations undoubtedly will, but let's not join them. Autonomous robotic weapons would concentrate too much power in too few unseen hands, and that would be corrosive to representative government. Let's make sure, for democracies at least, killer robots remain fiction.
My colleagues and I are fascinated by the science of moving dots. So what are these dots? Well, it's all of us. And we're moving in our homes, in our offices, as we shop and travel throughout our cities and around the world. And wouldn't it be great if we could understand all this movement? If we could find patterns and meaning and insight in it. And luckily for us, we live in a time where we're incredibly good at capturing information about ourselves. So whether it's through sensors or videos, or apps, we can track our movement with incredibly fine detail.
So it turns out one of the places where we have the best data about movement is sports. So whether it's basketball or baseball, or football or the other football, we're instrumenting our stadiums and our players to track their movements every fraction of a second. So what we're doing is turning our athletes into -- you probably guessed it -- moving dots.
So we've got mountains of moving dots and like most raw data, it's hard to deal with and not that interesting. But there are things that, for example, basketball coaches want to know. And the problem is they can't know them because they'd have to watch every second of every game, remember it and process it. And a person can't do that, but a machine can. The problem is a machine can't see the game with the eye of a coach. At least they couldn't until now. So what have we taught the machine to see?
So, we started simply. We taught it things like passes, shots and rebounds. Things that most casual fans would know. And then we moved on to things slightly more complicated. Events like post-ups, and pick-and-rolls, and isolations. And if you don't know them, that's okay. Most casual players probably do. Now, we've gotten to a point where today, the machine understands complex events like down screens and wide pins. Basically things only professionals know. So we have taught a machine to see with the eyes of a coach.
So how have we been able to do this? If I asked a coach to describe something like a pick-and-roll, they would give me a description, and if I encoded that as an algorithm, it would be terrible. The pick-and-roll happens to be this dance in basketball between four players, two on offense and two on defense. And here's kind of how it goes. So there's the guy on offense without the ball the ball and he goes next to the guy guarding the guy with the ball, and he kind of stays there and they both move and stuff happens, and ta-da, it's a pick-and-roll.
So that is also an example of a terrible algorithm. So, if the player who's the interferer -- he's called the screener -- goes close by, but he doesn't stop, it's probably not a pick-and-roll. Or if he does stop, but he doesn't stop close enough, it's probably not a pick-and-roll. Or, if he does go close by and he does stop but they do it under the basket, it's probably not a pick-and-roll. Or I could be wrong, they could all be pick-and-rolls. It really depends on the exact timing, the distances, the locations, and that's what makes it hard. So, luckily, with machine learning, we can go beyond our own ability to describe the things we know.
So how does this work? Well, it's by example. So we go to the machine and say, "Good morning, machine. Here are some pick-and-rolls, and here are some things that are not. Please find a way to tell the difference." And the key to all of this is to find features that enable it to separate. So if I was going to teach it the difference between an apple and orange, I might say, "Why don't you use color or shape?" And the problem that we're solving is, what are those things? What are the key features that let a computer navigate the world of moving dots? So figuring out all these relationships with relative and absolute location, distance, timing, velocities -- that's really the key to the science of moving dots, or as we like to call it, spatiotemporal pattern recognition, in academic vernacular. Because the first thing is, you have to make it sound hard -- because it is.
The key thing is, for NBA coaches, it's not that they want to know whether a pick-and-roll happened or not. It's that they want to know how it happened. And why is it so important to them? So here's a little insight. It turns out in modern basketball, this pick-and-roll is perhaps the most important play. And knowing how to run it, and knowing how to defend it, is basically a key to winning and losing most games. So it turns out that this dance has a great many variations and identifying the variations is really the thing that matters, and that's why we need this to be really, really good.
So, here's an example. There are two offensive and two defensive players, getting ready to do the pick-and-roll dance. So the guy with ball can either take, or he can reject. His teammate can either roll or pop. The guy guarding the ball can either go over or under. His teammate can either show or play up to touch, or play soft and together they can either switch or blitz and I didn't know most of these things when I started and it would be lovely if everybody moved according to those arrows. It would make our lives a lot easier, but it turns out movement is very messy. People wiggle a lot and getting these variations identified with very high accuracy, both in precision and recall, is tough because that's what it takes to get a professional coach to believe in you. And despite all the difficulties with the right spatiotemporal features we have been able to do that.
Coaches trust our ability of our machine to identify these variations. We're at the point where almost every single contender for an NBA championship this year is using our software, which is built on a machine that understands the moving dots of basketball. So not only that, we have given advice that has changed strategies that have helped teams win very important games, and it's very exciting because you have coaches who've been in the league for 30 years that are willing to take advice from a machine. And it's very exciting, it's much more than the pick-and-roll. Our computer started out with simple things and learned more and more complex things and now it knows so many things. Frankly, I don't understand much of what it does, and while it's not that special to be smarter than me, we were wondering, can a machine know more than a coach? Can it know more than person could know? And it turns out the answer is yes.
The coaches want players to take good shots. So if I'm standing near the basket and there's nobody near me, it's a good shot. If I'm standing far away surrounded by defenders, that's generally a bad shot. But we never knew how good "good" was, or how bad "bad" was quantitatively. Until now.
So what we can do, again, using spatiotemporal features, we looked at every shot. We can see: Where is the shot? What's the angle to the basket? Where are the defenders standing? What are their distances? What are their angles? For multiple defenders, we can look at how the player's moving and predict the shot type. We can look at all their velocities and we can build a model that predicts what is the likelihood that this shot would go in under these circumstances? So why is this important? We can take something that was shooting, which was one thing before, and turn it into two things: the quality of the shot and the quality of the shooter. So here's a bubble chart, because what's TED without a bubble chart?
Those are NBA players. The size is the size of the player and the color is the position. On the x-axis, we have the shot probability. People on the left take difficult shots, on the right, they take easy shots. On the [y-axis] is their shooting ability. People who are good are at the top, bad at the bottom. So for example, if there was a player who generally made 47 percent of their shots, that's all you knew before. But today, I can tell you that player takes shots that an average NBA player would make 49 percent of the time, and they are two percent worse. And the reason that's important is that there are lots of 47s out there. And so it's really important to know if the 47 that you're considering giving 100 million dollars to is a good shooter who takes bad shots or a bad shooter who takes good shots. Machine understanding doesn't just change how we look at players, it changes how we look at the game.
So there was this very exciting game a couple of years ago, in the NBA finals. Miami was down by three, there was 20 seconds left. They were about to lose the championship. A gentleman named LeBron James came up and he took a three to tie. He missed. His teammate Chris Bosh got a rebound, passed it to another teammate named Ray Allen. He sank a three. It went into overtime. They won the game. They won the championship. It was one of the most exciting games in basketball. And our ability to know the shot probability for every player at every second, and the likelihood of them getting a rebound at every second can illuminate this moment in a way that we never could before. Now unfortunately, I can't show you that video. But for you, we recreated that moment at our weekly basketball game about 3 weeks ago.
And we recreated the tracking that led to the insights. So, here is us. This is Chinatown in Los Angeles, a park we play at every week, and that's us recreating the Ray Allen moment and all the tracking that's associated with it. So, here's the shot. I'm going to show you that moment and all the insights of that moment. The only difference is, instead of the professional players, it's us, and instead of a professional announcer, it's me. So, bear with me.
Miami. Down three. Twenty seconds left. Jeff brings up the ball. Josh catches, puts up a three!
 [Calculating shot probability]
 [Shot quality]
 [Rebound probability]
Won't go!
 [Rebound probability]
Rebound, Noel. Back to Daria.
 [Shot quality]
Her three-pointer -- bang! Tie game with five seconds left. The crowd goes wild.
That's roughly how it happened.
Roughly.
 (Applause) That moment had about a nine percent chance of happening in the NBA and we know that and a great many other things. I'm not going to tell you how many times it took us to make that happen.
Okay, I will! It was four.
Way to go, Daria.
But the important thing about that video and the insights we have for every second of every NBA game -- it's not that. It's the fact you don't have to be a professional team to track movement. You do not have to be a professional player to get insights about movement.
In fact, it doesn't even have to be about sports because we're moving everywhere. We're moving in our homes, in our offices, as we shop and we travel throughout our cities and around our world. What will we know? What will we learn? Perhaps, instead of identifying pick-and-rolls, a machine can identify the moment and let me know when my daughter takes her first steps. Which could literally be happening any second now.
Perhaps we can learn to better use our buildings, better plan our cities. I believe that with the development of the science of moving dots, we will move better, we will move smarter, we will move forward.
I am a neuroscientist with a mixed background in physics and medicine. My lab at the Swiss Federal Institute of Technology focuses on spinal cord injury, which affects more than 50,000 people around the world every year, with dramatic consequences for affected individuals, whose life literally shatters in a matter of a handful of seconds.
And for me, the Man of Steel, Christopher Reeve, has best raised the awareness on the distress of spinal cord injured people. And this is how I started my own personal journey in this field of research, working with the Christopher and Dana Reeve Foundation.
I still remember this decisive moment. It was just at the end of a regular day of work with the foundation. Chris addressed us, the scientists and experts, "You have to be more pragmatic. When leaving your laboratory tomorrow, I want you to stop by the rehabilitation center to watch injured people fighting to take a step, struggling to maintain their trunk. And when you go home, think of what you are going to change in your research on the following day to make their lives better."
These words, they stuck with me. This was more than 10 years ago, but ever since, my laboratory has followed the pragmatic approach to recovery after spinal cord injury. And my first step in this direction was to develop a new model of spinal cord injury that would more closely mimic some of the key features of human injury while offering well-controlled experimental conditions. And for this purpose, we placed two hemisections on opposite sides of the body. They completely interrupt the communication between the brain and the spinal cord, thus leading to complete and permanent paralysis of the leg. But, as observed, after most injuries in humans, there is this intervening gap of intact neural tissue through which recovery can occur. But how to make it happen?
Well, the classical approach consists of applying intervention that would promote the growth of the severed fiber to the original target. And while this certainly remained the key for a cure, this seemed extraordinarily complicated to me. To reach clinical fruition rapidly, it was obvious: I had to think about the problem differently.
It turned out that more than 100 years of research on spinal cord physiology, starting with the Nobel Prize Sherrington, had shown that the spinal cord, below most injuries, contained all the necessary and sufficient neural networks to coordinate locomotion, but because input from the brain is interrupted, they are in a nonfunctional state, like kind of dormant. My idea: We awaken this network.
And at the time, I was a post-doctoral fellow in Los Angeles, after completing my Ph.D. in France, where independent thinking is not necessarily promoted. (Laughter) I was afraid to talk to my new boss, but decided to muster up my courage. I knocked at the door of my wonderful advisor, Reggie Edgerton, to share my new idea.
He listened to me carefully, and responded with a grin. "Why don't you try?"
And I promise to you, this was such an important moment in my career, when I realized that the great leader believed in young people and new ideas.
And this was the idea: I'm going to use a simplistic metaphor to explain to you this complicated concept. Imagine that the locomotor system is a car. The engine is the spinal cord. The transmission is interrupted. The engine is turned off. How could we re-engage the engine? First, we have to provide the fuel; second, press the accelerator pedal; third, steer the car. It turned out that there are known neural pathways coming from the brain that play this very function during locomotion. My idea: Replace this missing input to provide the spinal cord with the kind of intervention that the brain would deliver naturally in order to walk.
For this, I leveraged 20 years of past research in neuroscience, first to replace the missing fuel with pharmacological agents that prepare the neurons in the spinal cord to fire, and second, to mimic the accelerator pedal with electrical stimulation. So here imagine an electrode implanted on the back of the spinal cord to deliver painless stimulation. It took many years, but eventually we developed an electrochemical neuroprosthesis that transformed the neural network in the spinal cord from dormant to a highly functional state. Immediately, the paralyzed rat can stand. As soon as the treadmill belt starts moving, the animal shows coordinated movement of the leg, but without the brain. Here what I call "the spinal brain" cognitively processes sensory information arising from the moving leg and makes decisions as to how to activate the muscle in order to stand, to walk, to run, and even here, while sprinting, instantly stand if the treadmill stops moving.
This was amazing. I was completely fascinated by this locomotion without the brain, but at the same time so frustrated. This locomotion was completely involuntary. The animal had virtually no control over the legs. Clearly, the steering system was missing. And it then became obvious from me that we had to move away from the classical rehabilitation paradigm, stepping on a treadmill, and develop conditions that would encourage the brain to begin voluntary control over the leg.
With this in mind, we developed a completely new robotic system to support the rat in any direction of space. Imagine, this is really cool. So imagine the little 200-gram rat attached at the extremity of this 200-kilo robot, but the rat does not feel the robot. The robot is transparent, just like you would hold a young child during the first insecure steps.
Let me summarize: The rat received a paralyzing lesion of the spinal cord. The electrochemical neuroprosthesis enabled a highly functional state of the spinal locomotor networks. The robot provided the safe environment to allow the rat to attempt anything to engage the paralyzed legs. And for motivation, we used what I think is the most powerful pharmacology of Switzerland: fine Swiss chocolate.
Actually, the first results were very, very, very disappointing. Here is my best physical therapist completely failing to encourage the rat to take a single step, whereas the same rat, five minutes earlier, walked beautifully on the treadmill. We were so frustrated.
But you know, one of the most essential qualities of a scientist is perseverance. We insisted. We refined our paradigm, and after several months of training, the otherwise paralyzed rat could stand, and whenever she decided, initiated full weight-bearing locomotion to sprint towards the rewards. This is the first recovery ever observed of voluntary leg movement after an experimental lesion of the spinal cord leading to complete and permanent paralysis.
In fact, not only could the rat initiate and sustain locomotion on the ground, they could even adjust leg movement, for example, to resist gravity in order to climb a staircase. I can promise you this was such an emotional moment in my laboratory. It took us 10 years of hard work to reach this goal.
But the remaining question was, how? I mean, how is it possible? And here, what we found was completely unexpected. This novel training paradigm encouraged the brain to create new connections, some relay circuits that relay information from the brain past the injury and restore cortical control over the locomotor networks below the injury. And here, you can see one such example, where we label the fibers coming from the brain in red. This blue neuron is connected with the locomotor center, and what this constellation of synaptic contacts means is that the brain is reconnected with the locomotor center with only one relay neuron. But the remodeling was not restricted to the lesion area. It occurred throughout the central nervous system, including in the brain stem, where we observed up to 300-percent increase in the density of fibers coming from the brain. We did not aim to repair the spinal cord, yet we were able to promote one of the more extensive remodeling of axonal projections ever observed in the central nervous system of adult mammal after an injury.
And there is a very important message hidden behind this discovery. They are the result of a young team of very talented people: physical therapists, neurobiologists, neurosurgeons, engineers of all kinds, who have achieved together what would have been impossible by single individuals. This is truly a trans-disciplinary team. They are working so close to each other that there is horizontal transfer of DNA. We are creating the next generation of M.D.'s and engineers capable of translating discoveries all the way from bench to bedside. And me? I am only the maestro who orchestrated this beautiful symphony.
Now, I am sure you are all wondering, aren't you, will this help injured people? Me too, every day. The truth is that we don't know enough yet. This is certainly not a cure for spinal cord injury, but I begin to believe that this may lead to an intervention to improve recovery and people's quality of life.
I would like you all to take a moment and dream with me. Imagine a person just suffered a spinal cord injury. After a few weeks of recovery, we will implant a programmable pump to deliver a personalized pharmacological cocktail directly to the spinal cord. At the same time, we will implant an electrode array, a sort of second skin covering the area of the spinal cord controlling leg movement, and this array is attached to an electrical pulse generator that delivers stimulations that are tailored to the person's needs. This defines a personalized electrochemical neuroprosthesis that will enable locomotion during training with a newly designed supporting system. And my hope is that after several months of training, there may be enough remodeling of residual connection to allow locomotion without the robot, maybe even without pharmacology or stimulation. My hope here is to be able to create the personalized condition to boost the plasticity of the brain and the spinal cord. And this is a radically new concept that may apply to other neurological disorders, what I termed "personalized neuroprosthetics," where by sensing and stimulating neural interfaces, I implanted throughout the nervous system, in the brain, in the spinal cord, even in peripheral nerves, based on patient-specific impairments. But not to replace the lost function, no -- to help the brain help itself.
And I hope this enticed your imagination, because I can promise to you this is not a matter of whether this revolution will occur, but when. And remember, we are only as great as our imagination, as big as our dream.
I'd like you to come back with me for a moment to the 19th century, specifically to June 24, 1833. The British Association for the Advancement of Science is holding its third meeting at the University of Cambridge. It's the first night of the meeting, and a confrontation is about to take place that will change science forever.
An elderly, white-haired man stands up. The members of the Association are shocked to realize that it's the poet Samuel Taylor Coleridge, who hadn't even left his house in years until that day. They're even more shocked by what he says.
"You must stop calling yourselves natural philosophers."
Coleridge felt that true philosophers like himself pondered the cosmos from their armchairs. They were not mucking around in the fossil pits or conducting messy experiments with electrical piles like the members of the British Association.
The crowd grew angry and began to complain loudly. A young Cambridge scholar named William Whewell stood up and quieted the audience. He politely agreed that an appropriate name for the members of the association did not exist.
"If 'philosophers' is taken to be too wide and lofty a term," he said, "then, by analogy with 'artist,' we may form 'scientist.'" This was the first time the word scientist was uttered in public, only 179 years ago.
I first found out about this confrontation when I was in graduate school, and it kind of blew me away. I mean, how could the word scientist not have existed until 1833? What were scientists called before? What had changed to make a new name necessary precisely at that moment? Prior to this meeting, those who studied the natural world were talented amateurs. Think of the country clergyman or squire collecting his beetles or fossils, like Charles Darwin, for example, or, the hired help of a nobleman, like Joseph Priestley, who was the literary companion to the Marquis of Lansdowne when he discovered oxygen. After this, they were scientists, professionals with a particular scientific method, goals, societies and funding.
Much of this revolution can be traced to four men who met at Cambridge University in 1812: Charles Babbage, John Herschel, Richard Jones and William Whewell. These were brilliant, driven men who accomplished amazing things. Charles Babbage, I think known to most TEDsters, invented the first mechanical calculator and the first prototype of a modern computer. John Herschel mapped the stars of the southern hemisphere, and, in his spare time, co-invented photography. I'm sure we could all be that productive without Facebook or Twitter to take up our time. Richard Jones became an important economist who later influenced Karl Marx. And Whewell not only coined the term scientist, as well as the words anode, cathode and ion, but spearheaded international big science with his global research on the tides. In the Cambridge winter of 1812 and 1813, the four met for what they called philosophical breakfasts. They talked about science and the need for a new scientific revolution. They felt science had stagnated since the days of the scientific revolution that had happened in the 17th century. It was time for a new revolution, which they pledged to bring about, and what's so amazing about these guys is, not only did they have these grandiose undergraduate dreams, but they actually carried them out, even beyond their wildest dreams. And I'm going to tell you today about four major changes to science these men made.
About 200 years before, Francis Bacon and then, later, Isaac Newton, had proposed an inductive scientific method. Now that's a method that starts from observations and experiments and moves to generalizations about nature called natural laws, which are always subject to revision or rejection should new evidence arise. However, in 1809, David Ricardo muddied the waters by arguing that the science of economics should use a different, deductive method. The problem was that an influential group at Oxford began arguing that because it worked so well in economics, this deductive method ought to be applied to the natural sciences too. The members of the philosophical breakfast club disagreed. They wrote books and articles promoting inductive method in all the sciences that were widely read by natural philosophers, university students and members of the public. Reading one of Herschel's books was such a watershed moment for Charles Darwin that he would later say, "Scarcely anything in my life made so deep an impression on me. It made me wish to add my might to the accumulated store of natural knowledge." It also shaped Darwin's scientific method, as well as that used by his peers. [Science for the public good]
Previously, it was believed that scientific knowledge ought to be used for the good of the king or queen, or for one's own personal gain. For example, ship captains needed to know information about the tides in order to safely dock at ports. Harbormasters would gather this knowledge and sell it to the ship captains. The philosophical breakfast club changed that, working together. Whewell's worldwide study of the tides resulted in public tide tables and tidal maps that freely provided the harbormasters' knowledge to all ship captains. Herschel helped by making tidal observations off the coast of South Africa, and, as he complained to Whewell, he was knocked off the docks during a violent high tide for his trouble. The four men really helped each other in every way. They also relentlessly lobbied the British government for the money to build Babbage's engines because they believed these engines would have a huge practical impact on society. In the days before pocket calculators, the numbers that most professionals needed -- bankers, insurance agents, ship captains, engineers — were to be found in lookup books like this, filled with tables of figures. These tables were calculated using a fixed procedure over and over by part-time workers known as -- and this is amazing -- computers, but these calculations were really difficult. I mean, this nautical almanac published the lunar differences for every month of the year. Each month required 1,365 calculations, so these tables were filled with mistakes. Babbage's difference engine was the first mechanical calculator devised to accurately compute any of these tables. Two models of his engine were built in the last 20 years by a team from the Science Museum of London using his own plans. This is the one now at the Computer History Museum in California, and it calculates accurately. It actually works. Later, Babbage's analytical engine was the first mechanical computer in the modern sense. It had a separate memory and central processor. It was capable of iteration, conditional branching and parallel processing, and it was programmable using punched cards, an idea Babbage took from Jacquard's loom. Tragically, Babbage's engines never were built in his day because most people thought that non-human computers would have no usefulness for the public. [New scientific institutions]
Founded in Bacon's time, the Royal Society of London was the foremost scientific society in England and even in the rest of the world. By the 19th century, it had become a kind of gentleman's club populated mainly by antiquarians, literary men and the nobility. The members of the philosophical breakfast club helped form a number of new scientific societies, including the British Association. These new societies required that members be active researchers publishing their results. They reinstated the tradition of the Q&A after scientific papers were read, which had been discontinued by the Royal Society as being ungentlemanly. And for the first time, they gave women a foot in the door of science. Members were encouraged to bring their wives, daughters and sisters to the meetings of the British Association, and while the women were expected to attend only the public lectures and the social events like this one, they began to infiltrate the scientific sessions as well. The British Association would later be the first of the major national science organizations in the world to admit women as full members. [External funding for science]
Up to the 19th century, natural philosophers were expected to pay for their own equipment and supplies. Occasionally, there were prizes, such as that given to John Harrison in the 18th century, for solving the so-called longitude problem, but prizes were only given after the fact, when they were given at all. On the advice of the philosophical breakfast club, the British Association began to use the extra money generated by its meetings to give grants for research in astronomy, the tides, fossil fish, shipbuilding, and many other areas. These grants not only allowed less wealthy men to conduct research, but they also encouraged thinking outside the box, rather than just trying to solve one pre-set question. Eventually, the Royal Society and the scientific societies of other countries followed suit, and this has become -- fortunately it's become -- a major part of the scientific landscape today.
So the philosophical breakfast club helped invent the modern scientist. That's the heroic part of their story. There's a flip side as well. They did not foresee at least one consequence of their revolution. They would have been deeply dismayed by today's disjunction between science and the rest of culture. It's shocking to realize that only 28 percent of American adults have even a very basic level of science literacy, and this was tested by asking simple questions like, "Did humans and dinosaurs inhabit the Earth at the same time?" and "What proportion of the Earth is covered in water?" Once scientists became members of a professional group, they were slowly walled off from the rest of us. This is the unintended consequence of the revolution that started with our four friends.
Charles Darwin said, "I sometimes think that general and popular treatises are almost as important for the progress of science as original work." In fact, "Origin of Species" was written for a general and popular audience, and was widely read when it first appeared. Darwin knew what we seem to have forgotten, that science is not only for scientists.
In 1975, I met in Florence a professor, Carlo Pedretti, my former professor of art history, and today a world-renowned scholar of Leonardo da Vinci. Well, he asked me if I could find some technological way to unfold a five-centuries-old mystery related to a lost masterpiece by Leonardo da Vinci, the "Battle of Anghiari," which is supposed to be located in the Hall of the 500 in Palazzo Vecchio, in Florence. Well, in the mid-'70s, there were not great opportunities for a bioengineer like me, especially in Italy, and so I decided, with some researchers from the United States and the University of Florence, to start probing the murals decorated by Vasari on the long walls of the Hall of the 500 searching for the lost Leonardo.
Unfortunately, at that time we did not know that that was not exactly where we should be looking, because we had to go much deeper in, and so the research came to a halt, and it was only taken up in 2000 thanks to the interest and the enthusiasm of the Guinness family. Well, this time, we focused on trying to reconstruct the way the Hall of the 500 was before the remodeling, and the so-called Sala Grande, which was built in 1494, and to find out the original doors, windows, and in order to do that, we first created a 3D model, and then, with thermography, we went on to discover hidden windows. These are the original windows of the hall of the Sala Grande. We also found out about the height of the ceiling, and we managed to reconstruct, therefore, all the layout of this original hall the way it was before there came Vasari, and restructured the whole thing, including a staircase that was very important in order to precisely place "The Battle of Anghiari" on a specific area of one of the two walls.
Well, we also learned that Vasari, who was commissioned to remodel the Hall of the 500 between 1560 and 1574 by the Grand Duke Cosimo I of the Medici family, we have at least two instances when he saved masterpieces specifically by placing a brick wall in front of it and leaving a small air gap. One that we [see] here, Masaccio, the church of Santa Maria Novella in Florence, so we just said, well maybe, Visari has done something like that in the case of this great work of art by Leonardo, since he was a great admirer of Leonardo da Vinci.
And so we built some very sophisticated radio antennas just for probing both walls and searching for an air gap. And we did find many on the right panel of the east wall, an air gap, and that's where we believe "The Battle of Anghiari," or at least the part that we know has been painted, which is called "The Fight for the Standard," should be located.
Well, from there, unfortunately, in 2004, the project came to a halt. Many political reasons. So I decided to go back to my alma mater, and, at the University of California, San Diego, and I proposed to open up a research center for engineering sciences for cultural heritage. And in 2007, we created CISA3 as a research center for cultural heritage, specifically art, architecture and archaeology. So students started to flow in, and we started to build technologies, because that's basically what we also needed in order to move forward and go and do fieldwork.
We came back in the Hall of the 500 in 2011, and this time, with a great group of students, and my colleague, Professor Falko Kuester, who is now the director at CISA3, and we came back just since we knew already where to look for to find out if there was still something left. Well, we were confined though, limited, I should rather say, for several reasons that it's not worth explaining, to endoscopy only, of the many other options we had, and with a 4mm camera attached to it, we were successful in documenting and taking some fragments of what it turns out to be a reddish color, black color, and there is some beige fragments that later on we ran a much more sophisticated exams, XRF, X-ray diffraction, and the results are very positive so far. It seems to indicate that indeed we have found some pigments, and since we know for sure that no other artist has painted on that wall before Vasari came in about 60 years later, well, those pigments are therefore firmly related to mural painting and most likely to Leonardo.
Well, we are searching for the highest and highly praised work of art ever achieved by mankind. As a matter of fact, this is by far the most important commission that Leonardo has ever had, and for doing this great masterpiece, he was named the number one artist influence at the time.
I had also had the privilege since the last 37 years to work on several masterpieces as you can see behind me, but basically to do what? Well, to assess, for example, the state of conservation. See here the face of the Madonna of the Chair that when just shining a UV light on it you suddenly see another, different lady, aged lady, I should rather say. There is a lot of varnish still sitting there, several retouches, and some over cleaning. It becomes very visible.
But also, technology has helped to write new pages of our history, or at least to update pages of our histories. For example, the "Lady with the Unicorn," another painting by Rafael, well, you see the unicorn. A lot has been said and written about the unicorn, but if you take an X-ray of the unicorn, it becomes a puppy dog. And — (Laughter) — no problem, but, unfortunately, continuing with the scientific examination of this painting came out that Rafael did not paint the unicorn, did not paint the puppy dog, actually left the painting unfinished, so all this writing about the exotic symbol of the unicorn — (Laughter) — unfortunately, is not very reliable. (Laughter)
Well, also, authenticity. Just think for a moment if science really could move in the field of authenticity of works of art. There would be a cultural revolution to say the least, but also, I would say, a market revolution, let me add. Take this example: Otto Marseus, nice painting, which is "Still Life" at the Pitti Gallery, and just have an infrared camera peering through, and luckily for art historians, it just was confirmed that there is a signature of Otto Marseus. It even says when it was made and also the location. So that was a good result. Sometimes, it's not that good, and so, again, authenticity and science could go together and change the way, not attributions being made, but at least lay the ground for a more objective, or, I should rather say, less subjective attribution, as it is done today.
But I would say the discovery that really caught my imagination, my admiration, is the incredibly vivid drawing under this layer, brown layer, of "The Adoration of the Magi." Here you see a handmade setting XYZ scanner with an infrared camera put on it, and just peering through this brown layer of this masterpiece to reveal what could have been underneath. Well, this happens to be the most important painting we have in Italy by Leonardo da Vinci, and look at the wonderful images of faces that nobody has seen for five centuries. Look at these portraits. They're magnificent. You see Leonardo at work. You see the geniality of his creation, right directly on the ground layer of the panel, and see this cool thing, finding, I should rather say, an elephant. (Laughter) Because of this elephant, over 70 new images came out, never seen for centuries. This was an epiphany. We came to understand and to prove that the brown coating that we see today was not done by Leonardo da Vinci, which left us only the other drawing that for five centuries we were not able to see, so thanks only to technology.
Well, the tablet. Well, we thought, well, if we all have this pleasure, this privilege to see all this, to find all these discoveries, what about for everybody else? So we thought of an augmented reality application using a tablet. Let me show you just simulating what we could be doing, any of us could be doing, in a museum environment. So let's say that we go to a museum with a tablet, okay? And we just aim the camera of the tablet to the painting that we are interested to see, like this. Okay? And I will just click on it, we pause, and now let me turn to you so the moment the image, or, I should say, the camera, has locked in the painting, then the images you just saw up there in the drawing are being loaded. And so, see. We can, as we said, we can zoom in. Then we can scroll. Okay? Let's go and find the elephant. So all we need is one finger. Just wipe off and we see the elephant. (Applause) (Applause) Okay? And then if we want, we can continue the scroll to find out, for example, on the staircase, the whole iconography is going to be changed. There are a lot of laymen reconstructing from the ruins of an old temple a new temple, and there are a lot of figures showing up. See?
This is not just a curiosity, because it changes not just the iconography as you see it, but the iconology, the meaning of the painting, and we believe this is a cool way, easy way, that everybody could have access to, to become more the protagonist of your own discovery, and not just be so passive about it, as we are when we walk through endless rooms of museums. (Applause)
Another concept is the digital clinical chart, which sounds very obvious if we were to talk about real patients, but when we talk about works of art, unfortunately, it's never been tapped as an idea. Well, we believe, again, that this should be the beginning, the very first step, to do real conservation, and allowing us to really explore and to understand everything related to the state of our conservation, the technique, materials, and also if, when, and why we should restore, or, rather, to intervene on the environment surrounding the painting.
Well, our vision is to rediscover the spirit of the Renaissance, create a new discipline where engineering for cultural heritage is actually a symbol of blending art and science together. We definitely need a new breed of engineers that will go out and do this kind of work and rediscover for us these values, these cultural values that we badly need, especially today.
And if you want to summarize in one just single word, well, this is what we're trying to do. We're trying to give a future to our past in order to have a future. As long as we live a life of curiosity and passion, there is a bit of Leonardo in all of us. Thank you.
So, this is my grandfather, Salman Schocken, who was born into a poor and uneducated family with six children to feed, and when he was 14 years old, he was forced to drop out of school in order to help put bread on the table. He never went back to school. Instead, he went on to build a glittering empire of department stores. Salman was the consummate perfectionist, and every one of his stores was a jewel of Bauhaus architecture. He was also the ultimate self-learner, and like everything else, he did it in grand style. He surrounded himself with an entourage of young, unknown scholars like Martin Buber and Shai Agnon and Franz Kafka, and he paid each one of them a monthly salary so that they could write in peace.
And yet, in the late '30s, Salman saw what's coming. He fled Germany, together with his family, leaving everything else behind. His department stores confiscated, he spent the rest of his life in a relentless pursuit of art and culture. This high school dropout died at the age of 82, a formidable intellectual, cofounder and first CEO of the Hebrew University of Jerusalem, and founder of Schocken Books, an acclaimed imprint that was later acquired by Random House. Such is the power of self-study.
And these are my parents. They too did not enjoy the privilege of college education. They were too busy building a family and a country. And yet, just like Salman, they were lifelong, tenacious self-learners, and our home was stacked with thousands of books, records and artwork. I remember quite vividly my father telling me that when everyone in the neighborhood will have a TV set, then we'll buy a normal F.M. radio. (Laughter)
And that's me, I was going to say holding my first abacus, but actually holding what my father would consider an ample substitute to an iPad. (Laughter) So one thing that I took from home is this notion that educators don't necessarily have to teach. Instead, they can provide an environment and resources that tease out your natural ability to learn on your own. Self-study, self-exploration, self-empowerment: these are the virtues of a great education.
So I'd like to share with you a story about a self-study, self-empowering computer science course that I built, together with my brilliant colleague Noam Nisan. As you can see from the pictures, both Noam and I had an early fascination with first principles, and over the years, as our knowledge of science and technology became more sophisticated, this early awe with the basics has only intensified. So it's not surprising that, about 12 years ago, when Noam and I were already computer science professors, we were equally frustrated by the same phenomenon. As computers became increasingly more complex, our students were losing the forest for the trees, and indeed, it is impossible to connect with the soul of the machine if you interact with a black box P.C. or a Mac which is shrouded by numerous layers of closed, proprietary software. So Noam and I had this insight that if we want our students to understand how computers work, and understand it in the marrow of their bones, then perhaps the best way to go about it is to have them build a complete, working, general-purpose, useful computer, hardware and software, from the ground up, from first principles.
Now, we had to start somewhere, and so Noam and I decided to base our cathedral, so to speak, on the simplest possible building block, which is something called NAND. It is nothing more than a trivial logic gate with four input-output states. So we now start this journey by telling our students that God gave us NAND — (Laughter) — and told us to build a computer, and when we asked how, God said, "One step at a time." And then, following this advice, we start with this lowly, humble NAND gate, and we walk our students through an elaborate sequence of projects in which they gradually build a chip set, a hardware platform, an assembler, a virtual machine, a basic operating system and a compiler for a simple, Java-like language that we call "JACK." The students celebrate the end of this tour de force by using JACK to write all sorts of cool games like Pong, Snake and Tetris. You can imagine the tremendous joy of playing with a Tetris game that you wrote in JACK and then compiled into machine language in a compiler that you wrote also, and then seeing the result running on a machine that you built starting with nothing more than a few thousand NAND gates. It's a tremendous personal triumph of going from first principles all the way to a fantastically complex and useful system.
Noam and I worked five years to facilitate this ascent and to create the tools and infrastructure that will enable students to build it in one semester. And this is the great team that helped us make it happen. The trick was to decompose the computer's construction into numerous stand-alone modules, each of which could be individually specified, built and unit-tested in isolation from the rest of the project. And from day one, Noam and I decided to put all these building blocks freely available in open source on the Web. So chip specifications, APIs, project descriptions, software tools, hardware simulators, CPU emulators, stacks of hundreds of slides, lectures -- we laid out everything on the Web and invited the world to come over, take whatever they need, and do whatever they want with it.
And then something fascinating happened. The world came. And in short order, thousands of people were building our machine. And NAND2Tetris became one of the first massive, open, online courses, although seven years ago we had no idea that what we were doing is called MOOCs. We just observed how self-organized courses were kind of spontaneously spawning out of our materials. For example, Pramode C.E., an engineer from Kerala, India, has organized groups of self-learners who build our computer under his good guidance. And Parag Shah, another engineer, from Mumbai, has unbundled our projects into smaller, more manageable bites that he now serves in his pioneering do-it-yourself computer science program.
The people who are attracted to these courses typically have a hacker mentality. They want to figure out how things work, and they want to do it in groups, like this hackers club in Washington, D.C., that uses our materials to offer community courses. And because these materials are widely available and open-source, different people take them to very different and unpredictable directions. For example, Yu Fangmin, from Guangzhou, has used FPGA technology to build our computer and show others how to do the same using a video clip, and Ben Craddock developed a very nice computer game that unfolds inside our CPU architecture, which is quite a complex 3D maze that Ben developed using the Minecraft 3D simulator engine. The Minecraft community went bananas over this project, and Ben became an instant media celebrity.
And indeed, for quite a few people, taking this NAND2Tetris pilgrimage, if you will, has turned into a life-changing experience. For example, take Dan Rounds, who is a music and math major from East Lansing, Michigan. A few weeks ago, Dan posted a victorious post on our website, and I'd like to read it to you. So here's what Dan said.
"I did the coursework because understanding computers is important to me, just like literacy and numeracy, and I made it through. I never worked harder on anything, never been challenged to this degree. But given what I now feel capable of doing, I would certainly do it again. To anyone considering NAND2Tetris, it's a tough journey, but you'll be profoundly changed."
So Dan demonstrates the many self-learners who take this course off the Web, on their own traction, on their own initiative, and it's quite amazing because these people cannot care less about grades. They are doing it because of one motivation only. They have a tremendous passion to learn.
And with that in mind, I'd like to say a few words about traditional college grading. I'm sick of it. We are obsessed with grades because we are obsessed with data, and yet grading takes away all the fun from failing, and a huge part of education is about failing. Courage, according to Churchill, is the ability to go from one defeat to another without losing enthusiasm. (Laughter) And [Joyce] said that mistakes are the portals of discovery. And yet we don't tolerate mistakes, and we worship grades. So we collect your B pluses and your A minuses and we aggregate them into a number like 3.4, which is stamped on your forehead and sums up who you are. Well, in my opinion, we went too far with this nonsense, and grading became degrading.
So with that, I'd like to say a few words about upgrading, and share with you a glimpse from my current project, which is different from the previous one, but it shares exactly the same characteristics of self-learning, learning by doing, self-exploration and community-building, and this project deals with K-12 math education, beginning with early age math, and we do it on tablets because we believe that math, like anything else, should be taught hands on.
So here's what we do. Basically, we developed numerous mobile apps, every one of them explaining a particular concept in math. So for example, let's take area. When you deal with a concept like area -- well, we also provide a set of tools that the child is invited to experiment with in order to learn. So if area is what interests us, then one thing which is natural to do is to tile the area of this particular shape and simply count how many tiles it takes to cover it completely. And this little exercise here gives you a first good insight of the notion of area.
Moving along, what about the area of this figure? Well, if you try to tile it, it doesn't work too well, does it. So instead, you can experiment with these different tools here by some process of guided trial and error, and at some point you will discover that one thing that you can do among several legitimate transformations is the following one. You can cut the figure, you can rearrange the parts, you can glue them and then proceed to tile just like we did before. (Applause) Now this particular transformation did not change the area of the original figure, so a six-year-old who plays with this has just discovered a clever algorithm to compute the area of any given parallelogram.
We don't replace teachers, by the way. We believe that teachers should be empowered, not replaced.
Moving along, what about the area of a triangle? So after some guided trial and error, the child will discover, with or without help, that he or she can duplicate the original figure and then take the result, transpose it, glue it to the original and then proceed [with] what we did before: cut, rearrange, paste — oops— paste and glue, and tile. Now this transformation has doubled the area of the original figure, and therefore we have just learned that the area of the triangle equals the area of this rectangle divided by two. But we discovered it by self-exploration.
So, in addition to learning some useful geometry, the child has been exposed to some pretty sophisticated science strategies, like reduction, which is the art of transforming a complex problem into a simple one, or generalization, which is at the heart of any scientific discipline, or the fact that some properties are invariant under some transformations. And all this is something that a very young child can pick up using such mobile apps. So presently, we are doing the following: First of all, we are decomposing the K-12 math curriculum into numerous such apps. And because we cannot do it on our own, we've developed a very fancy authoring tool that any author, any parent or actually anyone who has an interest in math education, can use this authoring tool to develop similar apps on tablets without programming. And finally, we are putting together an adaptive ecosystem that will match different learners with different apps according to their evolving learning style.
The driving force behind this project is my colleague Shmulik London, and, you see, just like Salman did about 90 years ago, the trick is to surround yourself with brilliant people, because at the end, it's all about people. And a few years ago, I was walking in Tel Aviv and I saw this graffiti on a wall, and I found it so compelling that by now I preach it to my students, and I'd like to try to preach it to you. Now, I don't know how many people here are familiar with the term "mensch." It basically means to be human and to do the right thing. And with that, what this graffiti says is, "High-tech schmigh-tech. The most important thing is to be a mensch." Thank you.
There's an old joke about a cop who's walking his beat in the middle of the night, and he comes across a guy under a street lamp who's looking at the ground and moving from side to side, and the cop asks him what he's doing. The guys says he's looking for his keys. So the cop takes his time and looks over and kind of makes a little matrix and looks for about two, three minutes. No keys. The cop says, "Are you sure? Hey buddy, are you sure you lost your keys here?" And the guy says, "No, no, actually I lost them down at the other end of the street, but the light is better here." There's a concept that people talk about nowadays called big data, and what they're talking about is all of the information that we're generating through our interaction with and over the Internet, everything from Facebook and Twitter to music downloads, movies, streaming, all this kind of stuff, the live streaming of TED. And the folks who work with big data, for them, they talk about that their biggest problem is we have so much information, the biggest problem is, how do we organize all that information? I can tell you that working in global health, that is not our biggest problem. Because for us, even though the light is better on the Internet, the data that would help us solve the problems we're trying to solve is not actually present on the Internet. So we don't know, for example, how many people right now are being affected by disasters or by conflict situations. We don't know for really basically any of the clinics in the developing world, which ones have medicines and which ones don't. We have no idea of what the supply chain is for those clinics. We don't know -- and this is really amazing to me -- we don't know how many children were born, or how many children there are in Bolivia or Botswana or Bhutan. We don't know how many kids died last week in any of those countries. We don't know the needs of the elderly, the mentally ill. For all of these different critically important problems or critically important areas that we want to solve problems in, we basically know nothing at all. And part of the reason why we don't know anything at all is that the information technology systems that we use in global health to find the data to solve these problems is what you see here. And this is about a 5,000-year-old technology. Some of you may have used it before. It's kind of on its way out now, but we still use it for 99 percent of our stuff. This is a paper form, and what you're looking at is a paper form in the hand of a Ministry of Health nurse in Indonesia who is tramping out across the countryside in Indonesia on, I'm sure, a very hot and humid day, and she is going to be knocking on thousands of doors over a period of weeks or months, knocking on the doors and saying, "Excuse me, we'd like to ask you some questions. Do you have any children? Were your children vaccinated?" Because the only way we can actually find out how many children were vaccinated in the country of Indonesia, what percentage were vaccinated, is actually not on the Internet but by going out and knocking on doors, sometimes tens of thousands of doors. Sometimes it takes months to even years to do something like this. You know, a census of Indonesia would probably take two years to accomplish. And the problem, of course, with all of this is that with all those paper forms — and I'm telling you we have paper forms for every possible thing. We have paper forms for vaccination surveys. We have paper forms to track people who come into clinics. We have paper forms to track drug supplies, blood supplies, all these different paper forms for many different topics, they all have a single common endpoint, and the common endpoint looks something like this. And what we're looking at here is a truckful o' data. This is the data from a single vaccination coverage survey in a single district in the country of Zambia from a few years ago that I participated in. The only thing anyone was trying to find out is what percentage of Zambian children are vaccinated, and this is the data, collected on paper over weeks from a single district, which is something like a county in the United States. You can imagine that, for the entire country of Zambia, answering just that single question looks something like this. Truck after truck after truck filled with stack after stack after stack of data. And what makes it even worse is that that's just the beginning, because once you've collected all that data, of course someone's going to have to -- some unfortunate person is going to have to type that into a computer. When I was a graduate student, I actually was that unfortunate person sometimes. I can tell you, I often wasn't really paying attention. I probably made a lot of mistakes when I did it that no one ever discovered, so data quality goes down. But eventually that data hopefully gets typed into a computer, and someone can begin to analyze it, and once they have an analysis and a report, hopefully then you can take the results of that data collection and use it to vaccinate children better. Because if there's anything worse in the field of global public health, I don't know what's worse than allowing children on this planet to die of vaccine-preventable diseases, diseases for which the vaccine costs a dollar. And millions of children die of these diseases every year. And the fact is, millions is a gross estimate because we don't really know how many kids die each year of this. What makes it even more frustrating is that the data entry part, the part that I used to do as a grad student, can take sometimes six months. Sometimes it can take two years to type that information into a computer, and sometimes, actually not infrequently, it actually never happens. Now try and wrap your head around that for a second. You just had teams of hundreds of people. They went out into the field to answer a particular question. You probably spent hundreds of thousands of dollars on fuel and photocopying and per diem, and then for some reason, momentum is lost or there's no money left, and all of that comes to nothing because no one actually types it into the computer at all. The process just stops. Happens all the time. This is what we base our decisions on in global health: little data, old data, no data. So back in 1995, I began to think about ways in which we could improve this process. Now 1995, obviously that was quite a long time ago. It kind of frightens me to think of how long ago that was. The top movie of the year was "Die Hard with a Vengeance." As you can see, Bruce Willis had a lot more hair back then. I was working in the Centers for Disease Control, and I had a lot more hair back then as well. But to me, the most significant thing that I saw in 1995 was this. Hard for us to imagine, but in 1995, this was the ultimate elite mobile device. Right? It wasn't an iPhone. It wasn't a Galaxy phone. It was a Palm Pilot. And when I saw the Palm Pilot for the first time, I thought, why can't we put the forms on these Palm Pilots and go out into the field just carrying one Palm Pilot, which can hold the capacity of tens of thousands of paper forms? Why don't we try to do that? Because if we can do that, if we can actually just collect the data electronically, digitally, from the very beginning, we can just put a shortcut right through that whole process of typing, of having somebody type that stuff into the computer. We can skip straight to the analysis and then straight to the use of the data to actually save lives. So that's actually what I began to do. Working at CDC, I began to travel to different programs around the world and to train them in using Palm Pilots to do data collection instead of using paper. And it actually worked great. It worked exactly as well as anybody would have predicted. What do you know? Digital data collection is actually more efficient than collecting on paper. While I was doing it, my business partner, Rose, who's here with her husband, Matthew, here in the audience, Rose was out doing similar stuff for the American Red Cross. The problem was, after a few years of doing that, I realized I had done -- I had been to maybe six or seven programs, and I thought, you know, if I keep this up at this pace, over my whole career, maybe I'm going to go to maybe 20 or 30 programs. But the problem is, 20 or 30 programs, like, training 20 or 30 programs to use this technology, that is a tiny drop in the bucket. The demand for this, the need for data to run better programs, just within health, not to mention all of the other fields in developing countries, is enormous. There are millions and millions and millions of programs, millions of clinics that need to track drugs, millions of vaccine programs. There are schools that need to track attendance. There are all these different things for us to get the data that we need to do. And I realized, if I kept up the way that I was doing, I was basically hardly going to make any impact by the end of my career. And so I began to wrack my brain trying to think about, you know, what was the process that I was doing, how was I training folks, and what were the bottlenecks and what were the obstacles to doing it faster and to doing it more efficiently? And unfortunately, after thinking about this for some time, I realized -- I identified the main obstacle. And the main obstacle, it turned out, and this is a sad realization, the main obstacle was me. So what do I mean by that? I had developed a process whereby I was the center of the universe of this technology. If you wanted to use this technology, you had to get in touch with me. That means you had to know I existed. Then you had to find the money to pay for me to fly out to your country and the money to pay for my hotel and my per diem and my daily rate. So you could be talking about 10,000 or 20,000 or 30,000 dollars if I actually had the time or it fit my schedule and I wasn't on vacation. The point is that anything, any system that depends on a single human being or two or three or five human beings, it just doesn't scale. And this is a problem for which we need to scale this technology and we need to scale it now. And so I began to think of ways in which I could basically take myself out of the picture. And, you know, I was thinking, how could I take myself out of the picture for quite some time. You know, I'd been trained that the way that you distribute technology within international development is always consultant-based. It's always guys that look pretty much like me flying from countries that look pretty much like this to other countries with people with darker skin. And you go out there, and you spend money on airfare and you spend time and you spend per diem and you spend [on a] hotel and you spend all that stuff. As far as I knew, that was the only way you could distribute technology, and I couldn't figure out a way around it. But the miracle that happened, I'm going to call it Hotmail for short. Now you may not think of Hotmail as being miraculous, but for me it was miraculous, because I noticed, just as I was wrestling with this problem, I was working in sub-Saharan Africa mostly at the time. I noticed that every sub-Saharan African health worker that I was working with had a Hotmail account. And I thought, it struck me, wait a minute, I know that the Hotmail people surely didn't fly to the Ministry of Health of Kenya to train people in how to use Hotmail. So these guys are distributing technology. They're getting software capacity out there but they're not actually flying around the world. I need to think about this some more. While I was thinking about it, people started using even more things just like this, just as we were. They started using LinkedIn and Flickr and Gmail and Google Maps, all these things. Of course, all of these things are cloud-based and don't require any training. They don't require any programmers. They don't require any consultants, because the business model for all these businesses requires that something be so simple we can use it ourselves with little or no training. You just have to hear about it and go to the website. And so I thought, what would happen if we built software to do what I'd been consulting in? Instead of training people how to put forms onto mobile devices, let's create software that lets them do it themselves with no training and without me being involved? And that's exactly what we did. So we created software called Magpi, which has an online form creator. No one has to speak to me. You just have to hear about it and go to the website. You can create forms, and once you've created the forms, you push them to a variety of common mobile phones. Obviously nowadays, we've moved past Palm Pilots to mobile phones. And it doesn't have to be a smartphone. It can be a basic phone like the phone on the right there, you know, the basic kind of Symbian phone that's very common in developing countries. And the great part about this is, it's just like Hotmail. It's cloud-based, and it doesn't require any training, programming, consultants. But there are some additional benefits as well. Now we knew, when we built this system, the whole point of it, just like with the Palm Pilots, was that you'd have to, you'd be able to collect the data and immediately upload the data and get your data set. But what we found, of course, since it's already on a computer, we can deliver instant maps and analysis and graphing. We can take a process that took two years and compress that down to the space of five minutes. Unbelievable improvements in efficiency. Cloud-based, no training, no consultants, no me. And I told you that in the first few years of trying to do this the old-fashioned way, going out to each country, we reached about, I don't know, probably trained about 1,000 people. What happened after we did this? In the second three years, we had 14,000 people find the website, sign up, and start using it to collect data, data for disaster response, Canadian pig farmers tracking pig disease and pig herds, people tracking drug supplies. One of my favorite examples, the IRC, International Rescue Committee, they have a program where semi-literate midwives using $10 mobile phones send a text message using our software once a week with the number of births and the number of deaths, which gives IRC something that no one in global health has ever had: a near real-time system of counting babies, of knowing how many kids are born, of knowing how many children there are in Sierra Leone, which is the country where this is happening, and knowing how many children die. Physicians for Human Rights -- this is moving a little bit outside the health field — they are gathering, they're basically training people to do rape exams in Congo, where this is an epidemic, a horrible epidemic, and they're using our software to document the evidence they find, including photographically, so that they can bring the perpetrators to justice. Camfed, another charity based out of the U.K., Camfed pays girls' families to keep them in school. They understand this is the most significant intervention they can make. They used to track the dispersements, the attendance, the grades, on paper. The turnaround time between a teacher writing down grades or attendance and getting that into a report was about two to three years. Now it's real time, and because this is such a low-cost system and based in the cloud, it costs, for the entire five countries that Camfed runs this in with tens of thousands of girls, the whole cost combined is 10,000 dollars a year. That's less than I used to get just traveling out for two weeks to do a consultation. So I told you before that when we were doing it the old-fashioned way, I realized all of our work was really adding up to just a drop in the bucket -- 10, 20, 30 different programs. We've made a lot of progress, but I recognize that right now, even the work that we've done with 14,000 people using this, is still a drop in the bucket. But something's changed. And I think it should be obvious. What's changed now is, instead of having a program in which we're scaling at such a slow rate that we can never reach all the people who need us, we've made it unnecessary for people to get reached by us. We've created a tool that lets programs keep kids in school, track the number of babies that are born and the number of babies that die, to catch criminals and successfully prosecute them, to do all these different things to learn more about what's going on, to understand more, to see more, and to save lives and improve lives.
Twenty-five years ago, scientists at CERN created the World Wide Web. Since then, the Internet has transformed the way we communicate, the way we do business, and even the way we live. In many ways, the ideas that gave birth to Google, Facebook, Twitter, and so many others, have now really transformed our lives, and this has brought us many real benefits such as a more connected society. However, there are also some downsides to this. Today, the average person has an astounding amount of personal information online, and we add to this online information every single time we post on Facebook, each time we search on Google, and each time we send an email.
Now, many of us probably think, well, one email, there's nothing in there, right? But if you consider a year's worth of emails, or maybe even a lifetime of email, collectively, this tells a lot. It tells where we have been, who we have met, and in many ways, even what we're thinking about. And the more scary part about this is our data now lasts forever, so your data can and will outlive you. What has happened is that we've largely lost control over our data and also our privacy.
So this year, as the web turns 25, it's very important for us to take a moment and think about the implications of this. We have to really think. We've lost privacy, yes, but actually what we've also lost is the idea of privacy itself. If you think about it, most of us here today probably remember what life was like before the Internet, but today, there's a new generation that is being taught from a very young age to share everything online, and this is a generation that is not going to remember when data was private. So we keep going down this road, 20 years from now, the word 'privacy' is going to have a completely different meaning from what it means to you and I.
So, it's time for us to take a moment and think, is there anything we can do about this? And I believe there is.
Let's take a look at one of the most widely used forms of communication in the world today: email. Before the invention of email, we largely communicated using letters, and the process was quite simple. You would first start by writing your message on a piece of paper, then you would place it into a sealed envelope, and from there, you would go ahead and send it after you put a stamp and address on it. Unfortunately, today, when we actually send an email, we're not sending a letter. What you are sending, in many ways, is actually a postcard, and it's a postcard in the sense that everybody that sees it from the time it leaves your computer to when it gets to the recipient can actually read the entire contents.
So, the solution to this has been known for some time, and there's many attempts to do it. The most basic solution is to use encryption, and the idea is quite simple. First, you encrypt the connection between your computer and the email server. Then, you also encrypt the data as it sits on the server itself. But there's a problem with this, and that is, the email servers also hold the encryption keys, so now you have a really big lock with a key placed right next to it. But not only that, any government could lawfully ask for and get the key to your data, and this is all without you being aware of it.
So the way we fix this problem is actually relatively easy, in principle: You give everybody their own keys, and then you make sure the server doesn't actually have the keys. This seems like common sense, right? So the question that comes up is, why hasn't this been done yet?
Well, if we really think about it, we see that the business model of the Internet today really isn't compatible with privacy. Just take a look at some of the biggest names on the web, and you see that advertising plays a huge role. In fact, this year alone, advertising is 137 billion dollars, and to optimize the ads that are shown to us, companies have to know everything about us. They need to know where we live, how old we are, what we like, what we don't like, and anything else they can get their hands on. And if you think about it, the best way to get this information is really just to invade our privacy. So these companies aren't going to give us our privacy. If we want to have privacy online, what we have to do is we've got to go out and get it ourselves.
For many years, when it came to email, the only solution was something known as PGP, which was quite complicated and only accessible to the tech-savvy. Here's a diagram that basically shows the process for encrypting and decrypting messages. So needless to say, this is not a solution for everybody, and this actually is part of the problem, because if you think about communication, by definition, it involves having someone to communicate with. So while PGP does a great job of what it's designed to do, for the people out there who can't understand how to use it, the option to communicate privately simply does not exist. And this is a problem that we need to solve. So if we want to have privacy online, the only way we can succeed is if we get the whole world on board, and this is only possible if we bring down the barrier to entry. I think this is actually the key challenge that lies in the tech community. What we really have to do is work and make privacy more accessible.
So last summer, when the Edward Snowden story came out, several colleagues and I decided to see if we could make this happen. At that time, we were working at the European Organization for Nuclear Research at the world's largest particle collider, which collides protons, by the way. We were all scientists, so we used our scientific creativity and came up with a very creative name for our project: ProtonMail. (Laughter) Many startups these days actually begin in people's garages or people's basements. We were a bit different. We started out at the CERN cafeteria, which actually is great, because look, you have all the food and water you could ever want. But even better than this is that every day between 12 p.m. and 2 p.m., free of charge, the CERN cafeteria comes with several thousand scientists and engineers, and these guys basically know the answers to everything. So it was in this environment that we began working. What we actually want to do is we want to take your email and turn it into something that looks more like this, but more importantly, we want to do it in a way that you can't even tell that it's happened. So to do this, we actually need a combination of technology and also design.
So how do we go about doing something like this? Well, it's probably a good idea not to put the keys on the server. So what we do is we generate encryption keys on your computer, and we don't generate a single key, but actually a pair of keys, so there's an RSA private key and an RSA public key, and these keys are mathematically connected.
So let's have a look and see how this works when multiple people communicate. So here we have Bob and Alice, who want to communicate privately. So the key challenge is to take Bob's message and to get it to Alice in such a way that the server cannot read that message. So what we have to do is we have to encrypt it before it even leaves Bob's computer, and one of the tricks is, we encrypt it using the public key from Alice. Now this encrypted data is sent through the server to Alice, and because the message was encrypted using Alice's public key, the only key that can now decrypt it is a private key that belongs to Alice, and it turns out Alice is the only person that actually has this key. So we've now accomplished the objective, which is to get the message from Bob to Alice without the server being able to read what's going on.
Actually, what I've shown here is a highly simplified picture. The reality is much more complex and it requires a lot of software that looks a bit like this. And that's actually the key design challenge: How do we take all this complexity, all this software, and implement it in a way that the user cannot see it. I think with ProtonMail, we have gotten pretty close to doing this.
So let's see how it works in practice. Here, we've got Bob and Alice again, who also want to communicate securely. They simply create accounts on ProtonMail, which is quite simple and takes a few moments, and all the key encryption and generation is happening automatically in the background as Bob is creating his account. Once his account is created, he just clicks "compose," and now he can write his email like he does today. So he fills in his information, and then after that, all he has to do is click "send," and just like that, without understanding cryptography, and without doing anything different from how he writes email today, Bob has just sent an encrypted message.
What we have here is really just the first step, but it shows that with improving technology, privacy doesn't have to be difficult, it doesn't have to be disruptive. If we change the goal from maximizing ad revenue to protecting data, we can actually make it accessible. Now, I know a question on everybody's minds is, okay, protecting privacy, this is a great goal, but can you actually do this without the tons of money that advertisements give you? And I think the answer is actually yes, because today, we've reached a point where people around the world really understand how important privacy is, and when you have that, anything is possible. Earlier this year, ProtonMail actually had so many users that we ran out of resources, and when this happened, our community of users got together and donated half a million dollars. So this is just an example of what can happen when you bring the community together towards a common goal. We can also leverage the world. Right now, we have a quarter of a million people that have signed up for ProtonMail, and these people come from everywhere, and this really shows that privacy is not just an American or a European issue, it's a global issue that impacts all of us. It's something that we really have to pay attention to going forward.
So what do we have to do to solve this problem? Well, first of all, we need to support a different business model for the Internet, one that does not rely entirely on advertisements for revenue and for growth. We actually need to build a new Internet where our privacy and our ability to control our data is first and foremost. But even more importantly, we have to build an Internet where privacy is no longer just an option but is also the default.
We have done the first step with ProtonMail, but this is really just the first step in a very, very long journey. The good news I can share with you guys today, the exciting news, is that we're not traveling alone. The movement to protect people's privacy and freedom online is really gaining momentum, and today, there are dozens of projects from all around the world who are working together to improve our privacy. These projects protect things from our chat to voice communications, also our file storage, our online search, our online browsing, and many other things. And these projects are not backed by billions of dollars in advertising, but they've found support really from the people, from private individuals like you and I from all over the world.
This really matters, because ultimately, privacy depends on each and every one of us, and we have to protect it now because our online data is more than just a collection of ones and zeros. It's actually a lot more than that. It's our lives, our personal stories, our friends, our families, and in many ways, also our hopes and our aspirations. We need to spend time now to really protect our right to share this only with people that we want to share this with, because without this, we simply can't have a free society. So now's the time for us to collectively stand up and say, yes, we do want to live in a world with online privacy, and yes, we can work together to turn this vision into a reality.
On March 10, 2011, I was in Cambridge at the MIT Media Lab meeting with faculty, students and staff, and we were trying to figure out whether I should be the next director.
That night, at midnight, a magnitude 9 earthquake hit off of the Pacific coast of Japan. My wife and family were in Japan, and as the news started to come in, I was panicking. I was looking at the news streams and listening to the press conferences of the government officials and the Tokyo Power Company, and hearing about this explosion at the nuclear reactors and this cloud of fallout that was headed towards our house which was only about 200 kilometers away. And the people on TV weren't telling us anything that we wanted to hear. I wanted to know what was going on with the reactor, what was going on with the radiation, whether my family was in danger.
So I did what instinctively felt like the right thing, which was to go onto the Internet and try to figure out if I could take matters into my own hands. On the Net, I found there were a lot of other people like me trying to figure out what was going on, and together we sort of loosely formed a group and we called it Safecast, and we decided we were going to try to measure the radiation and get the data out to everybody else, because it was clear that the government wasn't going to be doing this for us.
Three years later, we have 16 million data points, we have designed our own Geiger counters that you can download the designs and plug it into the network. We have an app that shows you most of the radiation in Japan and other parts of the world. We are arguably one of the most successful citizen science projects in the world, and we have created the largest open dataset of radiation measurements.
And the interesting thing here is how did — (Applause) — Thank you. How did a bunch of amateurs who really didn't know what we were doing somehow come together and do what NGOs and the government were completely incapable of doing? And I would suggest that this has something to do with the Internet. It's not a fluke. It wasn't luck, and it wasn't because it was us. It helped that it was an event that pulled everybody together, but it was a new way of doing things that was enabled by the Internet and a lot of the other things that were going on, and I want to talk a little bit about what those new principles are.
So remember before the Internet? (Laughter) I call this B.I. Okay? So, in B.I., life was simple. Things were Euclidian, Newtonian, somewhat predictable. People actually tried to predict the future, even the economists. And then the Internet happened, and the world became extremely complex, extremely low-cost, extremely fast, and those Newtonian laws that we so dearly cherished turned out to be just local ordinances, and what we found was that in this completely unpredictable world that most of the people who were surviving were working with sort of a different set of principles, and I want to talk a little bit about that.
Before the Internet, if you remember, when we tried to create services, what you would do is you'd create the hardware layer and the network layer and the software and it would cost millions of dollars to do anything that was substantial. So when it costs millions of dollars to do something substantial, what you would do is you'd get an MBA who would write a plan and get the money from V.C.s or big companies, and then you'd hire the designers and the engineers, and they'd build the thing. This is the Before Internet, B.I., innovation model. What happened after the Internet was the cost of innovation went down so much because the cost of collaboration, the cost of distribution, the cost of communication, and Moore's Law made it so that the cost of trying a new thing became nearly zero, and so you would have Google, Facebook, Yahoo, students that didn't have permission — permissionless innovation — didn't have permission, didn't have PowerPoints, they just built the thing, then they raised the money, and then they sort of figured out a business plan and maybe later on they hired some MBAs. So the Internet caused innovation, at least in software and services, to go from an MBA-driven innovation model to a designer-engineer-driven innovation model, and it pushed innovation to the edges, to the dorm rooms, to the startups, away from the large institutions, the stodgy old institutions that had the power and the money and the authority. And we all know this. We all know this happened on the Internet. It turns out it's happening in other things, too. Let me give you some examples.
So at the Media Lab, we don't just do hardware. We do all kinds of things. We do biology, we do hardware, and Nicholas Negroponte famously said, "Demo or die," as opposed to "Publish or perish," which was the traditional academic way of thinking. And he often said, the demo only has to work once, because the primary mode of us impacting the world was through large companies being inspired by us and creating products like the Kindle or Lego Mindstorms. But today, with the ability to deploy things into the real world at such low cost, I'm changing the motto now, and this is the official public statement. I'm officially saying, "Deploy or die." You have to get the stuff into the real world for it to really count, and sometimes it will be large companies, and Nicholas can talk about satellites. (Applause) Thank you. But we should be getting out there ourselves and not depending on large institutions to do it for us.
So last year, we sent a bunch of students to Shenzhen, and they sat on the factory floors with the innovators in Shenzhen, and it was amazing. What was happening there was you would have these manufacturing devices, and they weren't making prototypes or PowerPoints. They were fiddling with the manufacturing equipment and innovating right on the manufacturing equipment. The factory was in the designer, and the designer was literally in the factory. And so what you would do is, you'd go down to the stalls and you would see these cell phones. So instead of starting little websites like the kids in Palo Alto do, the kids in Shenzhen make new cell phones. They make new cell phones like kids in Palo Alto make websites, and so there's a rainforest of innovation going on in the cell phone. What they do is, they make a cell phone, go down to the stall, they sell some, they look at the other kids' stuff, go up, make a couple thousand more, go down. Doesn't this sound like a software thing? It sounds like agile software development, A/B testing and iteration, and what we thought you could only do with software kids in Shenzhen are doing this in hardware. My next fellow, I hope, is going to be one of these innovators from Shenzhen.
And so what you see is that is pushing innovation to the edges. We talk about 3D printers and stuff like that, and that's great, but this is Limor. She is one of our favorite graduates, and she is standing in front of a Samsung Techwin Pick and Place Machine. This thing can put 23,000 components per hour onto an electronics board. This is a factory in a box. So what used to take a factory full of workers working by hand in this little box in New York, she's able to have effectively — She doesn't actually have to go to Shenzhen to do this manufacturing. She can buy this box and she can manufacture it. So manufacturing, the cost of innovation, the cost of prototyping, distribution, manufacturing, hardware, is getting so low that innovation is being pushed to the edges and students and startups are being able to build it. This is a recent thing, but this will happen and this will change just like it did with software.
Sorona is a DuPont process that uses a genetically engineered microbe to turn corn sugar into polyester. It's 30 percent more efficient than the fossil fuel method, and it's much better for the environment. Genetic engineering and bioengineering are creating a whole bunch of great new opportunities for chemistry, for computation, for memory. We will probably be doing a lot, obviously doing health things, but we will probably be growing chairs and buildings soon. The problem is, Sorona costs about 400 million dollars and took seven years to build. It kind of reminds you of the old mainframe days. The thing is, the cost of innovation in bioengineering is also going down. This is desktop gene sequencer. It used to cost millions and millions of dollars to sequence genes. Now you can do it on a desktop like this, and kids can do this in dorm rooms. This is Gen9 gene assembler, and so right now when you try to print a gene, what you do is somebody in a factory with pipettes puts the thing together by hand, you have one error per 100 base pairs, and it takes a long time and costs a lot of money. This new device assembles genes on a chip, and instead of one error per 100 base pairs, it's one error per 10,000 base pairs. In this lab, we will have the world's capacity of gene printing within a year, 200 million base pairs a year. This is kind of like when we went from transistor radios wrapped by hand to the Pentium. This is going to become the Pentium of bioengineering, pushing bioengineering into the hands of dorm rooms and startup companies.
So it's happening in software and in hardware and bioengineering, and so this is a fundamental new way of thinking about innovation. It's a bottom-up innovation, it's democratic, it's chaotic, it's hard to control. It's not bad, but it's very different, and I think that the traditional rules that we have for institutions don't work anymore, and most of us here operate with a different set of principles. One of my favorite principles is the power of pull, which is the idea of pulling resources from the network as you need them rather than stocking them in the center and controlling everything.
So in the case of the Safecast story, I didn't know anything when the earthquake happened, but I was able to find Sean who was the hackerspace community organizer, and Peter, the analog hardware hacker who made our first Geiger counter, and Dan, who built the Three Mile Island monitoring system after the Three Mile Island meltdown. And these people I wouldn't have been able to find beforehand and probably were better that I found them just in time from the network.
I'm a three-time college dropout, so learning over education is very near and dear to my heart, but to me, education is what people do to you and learning is what you do to yourself.
And it feels like, and I'm biased, it feels like they're trying to make you memorize the whole encyclopedia before they let you go out and play, and to me, I've got Wikipedia on my cell phone, and it feels like they assume you're going to be on top of some mountain all by yourself with a number 2 pencil trying to figure out what to do when in fact you're always going to be connected, you're always going to have friends, and you can pull Wikipedia up whenever you need it, and what you need to learn is how to learn. In the case of Safecast, a bunch of amateurs when we started three years ago, I would argue that we probably as a group know more than any other organization about how to collect data and publish data and do citizen science.
Compass over maps. So this one, the idea is that the cost of writing a plan or mapping something is getting so expensive and it's not very accurate or useful. So in the Safecast story, we knew we needed to collect data, we knew we wanted to publish the data, and instead of trying to come up with the exact plan, we first said, oh, let's get Geiger counters. Oh, they've run out. Let's build them. There aren't enough sensors. Okay, then we can make a mobile Geiger counter. We can drive around. We can get volunteers. We don't have enough money. Let's Kickstarter it. We could not have planned this whole thing, but by having a very strong compass, we eventually got to where we were going, and to me it's very similar to agile software development, but this idea of compasses is very important.
So I think the good news is that even though the world is extremely complex, what you need to do is very simple. I think it's about stopping this notion that you need to plan everything, you need to stock everything, and you need to be so prepared, and focus on being connected, always learning, fully aware, and super present.
So I don't like the word "futurist." I think we should be now-ists, like we are right now.
Einstein said that "I never think about the future — it comes soon enough."
And he was right, of course. So today, I'm here to ask you to think of how the future is happening now. Over the past 200 years, the world has experienced two major waves of innovation. First, the Industrial Revolution brought us machines and factories, railways, electricity, air travel, and our lives have never been the same. Then the Internet revolution brought us computing power, data networks, unprecedented access to information and communication, and our lives have never been the same.
Now we are experiencing another metamorphic change: the industrial Internet. It brings together intelligent machines, advanced analytics, and the creativity of people at work. It's the marriage of minds and machines. And our lives will never be the same.
In my current role, I see up close how technology is beginning to transform industrial sectors that play a huge role in our economy and in our lives: energy, aviation, transportation, health care. For an economist, this is highly unusual, and it's extremely exciting, because this is a transformation as powerful as the Industrial Revolution and more, and before the Industrial Revolution, there was no economic growth to speak of.
So what is this industrial Internet? Industrial machines are being equipped with a growing number of electronic sensors that allow them to see, hear, feel a lot more than ever before, generating prodigious amounts of data. Increasingly sophisticated analytics then sift through the data, providing insights that allow us to operate the machines in entirely new ways, a lot more efficiently. And not just individual machines, but fleets of locomotives, airplanes, entire systems like power grids, hospitals. It is asset optimization and system optimization. Of course, electronic sensors have been around for some time, but something has changed: a sharp decline in the cost of sensors and, thanks to advances in cloud computing, a rapid decrease in the cost of storing and processing data.
So we are moving to a world where the machines we work with are not just intelligent; they are brilliant. They are self-aware, they are predictive, reactive and social. It's jet engines, locomotives, gas turbines, medical devices, communicating seamlessly with each other and with us. It's a world where information itself becomes intelligent and comes to us automatically when we need it without having to look for it. We are beginning to deploy throughout the industrial system embedded virtualization, multi-core processor technology, advanced cloud-based communications, a new software-defined machine infrastructure which allows machine functionality to become virtualized in software, decoupling machine software from hardware, and allowing us to remotely and automatically monitor, manage and upgrade industrial assets.
Why does any of this matter at all? Well first of all, it's already allowing us to shift towards preventive, condition-based maintenance, which means fixing machines just before they break, without wasting time servicing them on a fixed schedule. And this, in turn, is pushing us towards zero unplanned downtime, which means there will be no more power outages, no more flight delays.
So let me give you a few examples of how these brilliant machines work, and some of the examples may seem trivial, some are clearly more profound, but all of them are going to have a very powerful impact.
Let's start with aviation. Today, 10 percent of all flights cancellations and delays are due to unscheduled maintenance events. Something goes wrong unexpectedly. This results in eight billion dollars in costs for the airline industry globally every year, not to mention the impact on all of us: stress, inconvenience, missed meetings as we sit helplessly in an airport terminal. So how can the industrial Internet help here? We've developed a preventive maintenance system which can be installed on any aircraft. It's self-learning and able to predict issues that a human operator would miss. The aircraft, while in flight, will communicate with technicians on the ground. By the time it lands, they will already know if anything needs to be serviced. Just in the U.S., a system like this can prevent over 60,000 delays and cancellations every year, helping seven million passengers get to their destinations on time.
Or take healthcare. Today, nurses spend an average of 21 minutes per shift looking for medical equipment. That seems trivial, but it's less time spent caring for patients. St. Luke's Medical Center in Houston, Texas, which has deployed industrial Internet technology to electronically monitor and connect patients, staff and medical equipment, has reduced bed turnaround times by nearly one hour. If you need surgery, one hour matters. It means more patients can be treated, more lives can be saved. Another medical center, in Washington state, is piloting an application that allows medical images from city scanners and MRIs to be analyzed in the cloud, developing better analytics at a lower cost. Imagine a patient who has suffered a severe trauma, and needs the attention of several specialists: a neurologist, a cardiologist, an orthopedic surgeon. If all of them can have instantaneous and simultaneous access to scans and images as they are taken, they will be able to deliver better healthcare faster. So all of this translates into better health outcomes, but it can also deliver substantial economic benefits. Just a one-percent reduction in existing inefficiencies could yield savings of over 60 billion dollars to the healthcare industry worldwide, and that is just a drop in the sea compared to what we need to do to make healthcare affordable on a sustainable basis.
Similar advances are happening in energy, including renewable energy. Wind farms equipped with new remote monitorings and diagnostics that allow wind turbines to talk to each other and adjust the pitch of their blades in a coordinated way, depending on how the wind is blowing, can now produce electricity at a cost of less than five cents per kilowatt/hour. Ten years ago, that cost was 30 cents, six times as much.
The list goes on, and it will grow fast, because industrial data are now growing exponentially. By 2020, they will account for over 50 percent of all digital information.
But this is not just about data, so let me switch gears and tell you how this is impacting already the jobs we do every day, because this new wave of innovation is bringing about new tools and applications that will allow us to collaborate in a smarter and faster way, making our jobs not just more efficient but more rewarding. Imagine a field engineer arriving at the wind farm with a handheld device telling her which turbines need servicing. She already has all the spare parts, because the problems were diagnosed in advanced. And if she faces an unexpected issue, the same handheld device will allow her to communicate with colleagues at the service center, let them see what she sees, transmit data that they can run through diagnostics, and they can stream videos that will guide her, step by step, through whatever complex procedure is needed to get the machines back up and running. And their interaction gets documented and stored in a searchable database.
Let's stop and think about this for a minute, because this is a very important point. This new wave of innovation is fundamentally changing the way we work. And I know that many of you will be concerned about the impact that innovation might have on jobs. Unemployment is already high, and there is always a fear that innovation will destroy jobs. And innovation is disruptive. But let me stress two things here. First, we've already lived through mechanization of agriculture, automation of industry, and employment has gone up, because innovation is fundamentally about growth. It makes products more affordable. It creates new demand, new jobs. Second, there is a concern that in the future, there will only be room for engineers, data scientists, and other highly-specialized workers. And believe me, as an economist, I am also scared. But think about it: Just as a child can easily figure out how to operate an iPad, so a new generation of mobile and intuitive industrial applications will make life easier for workers of all skill levels. The worker of the future will be more like Iron Man than the Charlie Chaplin of "Modern Times." And to be sure, new high-skilled jobs will be created: mechanical digital engineers who understand both the machines and the data; managers who understand their industry and the analytics and can reorganize the business to take full advantage of the technology.
But now let's take a step back. Let's look at the big picture. There are people who argue that today's innovation is all about social media and silly games, with nowhere near the transformational power of the Industrial Revolution. They say that all the growth-enhancing innovations are behind us. And every time I hear this, I can't help thinking that even back in the Stone Age, there must have been a group of cavemen sitting around a fire one day looking very grumpy, and looking disapprovingly at another group of cavemen rolling a stone wheel up and down a hill, and saying to each other, "Yeah, this wheel thing, cool toy, sure, but compared to fire, it will have no impact. The big discoveries are all behind us." 
This technological revolution is as inspiring and transformational as anything we have ever seen. Human creativity and innovation have always propelled us forward. They've created jobs. They've raised living standards. They've made our lives healthier and more rewarding. And the new wave of innovation which is beginning to sweep through industry is no different. In the U.S. alone, the industrial Internet could raise average income by 25 to 40 percent over the next 15 years, boosting growth to rates we haven't seen in a long time, and adding between 10 and 15 trillion dollars to global GDP. That is the size of the entire U.S. economy today.
But this is not a foregone conclusion. We are just at the beginning of this transformation, and there will be barriers to break, obstacles to overcome. We will need to invest in the new technologies. We will need to adapt organizations and managerial practices. We will need a robust cybersecurity approach that protects sensitive information and intellectual property and safeguards critical infrastructure from cyberattacks. And the education system will need to evolve to ensure students are equipped with the right skills. It's not going to be easy, but it is going to be worth it. The economic challenges facing us are hard, but when I walk the factory floor, and I see how humans and brilliant machines are becoming interconnected, and I see the difference this makes in a hospital, in an airport, in a power generation plant, I'm not just optimistic, I'm enthusiastic. This new technological revolution is upon us.
So think about the future — it will be here soon enough.
I work with children with autism. Specifically, I make technologies to help them communicate.
Now, many of the problems that children with autism face, they have a common source, and that source is that they find it difficult to understand abstraction, symbolism. And because of this, they have a lot of difficulty with language.
Let me tell you a little bit about why this is. You see that this is a picture of a bowl of soup. All of us can see it. All of us understand this. These are two other pictures of soup, but you can see that these are more abstract These are not quite as concrete. And when you get to language, you see that it becomes a word whose look, the way it looks and the way it sounds, has absolutely nothing to do with what it started with, or what it represents, which is the bowl of soup. So it's essentially a completely abstract, a completely arbitrary representation of something which is in the real world, and this is something that children with autism have an incredible amount of difficulty with. Now that's why most of the people that work with children with autism -- speech therapists, educators -- what they do is, they try to help children with autism communicate not with words, but with pictures. So if a child with autism wanted to say, "I want soup," that child would pick three different pictures, "I," "want," and "soup," and they would put these together, and then the therapist or the parent would understand that this is what the kid wants to say. And this has been incredibly effective; for the last 30, 40 years people have been doing this. In fact, a few years back, I developed an app for the iPad which does exactly this. It's called Avaz, and the way it works is that kids select different pictures. These pictures are sequenced together to form sentences, and these sentences are spoken out. So Avaz is essentially converting pictures, it's a translator, it converts pictures into speech.
Now, this was very effective. There are thousands of children using this, you know, all over the world, and I started thinking about what it does and what it doesn't do. And I realized something interesting: Avaz helps children with autism learn words. What it doesn't help them do is to learn word patterns. Let me explain this in a little more detail. Take this sentence: "I want soup tonight." Now it's not just the words here that convey the meaning. It's also the way in which these words are arranged, the way these words are modified and arranged. And that's why a sentence like "I want soup tonight" is different from a sentence like "Soup want I tonight," which is completely meaningless. So there is another hidden abstraction here which children with autism find a lot of difficulty coping with, and that's the fact that you can modify words and you can arrange them to have different meanings, to convey different ideas. Now, this is what we call grammar. And grammar is incredibly powerful, because grammar is this one component of language which takes this finite vocabulary that all of us have and allows us to convey an infinite amount of information, an infinite amount of ideas. It's the way in which you can put things together in order to convey anything you want to.
And so after I developed Avaz, I worried for a very long time about how I could give grammar to children with autism. The solution came to me from a very interesting perspective. I happened to chance upon a child with autism conversing with her mom, and this is what happened. Completely out of the blue, very spontaneously, the child got up and said, "Eat." Now what was interesting was the way in which the mom was trying to tease out the meaning of what the child wanted to say by talking to her in questions. So she asked, "Eat what? Do you want to eat ice cream? You want to eat? Somebody else wants to eat? You want to eat cream now? You want to eat ice cream in the evening?" And then it struck me that what the mother had done was something incredible. She had been able to get that child to communicate an idea to her without grammar. And it struck me that maybe this is what I was looking for. Instead of arranging words in an order, in sequence, as a sentence, you arrange them in this map, where they're all linked together not by placing them one after the other but in questions, in question-answer pairs. And so if you do this, then what you're conveying is not a sentence in English, but what you're conveying is really a meaning, the meaning of a sentence in English. Now, meaning is really the underbelly, in some sense, of language. It's what comes after thought but before language. And the idea was that this particular representation might convey meaning in its raw form.
So I was very excited by this, you know, hopping around all over the place, trying to figure out if I can convert all possible sentences that I hear into this. And I found that this is not enough. Why is this not enough? This is not enough because if you wanted to convey something like negation, you want to say, "I don't want soup," then you can't do that by asking a question. You do that by changing the word "want." Again, if you wanted to say, "I wanted soup yesterday," you do that by converting the word "want" into "wanted." It's a past tense. So this is a flourish which I added to make the system complete. This is a map of words joined together as questions and answers, and with these filters applied on top of them in order to modify them to represent certain nuances. Let me show you this with a different example.
Let's take this sentence: "I told the carpenter I could not pay him." It's a fairly complicated sentence. The way that this particular system works, you can start with any part of this sentence. I'm going to start with the word "tell." So this is the word "tell." Now this happened in the past, so I'm going to make that "told." Now, what I'm going to do is, I'm going to ask questions. So, who told? I told. I told whom? I told the carpenter. Now we start with a different part of the sentence. We start with the word "pay," and we add the ability filter to it to make it "can pay." Then we make it "can't pay," and we can make it "couldn't pay" by making it the past tense. So who couldn't pay? I couldn't pay. Couldn't pay whom? I couldn't pay the carpenter. And then you join these two together by asking this question: What did I tell the carpenter? I told the carpenter I could not pay him.
Now think about this. This is —(Applause)— this is a representation of this sentence without language. And there are two or three interesting things about this. First of all, I could have started anywhere. I didn't have to start with the word "tell." I could have started anywhere in the sentence, and I could have made this entire thing. The second thing is, if I wasn't an English speaker, if I was speaking in some other language, this map would actually hold true in any language. So long as the questions are standardized, the map is actually independent of language. So I call this FreeSpeech, and I was playing with this for many, many months. I was trying out so many different combinations of this.
And then I noticed something very interesting about FreeSpeech. I was trying to convert language, convert sentences in English into sentences in FreeSpeech, and vice versa, and back and forth. And I realized that this particular configuration, this particular way of representing language, it allowed me to actually create very concise rules that go between FreeSpeech on one side and English on the other. So I could actually write this set of rules that translates from this particular representation into English. And so I developed this thing. I developed this thing called the FreeSpeech Engine which takes any FreeSpeech sentence as the input and gives out perfectly grammatical English text. And by putting these two pieces together, the representation and the engine, I was able to create an app, a technology for children with autism, that not only gives them words but also gives them grammar.
So I tried this out with kids with autism, and I found that there was an incredible amount of identification. They were able to create sentences in FreeSpeech which were much more complicated but much more effective than equivalent sentences in English, and I started thinking about why that might be the case. And I had an idea, and I want to talk to you about this idea next. In about 1997, about 15 years back, there were a group of scientists that were trying to understand how the brain processes language, and they found something very interesting. They found that when you learn a language as a child, as a two-year-old, you learn it with a certain part of your brain, and when you learn a language as an adult -- for example, if I wanted to learn Japanese right now — a completely different part of my brain is used. Now I don't know why that's the case, but my guess is that that's because when you learn a language as an adult, you almost invariably learn it through your native language, or through your first language. So what's interesting about FreeSpeech is that when you create a sentence or when you create language, a child with autism creates language with FreeSpeech, they're not using this support language, they're not using this bridge language. They're directly constructing the sentence.
And so this gave me this idea. Is it possible to use FreeSpeech not for children with autism but to teach language to people without disabilities? And so I tried a number of experiments. The first thing I did was I built a jigsaw puzzle in which these questions and answers are coded in the form of shapes, in the form of colors, and you have people putting these together and trying to understand how this works. And I built an app out of it, a game out of it, in which children can play with words and with a reinforcement, a sound reinforcement of visual structures, they're able to learn language. And this, this has a lot of potential, a lot of promise, and the government of India recently licensed this technology from us, and they're going to try it out with millions of different children trying to teach them English. And the dream, the hope, the vision, really, is that when they learn English this way, they learn it with the same proficiency as their mother tongue.
All right, let's talk about something else. Let's talk about speech. This is speech. So speech is the primary mode of communication delivered between all of us. Now what's interesting about speech is that speech is one-dimensional. Why is it one-dimensional? It's one-dimensional because it's sound. It's also one-dimensional because our mouths are built that way. Our mouths are built to create one-dimensional sound. But if you think about the brain, the thoughts that we have in our heads are not one-dimensional. I mean, we have these rich, complicated, multi-dimensional ideas. Now, it seems to me that language is really the brain's invention to convert this rich, multi-dimensional thought on one hand into speech on the other hand. Now what's interesting is that we do a lot of work in information nowadays, and almost all of that is done in the language domain. Take Google, for example. Google trawls all these countless billions of websites, all of which are in English, and when you want to use Google, you go into Google search, and you type in English, and it matches the English with the English. What if we could do this in FreeSpeech instead? I have a suspicion that if we did this, we'd find that algorithms like searching, like retrieval, all of these things, are much simpler and also more effective, because they don't process the data structure of speech. Instead they're processing the data structure of thought. The data structure of thought. That's a provocative idea.
But let's look at this in a little more detail. So this is the FreeSpeech ecosystem. We have the Free Speech representation on one side, and we have the FreeSpeech Engine, which generates English. Now if you think about it, FreeSpeech, I told you, is completely language-independent. It doesn't have any specific information in it which is about English. So everything that this system knows about English is actually encoded into the engine. That's a pretty interesting concept in itself. You've encoded an entire human language into a software program. But if you look at what's inside the engine, it's actually not very complicated. It's not very complicated code. And what's more interesting is the fact that the vast majority of the code in that engine is not really English-specific. And that gives this interesting idea. It might be very easy for us to actually create these engines in many, many different languages, in Hindi, in French, in German, in Swahili. And that gives another interesting idea. For example, supposing I was a writer, say, for a newspaper or for a magazine. I could create content in one language, FreeSpeech, and the person who's consuming that content, the person who's reading that particular information could choose any engine, and they could read it in their own mother tongue, in their native language. I mean, this is an incredibly attractive idea, especially for India. We have so many different languages. There's a song about India, and there's a description of the country as, it says, (in Sanskrit). That means "ever-smiling speaker of beautiful languages."
Language is beautiful. I think it's the most beautiful of human creations. I think it's the loveliest thing that our brains have invented. It entertains, it educates, it enlightens, but what I like the most about language is that it empowers.
I want to leave you with this. This is a photograph of my collaborators, my earliest collaborators when I started working on language and autism and various other things. The girl's name is Pavna, and that's her mother, Kalpana. And Pavna's an entrepreneur, but her story is much more remarkable than mine, because Pavna is about 23. She has quadriplegic cerebral palsy, so ever since she was born, she could neither move nor talk. And everything that she's accomplished so far, finishing school, going to college, starting a company, collaborating with me to develop Avaz, all of these things she's done with nothing more than moving her eyes.
Daniel Webster said this: He said, "If all of my possessions were taken from me with one exception, I would choose to keep the power of communication, for with it, I would regain all the rest." And that's why, of all of these incredible applications of FreeSpeech, the one that's closest to my heart still remains the ability for this to empower children with disabilities to be able to communicate, the power of communication, to get back all the rest.
Thank you. (Applause) Thank you. (Applause) Thank you. Thank you. Thank you. (Applause) Thank you. Thank you. Thank you. 
So in 1781, an English composer, technologist and astronomer called William Herschel noticed an object on the sky that didn't quite move the way the rest of the stars did. And Herschel's recognition that something was different, that something wasn't quite right, was the discovery of a planet, the planet Uranus, a name that has entertained countless generations of children, but a planet that overnight doubled the size of our known solar system. Just last month, NASA announced the discovery of 517 new planets in orbit around nearby stars, almost doubling overnight the number of planets we know about within our galaxy. So astronomy is constantly being transformed by this capacity to collect data, and with data almost doubling every year, within the next two decades, me may even reach the point for the first time in history where we've discovered the majority of the galaxies within the universe.
But as we enter this era of big data, what we're beginning to find is there's a difference between more data being just better and more data being different, capable of changing the questions we want to ask, and this difference is not about how much data we collect, it's whether those data open new windows into our universe, whether they change the way we view the sky. So what is the next window into our universe? What is the next chapter for astronomy? Well, I'm going to show you some of the tools and the technologies that we're going to develop over the next decade, and how these technologies, together with the smart use of data, may once again transform astronomy by opening up a window into our universe, the window of time.
Why time? Well, time is about origins, and it's about evolution. The origins of our solar system, how our solar system came into being, is it unusual or special in any way? About the evolution of our universe. Why our universe is continuing to expand, and what is this mysterious dark energy that drives that expansion?
But first, I want to show you how technology is going to change the way we view the sky. So imagine if you were sitting in the mountains of northern Chile looking out to the west towards the Pacific Ocean a few hours before sunrise. This is the view of the night sky that you would see, and it's a beautiful view, with the Milky Way just peeking out over the horizon. but it's also a static view, and in many ways, this is the way we think of our universe: eternal and unchanging. But the universe is anything but static. It constantly changes on timescales of seconds to billions of years. Galaxies merge, they collide at hundreds of thousands of miles per hour. Stars are born, they die, they explode in these extravagant displays. In fact, if we could go back to our tranquil skies above Chile, and we allow time to move forward to see how the sky might change over the next year, the pulsations that you see are supernovae, the final remnants of a dying star exploding, brightening and then fading from view, each one of these supernovae five billion times the brightness of our sun, so we can see them to great distances but only for a short amount of time. Ten supernova per second explode somewhere in our universe. If we could hear it, it would be popping like a bag of popcorn. Now, if we fade out the supernovae, it's not just brightness that changes. Our sky is in constant motion. This swarm of objects you see streaming across the sky are asteroids as they orbit our sun, and it's these changes and the motion and it's the dynamics of the system that allow us to build our models for our universe, to predict its future and to explain its past.
But the telescopes we've used over the last decade are not designed to capture the data at this scale. The Hubble Space Telescope: for the last 25 years it's been producing some of the most detailed views of our distant universe, but if you tried to use the Hubble to create an image of the sky, it would take 13 million individual images, about 120 years to do this just once.
So this is driving us to new technologies and new telescopes, telescopes that can go faint to look at the distant universe but also telescopes that can go wide to capture the sky as rapidly as possible, telescopes like the Large Synoptic Survey Telescope, or the LSST, possibly the most boring name ever for one of the most fascinating experiments in the history of astronomy, in fact proof, if you should need it, that you should never allow a scientist or an engineer to name anything, not even your children. (Laughter) We're building the LSST. We expect it to start taking data by the end of this decade. I'm going to show you how we think it's going to transform our views of the universe, because one image from the LSST is equivalent to 3,000 images from the Hubble Space Telescope, each image three and a half degrees on the sky, seven times the width of the full moon. Well, how do you capture an image at this scale? Well, you build the largest digital camera in history, using the same technology you find in the cameras in your cell phone or in the digital cameras you can buy in the High Street, but now at a scale that is five and a half feet across, about the size of a Volkswagen Beetle, where one image is three billion pixels. So if you wanted to look at an image in its full resolution, just a single LSST image, it would take about 1,500 high-definition TV screens. And this camera will image the sky, taking a new picture every 20 seconds, constantly scanning the sky so every three nights, we'll get a completely new view of the skies above Chile. Over the mission lifetime of this telescope, it will detect 40 billion stars and galaxies, and that will be for the first time we'll have detected more objects in our universe than people on the Earth. Now, we can talk about this in terms of terabytes and petabytes and billions of objects, but a way to get a sense of the amount of data that will come off this camera is that it's like playing every TED Talk ever recorded simultaneously, 24 hours a day, seven days a week, for 10 years. And to process this data means searching through all of those talks for every new idea and every new concept, looking at each part of the video to see how one frame may have changed from the next. And this is changing the way that we do science, changing the way that we do astronomy, to a place where software and algorithms have to mine through this data, where the software is as critical to the science as the telescopes and the cameras that we've built.
Now, thousands of discoveries will come from this project, but I'm just going to tell you about two of the ideas about origins and evolution that may be transformed by our access to data at this scale.
In the last five years, NASA has discovered over 1,000 planetary systems around nearby stars, but the systems we're finding aren't much like our own solar system, and one of the questions we face is is it just that we haven't been looking hard enough or is there something special or unusual about how our solar system formed? And if we want to answer that question, we have to know and understand the history of our solar system in detail, and it's the details that are crucial. So now, if we look back at the sky, at our asteroids that were streaming across the sky, these asteroids are like the debris of our solar system. The positions of the asteroids are like a fingerprint of an earlier time when the orbits of Neptune and Jupiter were much closer to the sun, and as these giant planets migrated through our solar system, they were scattering the asteroids in their wake. So studying the asteroids is like performing forensics, performing forensics on our solar system, but to do this, we need distance, and we get the distance from the motion, and we get the motion because of our access to time.
So what does this tell us? Well, if you look at the little yellow asteroids flitting across the screen, these are the asteroids that are moving fastest, because they're closest to us, closest to Earth. These are the asteroids we may one day send spacecraft to, to mine them for minerals, but they're also the asteroids that may one day impact the Earth, like happened 60 million years ago with the extinction of the dinosaurs, or just at the beginning of the last century, when an asteroid wiped out almost 1,000 square miles of Siberian forest, or even just last year, as one burnt up over Russia, releasing the energy of a small nuclear bomb. So studying the forensics of our solar system doesn't just tell us about the past, it can also predict the future, including our future.
Now when we get distance, we get to see the asteroids in their natural habitat, in orbit around the sun. So every point in this visualization that you can see is a real asteroid. Its orbit has been calculated from its motion across the sky. The colors reflect the composition of these asteroids, dry and stony in the center, water-rich and primitive towards the edge, water-rich asteroids which may have seeded the oceans and the seas that we find on our planet when they bombarded the Earth at an earlier time. Because the LSST will be able to go faint and not just wide, we will be able to see these asteroids far beyond the inner part of our solar system, to asteroids beyond the orbits of Neptune and Mars, to comets and asteroids that may exist almost a light year from our sun. And as we increase the detail of this picture, increasing the detail by factors of 10 to 100, we will be able to answer questions such as, is there evidence for planets outside the orbit of Neptune, to find Earth-impacting asteroids long before they're a danger, and to find out whether, maybe, our sun formed on its own or in a cluster of stars, and maybe it's this sun's stellar siblings that influenced the formation of our solar system, and maybe that's one of the reasons why solar systems like ours seem to be so rare.
Now, distance and changes in our universe — distance equates to time, as well as changes on the sky. Every foot of distance you look away, or every foot of distance an object is away, you're looking back about a billionth of a second in time, and this idea or this notion of looking back in time has revolutionized our ideas about the universe, not once but multiple times.
The first time was in 1929, when an astronomer called Edwin Hubble showed that the universe was expanding, leading to the ideas of the Big Bang. And the observations were simple: just 24 galaxies and a hand-drawn picture. But just the idea that the more distant a galaxy, the faster it was receding, was enough to give rise to modern cosmology.
A second revolution happened 70 years later, when two groups of astronomers showed that the universe wasn't just expanding, it was accelerating, a surprise like throwing up a ball into the sky and finding out the higher that it gets, the faster it moves away. And they showed this by measuring the brightness of supernovae, and how the brightness of the supernovae got fainter with distance. And these observations were more complex. They required new technologies and new telescopes, because the supernovae were in galaxies that were 2,000 times more distant than the ones used by Hubble. And it took three years to find just 42 supernovae, because a supernova only explodes once every hundred years within a galaxy. Three years to find 42 supernovae by searching through tens of thousands of galaxies. And once they'd collected their data, this is what they found. Now, this may not look impressive, but this is what a revolution in physics looks like: a line predicting the brightness of a supernova 11 billion light years away, and a handful of points that don't quite fit that line.
Small changes give rise to big consequences. Small changes allow us to make discoveries, like the planet found by Herschel. Small changes turn our understanding of the universe on its head. So 42 supernovae, slightly too faint, meaning slightly further away, requiring that a universe must not just be expanding, but this expansion must be accelerating, revealing a component of our universe which we now call dark energy, a component that drives this expansion and makes up 68 percent of the energy budget of our universe today.
So what is the next revolution likely to be? Well, what is dark energy and why does it exist? Each of these lines shows a different model for what dark energy might be, showing the properties of dark energy. They all are consistent with the 42 points, but the ideas behind these lines are dramatically different. Some people think about a dark energy that changes with time, or whether the properties of the dark energy are different depending on where you look on the sky. Others make differences and changes to the physics at the sub-atomic level. Or, they look at large scales and change how gravity and general relativity work, or they say our universe is just one of many, part of this mysterious multiverse, but all of these ideas, all of these theories, amazing and admittedly some of them a little crazy, but all of them consistent with our 42 points.
So how can we hope to make sense of this over the next decade? Well, imagine if I gave you a pair of dice, and I said you wanted to see whether those dice were loaded or fair. One roll of the dice would tell you very little, but the more times you rolled them, the more data you collected, the more confident you would become, not just whether they're loaded or fair, but by how much, and in what way. It took three years to find just 42 supernovae because the telescopes that we built could only survey a small part of the sky. With the LSST, we get a completely new view of the skies above Chile every three nights. In its first night of operation, it will find 10 times the number of supernovae used in the discovery of dark energy. This will increase by 1,000 within the first four months: 1.5 million supernovae by the end of its survey, each supernova a roll of the dice, each supernova testing which theories of dark energy are consistent, and which ones are not. And so, by combining these supernova data with other measures of cosmology, we'll progressively rule out the different ideas and theories of dark energy until hopefully at the end of this survey around 2030, we would expect to hopefully see a theory for our universe, a fundamental theory for the physics of our universe, to gradually emerge.
Now, in many ways, the questions that I posed are in reality the simplest of questions. We may not know the answers, but we at least know how to ask the questions. But if looking through tens of thousands of galaxies revealed 42 supernovae that turned our understanding of the universe on its head, when we're working with billions of galaxies, how many more times are we going to find 42 points that don't quite match what we expect? Like the planet found by Herschel or dark energy or quantum mechanics or general relativity, all ideas that came because the data didn't quite match what we expected. What's so exciting about the next decade of data in astronomy is, we don't even know how many answers are out there waiting, answers about our origins and our evolution. How many answers are out there that we don't even know the questions that we want to ask? 
I work with a bunch of mathematicians, philosophers and computer scientists, and we sit around and think about the future of machine intelligence, among other things. Some people think that some of these things are sort of science fiction-y, far out there, crazy. But I like to say, okay, let's look at the modern human condition. (Laughter) This is the normal way for things to be.
But if we think about it, we are actually recently arrived guests on this planet, the human species. Think about if Earth was created one year ago, the human species, then, would be 10 minutes old. The industrial era started two seconds ago. Another way to look at this is to think of world GDP over the last 10,000 years, I've actually taken the trouble to plot this for you in a graph. It looks like this. (Laughter) It's a curious shape for a normal condition. I sure wouldn't want to sit on it. (Laughter)
Let's ask ourselves, what is the cause of this current anomaly? Some people would say it's technology. Now it's true, technology has accumulated through human history, and right now, technology advances extremely rapidly -- that is the proximate cause, that's why we are currently so very productive. But I like to think back further to the ultimate cause.
Look at these two highly distinguished gentlemen: We have Kanzi -- he's mastered 200 lexical tokens, an incredible feat. And Ed Witten unleashed the second superstring revolution. If we look under the hood, this is what we find: basically the same thing. One is a little larger, it maybe also has a few tricks in the exact way it's wired. These invisible differences cannot be too complicated, however, because there have only been 250,000 generations since our last common ancestor. We know that complicated mechanisms take a long time to evolve. So a bunch of relatively minor changes take us from Kanzi to Witten, from broken-off tree branches to intercontinental ballistic missiles.
So this then seems pretty obvious that everything we've achieved, and everything we care about, depends crucially on some relatively minor changes that made the human mind. And the corollary, of course, is that any further changes that could significantly change the substrate of thinking could have potentially enormous consequences.
Some of my colleagues think we're on the verge of something that could cause a profound change in that substrate, and that is machine superintelligence. Artificial intelligence used to be about putting commands in a box. You would have human programmers that would painstakingly handcraft knowledge items. You build up these expert systems, and they were kind of useful for some purposes, but they were very brittle, you couldn't scale them. Basically, you got out only what you put in. But since then, a paradigm shift has taken place in the field of artificial intelligence.
Today, the action is really around machine learning. So rather than handcrafting knowledge representations and features, we create algorithms that learn, often from raw perceptual data. Basically the same thing that the human infant does. The result is A.I. that is not limited to one domain -- the same system can learn to translate between any pairs of languages, or learn to play any computer game on the Atari console. Now of course, A.I. is still nowhere near having the same powerful, cross-domain ability to learn and plan as a human being has. The cortex still has some algorithmic tricks that we don't yet know how to match in machines.
So the question is, how far are we from being able to match those tricks? A couple of years ago, we did a survey of some of the world's leading A.I. experts, to see what they think, and one of the questions we asked was, "By which year do you think there is a 50 percent probability that we will have achieved human-level machine intelligence?" We defined human-level here as the ability to perform almost any job at least as well as an adult human, so real human-level, not just within some limited domain. And the median answer was 2040 or 2050, depending on precisely which group of experts we asked. Now, it could happen much, much later, or sooner, the truth is nobody really knows.
What we do know is that the ultimate limit to information processing in a machine substrate lies far outside the limits in biological tissue. This comes down to physics. A biological neuron fires, maybe, at 200 hertz, 200 times a second. But even a present-day transistor operates at the Gigahertz. Neurons propagate slowly in axons, 100 meters per second, tops. But in computers, signals can travel at the speed of light. There are also size limitations, like a human brain has to fit inside a cranium, but a computer can be the size of a warehouse or larger. So the potential for superintelligence lies dormant in matter, much like the power of the atom lay dormant throughout human history, patiently waiting there until 1945. In this century, scientists may learn to awaken the power of artificial intelligence. And I think we might then see an intelligence explosion.
Now most people, when they think about what is smart and what is dumb, I think have in mind a picture roughly like this. So at one end we have the village idiot, and then far over at the other side we have Ed Witten, or Albert Einstein, or whoever your favorite guru is. But I think that from the point of view of artificial intelligence, the true picture is actually probably more like this: AI starts out at this point here, at zero intelligence, and then, after many, many years of really hard work, maybe eventually we get to mouse-level artificial intelligence, something that can navigate cluttered environments as well as a mouse can. And then, after many, many more years of really hard work, lots of investment, maybe eventually we get to chimpanzee-level artificial intelligence. And then, after even more years of really, really hard work, we get to village idiot artificial intelligence. And a few moments later, we are beyond Ed Witten. The train doesn't stop at Humanville Station. It's likely, rather, to swoosh right by.
Now this has profound implications, particularly when it comes to questions of power. For example, chimpanzees are strong -- pound for pound, a chimpanzee is about twice as strong as a fit human male. And yet, the fate of Kanzi and his pals depends a lot more on what we humans do than on what the chimpanzees do themselves. Once there is superintelligence, the fate of humanity may depend on what the superintelligence does. Think about it: Machine intelligence is the last invention that humanity will ever need to make. Machines will then be better at inventing than we are, and they'll be doing so on digital timescales. What this means is basically a telescoping of the future. Think of all the crazy technologies that you could have imagined maybe humans could have developed in the fullness of time: cures for aging, space colonization, self-replicating nanobots or uploading of minds into computers, all kinds of science fiction-y stuff that's nevertheless consistent with the laws of physics. All of this superintelligence could develop, and possibly quite rapidly.
Now, a superintelligence with such technological maturity would be extremely powerful, and at least in some scenarios, it would be able to get what it wants. We would then have a future that would be shaped by the preferences of this A.I. Now a good question is, what are those preferences? Here it gets trickier. To make any headway with this, we must first of all avoid anthropomorphizing. And this is ironic because every newspaper article about the future of A.I. has a picture of this: So I think what we need to do is to conceive of the issue more abstractly, not in terms of vivid Hollywood scenarios.
We need to think of intelligence as an optimization process, a process that steers the future into a particular set of configurations. A superintelligence is a really strong optimization process. It's extremely good at using available means to achieve a state in which its goal is realized. This means that there is no necessary conenction between being highly intelligent in this sense, and having an objective that we humans would find worthwhile or meaningful.
Suppose we give an A.I. the goal to make humans smile. When the A.I. is weak, it performs useful or amusing actions that cause its user to smile. When the A.I. becomes superintelligent, it realizes that there is a more effective way to achieve this goal: take control of the world and stick electrodes into the facial muscles of humans to cause constant, beaming grins. Another example, suppose we give A.I. the goal to solve a difficult mathematical problem. When the A.I. becomes superintelligent, it realizes that the most effective way to get the solution to this problem is by transforming the planet into a giant computer, so as to increase its thinking capacity. And notice that this gives the A.I.s an instrumental reason to do things to us that we might not approve of. Human beings in this model are threats, we could prevent the mathematical problem from being solved.
Of course, perceivably things won't go wrong in these particular ways; these are cartoon examples. But the general point here is important: if you create a really powerful optimization process to maximize for objective x, you better make sure that your definition of x incorporates everything you care about. This is a lesson that's also taught in many a myth. King Midas wishes that everything he touches be turned into gold. He touches his daughter, she turns into gold. He touches his food, it turns into gold. This could become practically relevant, not just as a metaphor for greed, but as an illustration of what happens if you create a powerful optimization process and give it misconceived or poorly specified goals.
Now you might say, if a computer starts sticking electrodes into people's faces, we'd just shut it off. A, this is not necessarily so easy to do if we've grown dependent on the system -- like, where is the off switch to the Internet? B, why haven't the chimpanzees flicked the off switch to humanity, or the Neanderthals? They certainly had reasons. We have an off switch, for example, right here. (Choking) The reason is that we are an intelligent adversary; we can anticipate threats and plan around them. But so could a superintelligent agent, and it would be much better at that than we are. The point is, we should not be confident that we have this under control here.
And we could try to make our job a little bit easier by, say, putting the A.I. in a box, like a secure software environment, a virtual reality simulation from which it cannot escape. But how confident can we be that the A.I. couldn't find a bug. Given that merely human hackers find bugs all the time, I'd say, probably not very confident. So we disconnect the ethernet cable to create an air gap, but again, like merely human hackers routinely transgress air gaps using social engineering. Right now, as I speak, I'm sure there is some employee out there somewhere who has been talked into handing out her account details by somebody claiming to be from the I.T. department.
More creative scenarios are also possible, like if you're the A.I., you can imagine wiggling electrodes around in your internal circuitry to create radio waves that you can use to communicate. Or maybe you could pretend to malfunction, and then when the programmers open you up to see what went wrong with you, they look at the source code -- Bam! -- the manipulation can take place. Or it could output the blueprint to a really nifty technology, and when we implement it, it has some surreptitious side effect that the A.I. had planned. The point here is that we should not be confident in our ability to keep a superintelligent genie locked up in its bottle forever. Sooner or later, it will out.
I believe that the answer here is to figure out how to create superintelligent A.I. such that even if -- when -- it escapes, it is still safe because it is fundamentally on our side because it shares our values. I see no way around this difficult problem.
Now, I'm actually fairly optimistic that this problem can be solved. We wouldn't have to write down a long list of everything we care about, or worse yet, spell it out in some computer language like C++ or Python, that would be a task beyond hopeless. Instead, we would create an A.I. that uses its intelligence to learn what we value, and its motivation system is constructed in such a way that it is motivated to pursue our values or to perform actions that it predicts we would approve of. We would thus leverage its intelligence as much as possible to solve the problem of value-loading.
This can happen, and the outcome could be very good for humanity. But it doesn't happen automatically. The initial conditions for the intelligence explosion might need to be set up in just the right way if we are to have a controlled detonation. The values that the A.I. has need to match ours, not just in the familiar context, like where we can easily check how the A.I. behaves, but also in all novel contexts that the A.I. might encounter in the indefinite future.
And there are also some esoteric issues that would need to be solved, sorted out: the exact details of its decision theory, how to deal with logical uncertainty and so forth. So the technical problems that need to be solved to make this work look quite difficult -- not as difficult as making a superintelligent A.I., but fairly difficult. Here is the worry: Making superintelligent A.I. is a really hard challenge. Making superintelligent A.I. that is safe involves some additional challenge on top of that. The risk is that if somebody figures out how to crack the first challenge without also having cracked the additional challenge of ensuring perfect safety.
So I think that we should work out a solution to the control problem in advance, so that we have it available by the time it is needed. Now it might be that we cannot solve the entire control problem in advance because maybe some elements can only be put in place once you know the details of the architecture where it will be implemented. But the more of the control problem that we solve in advance, the better the odds that the transition to the machine intelligence era will go well.
This to me looks like a thing that is well worth doing and I can imagine that if things turn out okay, that people a million years from now look back at this century and it might well be that they say that the one thing we did that really mattered was to get this thing right.
So this is Anna Hazare, and Anna Hazare may well be the most cutting-edge digital activist in the world today. And you wouldn't know it by looking at him. Hazare is a 77-year-old Indian anticorruption and social justice activist. And in 2011, he was running a big campaign to address everyday corruption in India, a topic that Indian elites love to ignore. So as part of this campaign, he was using all of the traditional tactics that a good Gandhian organizer would use. So he was on a hunger strike, and Hazare realized through his hunger that actually maybe this time, in the 21st century, a hunger strike wouldn't be enough.
So he started playing around with mobile activism. So the first thing he did is he said to people, "Okay, why don't you send me a text message if you support my campaign against corruption?" So he does this, he gives people a short code, and about 80,000 people do it. Okay, that's pretty respectable. But then he decides, "Let me tweak my tactics a little bit." He says, "Why don't you leave me a missed call?" Now, for those of you who have lived in the global South, you'll know that missed calls are a really critical part of global mobile culture. I see people nodding. People leave missed calls all the time: If you're running late for a meeting and you just want to let them know that you're on the way, you leave them a missed call. If you're dating someone and you just want to say "I miss you" you leave them a missed call. So a note for a dating tip here, in some cultures, if you want to please your lover, you call them and hang up. (Laughter) So why do people leave missed calls? Well, the reason of course is that they're trying to avoid charges associated with making calls and sending texts.
So when Hazare asked people to leave him a missed call, let's have a little guess how many people actually did this? Thirty-five million. So this is one of the largest coordinated actions in human history. It's remarkable. And this reflects the extraordinary strength of the emerging Indian middle class and the power that their mobile phones bring. But he used that, Hazare ended up with this massive CSV file of mobile phone numbers, and he used that to deploy real people power on the ground to get hundreds of thousands of people out on the streets in Delhi to make a national point of everyday corruption in India. It's a really striking story.
So this is me when I was 12 years old. I hope you see the resemblance. And I was also an activist, and I have been an activist all my life. I had this really funny childhood where I traipsed around the world meeting world leaders and Noble prize winners, talking about Third World debt, as it was then called, and demilitarization. I was a very, very serious child. (Laughter) And back then, in the early '90s, I had a very cutting-edge tech tool of my own: the fax. And the fax was the tool of my activism. And at that time, it was the best way to get a message to a lot of people all at once. I'll give you one example of a fax campaign that I ran. It was the eve of the Gulf War and I organized a global campaign to flood the hotel, the Intercontinental in Geneva, where James Baker and Tariq Aziz were meeting on the eve of the war, and I thought if I could flood them with faxes, we'll stop the war.
Well, unsurprisingly, that campaign was wholly unsuccessful. There are lots of reasons for that, but there's no doubt that one sputtering fax machine in Geneva was a little bit of a bandwidth constraint in terms of the ability to get a message to lots of people. And so, I went on to discover some better tools. I cofounded Avaaz, which uses the Internet to mobilize people and now has almost 40 million members, and I now run Purpose, which is a home for these kinds of technology-powered movements. So what's the moral of this story? Is the moral of this story, you know what, the fax is kind of eclipsed by the mobile phone? This is another story of tech-determinism? Well, I would argue that there's actually more to it than that. I'd argue that in the last 20 years, something more fundamental has changed than just new tech. I would argue that there has been a fundamental shift in the balance of power in the world.
You ask any activist how to understand the world, and they'll say, "Look at where the power is, who has it, how it's shifting." And I think we all sense that something big is happening.
So Henry Timms and I — Henry's a fellow movement builder — got talking one day and we started to think, how can we make sense of this new world? How can we describe it and give it a framework that makes it more useful? Because we realized that many of the lessons that we were discovering in movements actually applied all over the world in many sectors of our society. So I want to introduce you to this framework: Old power, meet new power. And I want to talk to you about what new power is today. New power is the deployment of mass participation and peer coordination — these are the two key elements — to create change and shift outcomes. And we see new power all around us.
This is Beppe Grillo he was a populist Italian blogger who, with a minimal political apparatus and only some online tools, won more than 25 percent of the vote in recent Italian elections. This is Airbnb, which in just a few years has radically disrupted the hotel industry without owning a single square foot of real estate. This is Kickstarter, which we know has raised over a billion dollars from more than five million people. Now, we're familiar with all of these models. But what's striking is the commonalities, the structural features of these new models and how they differ from old power.
Let's look a little bit at this. Old power is held like a currency. New power works like a current. Old power is held by a few. New power isn't held by a few, it's made by many. Old power is all about download, and new power uploads. And you see a whole set of characteristics that you can trace, whether it's in media or politics or education.
So we've talked a little bit about what new power is. Let's, for a second, talk about what new power isn't. New power is not your Facebook page. I assure you that having a social media strategy can enable you to do just as much download as you used to do when you had the radio. Just ask Syrian dictator Bashar al-Assad, I assure you that his Facebook page has not embraced the power of participation. New power is not inherently positive. In fact, this isn't an normative argument that we're making, there are many good things about new power, but it can produce bad outcomes. More participation, more peer coordination, sometimes distorts outcomes and there are some things, like things, for example, in the medical profession that we want new power to get nowhere near. And thirdly, new power is not the inevitable victor. In fact, unsurprisingly, as many of these new power models get to scale, what you see is this massive pushback from the forces of old power. Just look at this really interesting epic struggle going on right now between Edward Snowden and the NSA. You'll note that only one of the two people on this slide is currently in exile. And so, it's not at all clear that new power will be the inevitable victor.
That said, keep one thing in mind: We're at the beginning of a very steep curve. So you think about some of these new power models, right? These were just like someone's garage idea a few years ago, and now they're disrupting entire industries. And so, what's interesting about new power, is the way it feeds on itself. Once you have an experience of new power, you tend to expect and want more of it. So let's say you've used a peer-to-peer lending platform like Lending Tree or Prosper, then you've figured out that you don't need the bank, and who wants the bank, right? And so, that experience tends to embolden you it tends to make you want more participation across more aspects of your life. And what this gives rise to is a set of values. We talked about the models that new power has engendered — the Airbnbs, the Kickstarters. What about the values? And this is an early sketch at what new power values look like.
New power values prize transparency above all else. It's almost a religious belief in transparency, a belief that if you shine a light on something, it will be better. And remember that in the 20th century, this was not at all true. People thought that gentlemen should sit behind closed doors and make comfortable agreements. New power values of informal, networked governance. New power folks would never have invented the U.N. today, for better or worse. New power values participation, and new power is all about do-it-yourself. In fact, what's interesting about new power is that it eschews some of the professionalization and specialization that was all the rage in the 20th century.
So what's interesting about these new power values and these new power models is what they mean for organizations. So we've spent a bit of time thinking, how can we plot organizations on a two-by-two where, essentially, we look at new power values and new power models and see where different people sit? We started with a U.S. analysis, and let me show you some interesting findings. So the first is Apple. In this framework, we actually described Apple as an old power company. That's because the ideology, the governing ideology of Apple is the ideology of the perfectionist product designer in Cupertino. It's absolutely about that beautiful, perfect thing descending upon us in perfection. And it does not value, as a company, transparency. In fact, it's very secretive. Now, Apple is one of the most succesful companies in the world. So this shows that you can still pursue a successful old power strategy. But one can argue that there's real vulnerabilites in that model. I think another interesting comparison is that of the Obama campaign versus the Obama presidency. (Applause) Now, I like President Obama, but he ran with new power at his back, right? And he said to people, we are the ones we've been waiting for. And he used crowdfunding to power a campaign. But when he got into office, he governed like more or less all the other presidents did. And this is a really interesting trend, is when new power gets powerful, what happens? So this is a framework you should look at and think about where your own organization sits on it. And think about where it should be in five or 10 years. So what do you do if you're old power? Well, if you're there thinking, in old power, this won't happen to us. Then just look at the Wikipedia entry for Encyclopædia Britannica. Let me tell you, it's a very sad read.
But if you are old power, the most important thing you can do is to occupy yourself before others occupy you, before you are occupied. Imagine that a group of your biggest skeptics are camped in the heart of your organization asking the toughest questions and they can see everything inside of your organization. And ask them, would they like what they see and should our model change? What about if you're new power? Is new power kind of just riding the wave to glory? I would argue no. I would argue that there are some very real challenges to new power in this nascent phase. Let's stick with the Occupy Wall Street example for a moment. Occupy was this incredible example of new power, the purest example of new power. And yet, it failed to consolidate. So the energy that it created was great for the meme phase, but they were so committed to participation, that they never got anything done. And in fact that model means that the challenge for new power is: how do you use institutional power without being institutionalized?
One the other end of the spectra is Uber. Uber is an amazing, highly scalable new power model. That network is getting denser and denser by the day. But what's really interesting about Uber is it hasn't really adopted new power values. This is a real quote from the Uber CEO recently: He says, "Once we get rid of the dude in the car" — he means drivers — "Uber will be cheaper." Now, new power models live and die by the strength of their networks. By whether the drivers and the consumers who use the service actually believe in it. Because they're not an exercise of top-down perfectionism, they are about the network. And so, the challenge, and this is why it's in no way surprising, is that Uber's drivers are now unionizing. It's extraordinary. Uber's drivers are turning on Uber. And the challenge for Uber — this isn't an easy situation for them — is that they are locked into a broader superstrcuture that is really old power. They've raised more than a billion dollars in the capital markets. Those markets expect a financial return, and they way you get a financial return is by squeezing and squeezing your users and your drivers for more and more value and giving that value to your investors.
So the big question about the future of new power, in my view, is: Will that old power just emerge? So will new power elites just become old power and squeeze? Or will that new power base bite back? Will the next big Uber be co-owned by Uber drivers? And I think this going to be a very interesting structural question.
Finally, think about new power being more than just an entity that scales things that make us have slightly better consumer experiences. My call to action for new power is to not be an island. We have major structural problems in the world today that could benefit enormously from the kinds of mass participation and peer coordination that these new power players know so well how to generate. And we badly need them to turn their energies and their power to big, what economists might call public goods problems, that are often beyond markets where investors can easily be found. And I think if we can do that, we might be able to fundamentally change not only human beings' sense of their own agency and power — because I think that's the most wonderful thing about new power, is that people feel more powerful — but we might also be able to change the way we relate to each other and the way we relate to authority and institutions. And to me, that's absolutely worth trying for. Thank you very much.
I would like to tell you a story connecting the notorious privacy incident involving Adam and Eve, and the remarkable shift in the boundaries between public and private which has occurred in the past 10 years.
You know the incident. Adam and Eve one day in the Garden of Eden realize they are naked. They freak out. And the rest is history.
Nowadays, Adam and Eve would probably act differently.
We do reveal so much more information about ourselves online than ever before, and so much information about us is being collected by organizations. Now there is much to gain and benefit from this massive analysis of personal information, or big data, but there are also complex tradeoffs that come from giving away our privacy. And my story is about these tradeoffs.
We start with an observation which, in my mind, has become clearer and clearer in the past few years, that any personal information can become sensitive information. Back in the year 2000, about 100 billion photos were shot worldwide, but only a minuscule proportion of them were actually uploaded online. In 2010, only on Facebook, in a single month, 2.5 billion photos were uploaded, most of them identified. In the same span of time, computers' ability to recognize people in photos improved by three orders of magnitude. What happens when you combine these technologies together: increasing availability of facial data; improving facial recognizing ability by computers; but also cloud computing, which gives anyone in this theater the kind of computational power which a few years ago was only the domain of three-letter agencies; and ubiquitous computing, which allows my phone, which is not a supercomputer, to connect to the Internet and do there hundreds of thousands of face metrics in a few seconds? Well, we conjecture that the result of this combination of technologies will be a radical change in our very notions of privacy and anonymity.
To test that, we did an experiment on Carnegie Mellon University campus. We asked students who were walking by to participate in a study, and we took a shot with a webcam, and we asked them to fill out a survey on a laptop. While they were filling out the survey, we uploaded their shot to a cloud-computing cluster, and we started using a facial recognizer to match that shot to a database of some hundreds of thousands of images which we had downloaded from Facebook profiles. By the time the subject reached the last page on the survey, the page had been dynamically updated with the 10 best matching photos which the recognizer had found, and we asked the subjects to indicate whether he or she found themselves in the photo.
Do you see the subject? Well, the computer did, and in fact did so for one out of three subjects.
So essentially, we can start from an anonymous face, offline or online, and we can use facial recognition to give a name to that anonymous face thanks to social media data. But a few years back, we did something else. We started from social media data, we combined it statistically with data from U.S. government social security, and we ended up predicting social security numbers, which in the United States are extremely sensitive information.
Do you see where I'm going with this? So if you combine the two studies together, then the question becomes, can you start from a face and, using facial recognition, find a name and publicly available information about that name and that person, and from that publicly available information infer non-publicly available information, much more sensitive ones which you link back to the face? And the answer is, yes, we can, and we did. Of course, the accuracy keeps getting worse. [27% of subjects' first 5 SSN digits identified (with 4 attempts)] But in fact, we even decided to develop an iPhone app which uses the phone's internal camera to take a shot of a subject and then upload it to a cloud and then do what I just described to you in real time: looking for a match, finding public information, trying to infer sensitive information, and then sending back to the phone so that it is overlaid on the face of the subject, an example of augmented reality, probably a creepy example of augmented reality. In fact, we didn't develop the app to make it available, just as a proof of concept.
In fact, take these technologies and push them to their logical extreme. Imagine a future in which strangers around you will look at you through their Google Glasses or, one day, their contact lenses, and use seven or eight data points about you to infer anything else which may be known about you. What will this future without secrets look like? And should we care?
We may like to believe that the future with so much wealth of data would be a future with no more biases, but in fact, having so much information doesn't mean that we will make decisions which are more objective. In another experiment, we presented to our subjects information about a potential job candidate. We included in this information some references to some funny, absolutely legal, but perhaps slightly embarrassing information that the subject had posted online. Now interestingly, among our subjects, some had posted comparable information, and some had not. Which group do you think was more likely to judge harshly our subject? Paradoxically, it was the group who had posted similar information, an example of moral dissonance.
Now you may be thinking, this does not apply to me, because I have nothing to hide. But in fact, privacy is not about having something negative to hide. Imagine that you are the H.R. director of a certain organization, and you receive résumés, and you decide to find more information about the candidates. Therefore, you Google their names and in a certain universe, you find this information. Or in a parallel universe, you find this information. Do you think that you would be equally likely to call either candidate for an interview? If you think so, then you are not like the U.S. employers who are, in fact, part of our experiment, meaning we did exactly that. We created Facebook profiles, manipulating traits, then we started sending out résumés to companies in the U.S., and we detected, we monitored, whether they were searching for our candidates, and whether they were acting on the information they found on social media. And they were. Discrimination was happening through social media for equally skilled candidates.
Now marketers like us to believe that all information about us will always be used in a manner which is in our favor. But think again. Why should that be always the case? In a movie which came out a few years ago, "Minority Report," a famous scene had Tom Cruise walk in a mall and holographic personalized advertising would appear around him. Now, that movie is set in 2054, about 40 years from now, and as exciting as that technology looks, it already vastly underestimates the amount of information that organizations can gather about you, and how they can use it to influence you in a way that you will not even detect.
So as an example, this is another experiment actually we are running, not yet completed. Imagine that an organization has access to your list of Facebook friends, and through some kind of algorithm they can detect the two friends that you like the most. And then they create, in real time, a facial composite of these two friends. Now studies prior to ours have shown that people don't recognize any longer even themselves in facial composites, but they react to those composites in a positive manner. So next time you are looking for a certain product, and there is an ad suggesting you to buy it, it will not be just a standard spokesperson. It will be one of your friends, and you will not even know that this is happening.
Now the problem is that the current policy mechanisms we have to protect ourselves from the abuses of personal information are like bringing a knife to a gunfight. One of these mechanisms is transparency, telling people what you are going to do with their data. And in principle, that's a very good thing. It's necessary, but it is not sufficient. Transparency can be misdirected. You can tell people what you are going to do, and then you still nudge them to disclose arbitrary amounts of personal information.
So in yet another experiment, this one with students, we asked them to provide information about their campus behavior, including pretty sensitive questions, such as this one. [Have you ever cheated in an exam?] Now to one group of subjects, we told them, "Only other students will see your answers." To another group of subjects, we told them, "Students and faculty will see your answers." Transparency. Notification. And sure enough, this worked, in the sense that the first group of subjects were much more likely to disclose than the second. It makes sense, right? But then we added the misdirection. We repeated the experiment with the same two groups, this time adding a delay between the time we told subjects how we would use their data and the time we actually started answering the questions.
How long a delay do you think we had to add in order to nullify the inhibitory effect of knowing that faculty would see your answers? Ten minutes? Five minutes? One minute? How about 15 seconds? Fifteen seconds were sufficient to have the two groups disclose the same amount of information, as if the second group now no longer cares for faculty reading their answers.
Now I have to admit that this talk so far may sound exceedingly gloomy, but that is not my point. In fact, I want to share with you the fact that there are alternatives. The way we are doing things now is not the only way they can done, and certainly not the best way they can be done. When someone tells you, "People don't care about privacy," consider whether the game has been designed and rigged so that they cannot care about privacy, and coming to the realization that these manipulations occur is already halfway through the process of being able to protect yourself. When someone tells you that privacy is incompatible with the benefits of big data, consider that in the last 20 years, researchers have created technologies to allow virtually any electronic transactions to take place in a more privacy-preserving manner. We can browse the Internet anonymously. We can send emails that can only be read by the intended recipient, not even the NSA. We can have even privacy-preserving data mining. In other words, we can have the benefits of big data while protecting privacy. Of course, these technologies imply a shifting of cost and revenues between data holders and data subjects, which is why, perhaps, you don't hear more about them.
Which brings me back to the Garden of Eden. There is a second privacy interpretation of the story of the Garden of Eden which doesn't have to do with the issue of Adam and Eve feeling naked and feeling ashamed. You can find echoes of this interpretation in John Milton's "Paradise Lost." In the garden, Adam and Eve are materially content. They're happy. They are satisfied. However, they also lack knowledge and self-awareness. The moment they eat the aptly named fruit of knowledge, that's when they discover themselves. They become aware. They achieve autonomy. The price to pay, however, is leaving the garden. So privacy, in a way, is both the means and the price to pay for freedom.
Again, marketers tell us that big data and social media are not just a paradise of profit for them, but a Garden of Eden for the rest of us. We get free content. We get to play Angry Birds. We get targeted apps. But in fact, in a few years, organizations will know so much about us, they will be able to infer our desires before we even form them, and perhaps buy products on our behalf before we even know we need them.
Now there was one English author who anticipated this kind of future where we would trade away our autonomy and freedom for comfort. Even more so than George Orwell, the author is, of course, Aldous Huxley. In "Brave New World," he imagines a society where technologies that we created originally for freedom end up coercing us. However, in the book, he also offers us a way out of that society, similar to the path that Adam and Eve had to follow to leave the garden. In the words of the Savage, regaining autonomy and freedom is possible, although the price to pay is steep. So I do believe that one of the defining fights of our times will be the fight for the control over personal information, the fight over whether big data will become a force for freedom, rather than a force which will hiddenly manipulate us.
Right now, many of us do not even know that the fight is going on, but it is, whether you like it or not. And at the risk of playing the serpent, I will tell you that the tools for the fight are here, the awareness of what is going on, and in your hands, just a few clicks away.
In the early days of Twitter, it was like a place of radical de-shaming. People would admit shameful secrets about themselves, and other people would say, "Oh my God, I'm exactly the same." Voiceless people realized that they had a voice, and it was powerful and eloquent. If a newspaper ran some racist or homophobic column, we realized we could do something about it. We could get them. We could hit them with a weapon that we understood but they didn't -- a social media shaming. Advertisers would withdraw their advertising. When powerful people misused their privilege, we were going to get them. This was like the democratization of justice. Hierarchies were being leveled out. We were going to do things better.

Soon after that, a disgraced pop science writer called Jonah Lehrer -- he'd been caught plagiarizing and faking quotes, and he was drenched in shame and regret, he told me. And he had the opportunity to publicly apologize at a foundation lunch. This was going to be the most important speech of his life. Maybe it would win him some salvation. He knew before he arrived that the foundation was going to be live-streaming his event, but what he didn't know until he turned up, was that they'd erected a giant screen Twitter feed right next to his head. (Laughter) Another one in a monitor screen in his eye line.
I don't think the foundation did this because they were monstrous. I think they were clueless: I think this was a unique moment when the beautiful naivety of Twitter was hitting the increasingly horrific reality.
And here were some of the Tweets that were cascading into his eye line, as he was trying to apologize:
"Jonah Lehrer, boring us into forgiving him." (Laughter)
And, "Jonah Lehrer has not proven that he is capable of feeling shame."
That one must have been written by the best psychiatrist ever, to know that about such a tiny figure behind a lectern.
And, "Jonah Lehrer is just a frigging sociopath."
That last word is a very human thing to do, to dehumanize the people we hurt. It's because we want to destroy people but not feel bad about it. Imagine if this was an actual court, and the accused was in the dark, begging for another chance, and the jury was yelling out, "Bored! Sociopath!" (Laughter)
You know, when we watch courtroom dramas, we tend to identify with the kindhearted defense attorney, but give us the power, and we become like hanging judges.
Power shifts fast. We were getting Jonah because he was perceived to have misused his privilege, but Jonah was on the floor then, and we were still kicking, and congratulating ourselves for punching up. And it began to feel weird and empty when there wasn't a powerful person who had misused their privilege that we could get. A day without a shaming began to feel like a day picking fingernails and treading water.
Let me tell you a story. It's about a woman called Justine Sacco. She was a PR woman from New York with 170 Twitter followers, and she'd Tweet little acerbic jokes to them, like this one on a plane from New York to London: [Weird German Dude: You're in first class. It's 2014. Get some deodorant." -Inner monologue as inhale BO. Thank god for pharmaceuticals.] So Justine chuckled to herself, and pressed send, and got no replies, and felt that sad feeling that we all feel when the Internet doesn't congratulate us for being funny. (Laughter) Black silence when the Internet doesn't talk back. And then she got to Heathrow, and she had a little time to spare before her final leg, so she thought up another funny little acerbic joke:
And she chuckled to herself, pressed send, got on the plane, got no replies, turned off her phone, fell asleep, woke up 11 hours later, turned on her phone while the plane was taxiing on the runway, and straightaway there was a message from somebody that she hadn't spoken to since high school, that said, "I am so sorry to see what's happening to you." And then another message from a best friend, "You need to call me right now. You are the worldwide number one trending topic on Twitter." (Laughter)
What had happened is that one of her 170 followers had sent the Tweet to a Gawker journalist, and he retweeted it to his 15,000 followers: [And now, a funny holiday joke from IAC's PR boss] And then it was like a bolt of lightning. A few weeks later, I talked to the Gawker journalist. I emailed him and asked him how it felt, and he said, "It felt delicious." And then he said, "But I'm sure she's fine."
But she wasn't fine, because while she slept, Twitter took control of her life and dismantled it piece by piece. First there were the philanthropists: [If @JustineSacco's unfortunate words ... bother you, join me in supporting @CARE's work in Africa.] [In light of ... disgusting, racist tweet, I'm donating to @care today] Then came the beyond horrified: [... no words for that horribly disgusting racist as fuck tweet from Justine Sacco. I am beyond horrified.]
Was anybody on Twitter that night? A few of you. Did Justine's joke overwhelm your Twitter feed the way it did mine? It did mine, and I thought what everybody thought that night, which was, "Wow, somebody's screwed! Somebody's life is about to get terrible!" And I sat up in my bed, and I put the pillow behind my head, and then I thought, I'm not entirely sure that joke was intended to be racist. Maybe instead of gleefully flaunting her privilege, she was mocking the gleeful flaunting of privilege. There's a comedy tradition of this, like South Park or Colbert or Randy Newman. Maybe Justine Sacco's crime was not being as good at it as Randy Newman. In fact, when I met Justine a couple of weeks later in a bar, she was just crushed, and I asked her to explain the joke, and she said, "Living in America puts us in a bit of a bubble when it comes to what is going on in the Third World. I was making of fun of that bubble."
You know, another woman on Twitter that night, a New Statesman writer Helen Lewis, she reviewed my book on public shaming and wrote that she Tweeted that night, "I'm not sure that her joke was intended to be racist," and she said straightaway she got a fury of Tweets saying, "Well, you're just a privileged bitch, too." And so to her shame, she wrote, she shut up and watched as Justine's life got torn apart.
It started to get darker: [Everyone go report this cunt @JustineSacco] Then came the calls for her to be fired. [Good luck with the job hunt in the new year. #GettingFired] Thousands of people around the world decided it was their duty to get her fired. [@JustineSacco last tweet of your career. #SorryNotSorry Corporations got involved, hoping to sell their products on the back of Justine's annihilation: [Next time you plan to tweet something stupid before you take off, make sure you are getting on a @Gogo flight!] (Laughter)
A lot of companies were making good money that night. You know, Justine's name was normally Googled 40 times a month. That month, between December the 20th and the end of December, her name was Googled 1,220,000 times. And one Internet economist told me that that meant that Google made somewhere between 120,000 dollars and 468,000 dollars from Justine's annihilation, whereas those of us doing the actual shaming -- we got nothing. (Laughter) We were like unpaid shaming interns for Google. (Laughter)
And then came the trolls: [I'm actually kind of hoping Justine Sacco gets aids? lol] Somebody else on that wrote, "Somebody HIV-positive should rape this bitch and then we'll find out if her skin color protects her from AIDS." And that person got a free pass. Nobody went after that person. We were all so excited about destroying Justine, and our shaming brains are so simple-minded, that we couldn't also handle destroying somebody who was inappropriately destroying Justine. Justine was really uniting a lot of disparate groups that night, from philanthropists to "rape the bitch." [@JustineSacco I hope you get fired! You demented bitch... Just let the world know you're planning to ride bare back while in Africa.]
Women always have it worse than men. When a man gets shamed, it's, "I'm going to get you fired." When a woman gets shamed, it's, "I'm going to get you fired and raped and cut out your uterus."
And then Justine's employers got involved: [IAC on @JustineSacco tweet: This is an outrageous, offensive comment. Employee in question currently unreachable on an intl flight.] And that's when the anger turned to excitement: [All I want for Christmas is to see @JustineSacco's face when her plane lands and she checks her inbox/voicemail. #fired] [Oh man, @justinesacco is going to have the most painful phone-turning-on moment ever when her plane lands.] [We are about to watch this @JustineSacco bitch get fired. In REAL time. Before she even KNOWS she's getting fired.] What we had was a delightful narrative arc. We knew something that Justine didn't. Can you think of anything less judicial than this? Justine was asleep on a plane and unable to explain herself, and her inability was a huge part of the hilarity. On Twitter that night, we were like toddlers crawling towards a gun. Somebody worked out exactly which plane she was on, so they linked to a flight tracker website. [British Airways Flight 43 On-time - arrives in 1 hour 34 minutes] A hashtag began trending worldwide: # hasJustineLandedYet? [It is kinda wild to see someone self-destruct without them even being aware of it. #hasJustineLandedYet] [Seriously. I just want to go home to go to bed, but everyone at the bar is SO into #HasJustineLandedYet. Can't look away. Can't leave.] [#HasJustineLandedYet may be the best thing to happen to my Friday night.] [Is no one in Cape Town going to the airport to tweet her arrival? Come on, twitter! I'd like pictures] And guess what? Yes there was. [@JustineSacco HAS in fact landed at Cape Town international. And if you want to know what it looks like to discover that you've just been torn to shreds because of a misconstrued liberal joke, not by trolls, but by nice people like us, this is what it looks like: [... She's decided to wear sunnies as a disguise.]
So why did we do it? I think some people were genuinely upset, but I think for other people, it's because Twitter is basically a mutual approval machine. We surround ourselves with people who feel the same way we do, and we approve each other, and that's a really good feeling. And if somebody gets in the way, we screen them out. And do you know what that's the opposite of? It's the opposite of democracy. We wanted to show that we cared about people dying of AIDS in Africa. Our desire to be seen to be compassionate is what led us to commit this profoundly un-compassionate act. As Meghan O'Gieblyn wrote in the Boston Review, "This isn't social justice. It's a cathartic alternative."
For the past three years, I've been going around the world meeting people like Justine Sacco -- and believe me, there's a lot of people like Justine Sacco. There's more every day. And we want to think they're fine, but they're not fine. The people I met were mangled. They talked to me about depression, and anxiety and insomnia and suicidal thoughts. One woman I talked to, who also told a joke that landed badly, she stayed home for a year and a half. Before that, she worked with adults with learning difficulties, and was apparently really good at her job.
Justine was fired, of course, because social media demanded it. But it was worse than that. She was losing herself. She was waking up in the middle of the night, forgetting who she was. She was got because she was perceived to have misused her privilege. And of course, that's a much better thing to get people for than the things we used to get people for, like having children out of wedlock. But the phrase "misuse of privilege" is becoming a free pass to tear apart pretty much anybody we choose to. It's becoming a devalued term, and it's making us lose our capacity for empathy and for distinguishing between serious and unserious transgressions.
Justine had 170 Twitter followers, and so to make it work, she had to be fictionalized. Word got around that she was the daughter the mining billionaire Desmond Sacco. [Let us not be fooled by #JustineSacco her father is a SA mining billionaire. She's not sorry. And neither is her father.] I thought that was true about Justine, until I met her at a bar, and I asked her about her billionaire father, and she said, "My father sells carpets."
And I think back on the early days of Twitter, when people would admit shameful secrets about themselves, and other people would say, "Oh my God, I'm exactly the same." These days, the hunt is on for people's shameful secrets. You can lead a good, ethical life, but some bad phraseology in a Tweet can overwhelm it all, become a clue to your secret inner evil.
Maybe there's two types of people in the world: those people who favor humans over ideology, and those people who favor ideology over humans. I favor humans over ideology, but right now, the ideologues are winning, and they're creating a stage for constant artificial high dramas where everybody's either a magnificent hero or a sickening villain, even though we know that's not true about our fellow humans. What's true is that we are clever and stupid; what's true is that we're grey areas. The great thing about social media was how it gave a voice to voiceless people, but we're now creating a surveillance society, where the smartest way to survive is to go back to being voiceless.
BG: Don't go away. What strikes me about Justine's story is also the fact that if you Google her name today, this story covers the first 100 pages of Google results -- there is nothing else about her. In your book, you mention another story of another victim who actually got taken on by a reputation management firm, and by creating blogs and posting nice, innocuous stories about her love for cats and holidays and stuff, managed to get the story off the first couple pages of Google results, but it didn't last long. A couple of weeks later, they started creeping back up to the top result. Is this a totally lost battle?
Jon Ronson: You know, I think the very best thing we can do, if you see a kind of unfair or an ambiguous shaming, is to speak up, because I think the worst thing that happened to Justine was that nobody supported her -- like, everyone was against her, and that is profoundly traumatizing, to be told by tens of thousands of people that you need to get out. But if a shaming happens and there's a babble of voices, like in a democracy, where people are discussing it, I think that's much less damaging. So I think that's the way forward, but it's hard, because if you do stand up for somebody, it's incredibly unpleasant.
BG: So let's talk about your experience, because you stood up by writing this book. By the way, it's mandatory reading for everybody, okay? You stood up because the book actually puts the spotlight on shamers. And I assume you didn't only have friendly reactions on Twitter.
JR: It didn't go down that well with some people. (Laughter) I mean, you don't want to just concentrate -- because lots of people understood, and were really nice about the book. But yeah, for 30 years I've been writing stories about abuses of power, and when I say the powerful people over there in the military, or in the pharmaceutical industry, everybody applauds me. As soon as I say, "We are the powerful people abusing our power now," I get people saying, "Well you must be a racist too."
BG: So the other night -- yesterday -- we were at dinner, and there were two discussions going on. On one side you were talking with people around the table -- and that was a nice, constructive discussion. On the other, every time you turned to your phone, there is this deluge of insults.
JR: Yeah. This happened last night. We had like a TED dinner last night. We were chatting and it was lovely and nice, and I decided to check Twitter. Somebody said, "You are a white supremacist." And then I went back and had a nice conversation with somebody, and then I went back to Twitter, somebody said my very existence made the world a worse place. My friend Adam Curtis says that maybe the Internet is like a John Carpenter movie from the 1980s, when eventually everyone will start screaming at each other and shooting each other, and then eventually everybody would flee to somewhere safer, and I'm starting to think of that as a really nice option.
BG: Jon, thank you. JR: Thank you, Bruno.
When I wrote my memoir, the publishers were really confused. Was it about me as a child refugee, or as a woman who set up a high-tech software company back in the 1960s, one that went public and eventually employed over 8,500 people? Or was it as a mother of an autistic child? Or as a philanthropist that's now given away serious money? Well, it turns out, I'm all of these. So let me tell you my story.
All that I am stems from when I got onto a train in Vienna, part of the Kindertransport that saved nearly 10,000 Jewish children from Nazi Europe. I was five years old, clutching the hand of my nine-year-old sister and had very little idea as to what was going on. "What is England and why am I going there?" I'm only alive because so long ago, I was helped by generous strangers. I was lucky, and doubly lucky to be later reunited with my birth parents. But, sadly, I never bonded with them again. But I've done more in the seven decades since that miserable day when my mother put me on the train than I would ever have dreamed possible. And I love England, my adopted country, with a passion that perhaps only someone who has lost their human rights can feel. I decided to make mine a life that was worth saving. And then, I just got on with it. (Laughter)
Let me take you back to the early 1960s. To get past the gender issues of the time, I set up my own software house at one of the first such startups in Britain. But it was also a company of women, a company for women, an early social business. And people laughed at the very idea because software, at that time, was given away free with hardware. Nobody would buy software, certainly not from a woman. Although women were then coming out of the universities with decent degrees, there was a glass ceiling to our progress. And I'd hit that glass ceiling too often, and I wanted opportunities for women.
I recruited professionally qualified women who'd left the industry on marriage, or when their first child was expected and structured them into a home-working organization. We pioneered the concept of women going back into the workforce after a career break. We pioneered all sorts of new, flexible work methods: job shares, profit-sharing, and eventually, co-ownership when I took a quarter of the company into the hands of the staff at no cost to anyone but me. For years, I was the first woman this, or the only woman that. And in those days, I couldn't work on the stock exchange, I couldn't drive a bus or fly an airplane. Indeed, I couldn't open a bank account without my husband's permission. My generation of women fought the battles for the right to work and the right for equal pay.
Nobody really expected much from people at work or in society because all the expectations then were about home and family responsibilities. And I couldn't really face that, so I started to challenge the conventions of the time, even to the extent of changing my name from "Stephanie" to "Steve" in my business development letters, so as to get through the door before anyone realized that he was a she. 
My company, called Freelance Programmers, and that's precisely what it was, couldn't have started smaller: on the dining room table, and financed by the equivalent of 100 dollars in today's terms, and financed by my labor and by borrowing against the house. My interests were scientific, the market was commercial -- things such as payroll, which I found rather boring. So I had to compromise with operational research work, which had the intellectual challenge that interested me and the commercial value that was valued by the clients: things like scheduling freight trains, time-tabling buses, stock control, lots and lots of stock control. And eventually, the work came in. We disguised the domestic and part-time nature of the staff by offering fixed prices, one of the very first to do so. And who would have guessed that the programming of the black box flight recorder of Supersonic Concord would have been done by a bunch of women working in their own homes. (Applause)
All we used was a simple "trust the staff" approach and a simple telephone. We even used to ask job applicants, "Do you have access to a telephone?"
An early project was to develop software standards on management control protocols. And software was and still is a maddeningly hard-to-control activity, so that was enormously valuable. We used the standards ourselves, we were even paid to update them over the years, and eventually, they were adopted by NATO. Our programmers -- remember, only women, including gay and transgender -- worked with pencil and paper to develop flowcharts defining each task to be done. And they then wrote code, usually machine code, sometimes binary code, which was then sent by mail to a data center to be punched onto paper tape or card and then re-punched, in order to verify it. All this, before it ever got near a computer. That was programming in the early 1960s.
In 1975, 13 years from startup, equal opportunity legislation came in in Britain and that made it illegal to have our pro-female policies. And as an example of unintended consequences, my female company had to let the men in. (Laughter)
When I started my company of women, the men said, "How interesting, because it only works because it's small." And later, as it became sizable, they accepted, "Yes, it is sizable now, but of no strategic interest." And later, when it was a company valued at over three billion dollars, and I'd made 70 of the staff into millionaires, they sort of said, "Well done, Steve!" (Laughter) (Applause)
You can always tell ambitious women by the shape of our heads: They're flat on top for being patted patronizingly. (Laughter) (Applause) And we have larger feet to stand away from the kitchen sink. 
Let me share with you two secrets of success: Surround yourself with first-class people and people that you like; and choose your partner very, very carefully. Because the other day when I said, "My husband's an angel," a woman complained -- "You're lucky," she said, "mine's still alive." (Laughter)
If success were easy, we'd all be millionaires. But in my case, it came in the midst of family trauma and indeed, crisis. Our late son, Giles, was an only child, a beautiful, contented baby. And then, at two and a half, like a changeling in a fairy story, he lost the little speech that he had and turned into a wild, unmanageable toddler. Not the terrible twos; he was profoundly autistic and he never spoke again. Giles was the first resident in the first house of the first charity that I set up to pioneer services for autism. And then there's been a groundbreaking Prior's Court school for pupils with autism and a medical research charity, again, all for autism. Because whenever I found a gap in services, I tried to help. I like doing new things and making new things happen. And I've just started a three-year think tank for autism.
And so that some of my wealth does go back to the industry from which it stems, I've also founded the Oxford Internet Institute and other IT ventures. The Oxford Internet Institute focuses not on the technology, but on the social, economic, legal and ethical issues of the Internet.
Giles died unexpectedly 17 years ago now. And I have learned to live without him, and I have learned to live without his need of me. Philanthropy is all that I do now. I need never worry about getting lost because several charities would quickly come and find me. (Laughter)
It's one thing to have an idea for an enterprise, but as many people in this room will know, making it happen is a very difficult thing and it demands extraordinary energy, self-belief and determination, the courage to risk family and home, and a 24/7 commitment that borders on the obsessive. So it's just as well that I'm a workaholic. I believe in the beauty of work when we do it properly and in humility. Work is not just something I do when I'd rather be doing something else.
We live our lives forward. So what has all that taught me? I learned that tomorrow's never going to be like today, and certainly nothing like yesterday. And that made me able to cope with change, indeed, eventually to welcome change, though I'm told I'm still very difficult.
I'd like to reimagine education. The last year has seen the invention of a new four-letter word. It starts with an M. MOOC: massive open online courses. Many organizations are offering these online courses to students all over the world, in the millions, for free. Anybody who has an Internet connection and the will to learn can access these great courses from excellent universities and get a credential at the end of it. Now, in this discussion today, I'm going to focus on a different aspect of MOOCs. We are taking what we are learning and the technologies we are developing in the large and applying them in the small to create a blended model of education to really reinvent and reimagine what we do in the classroom.
Now, our classrooms could use change. So, here's a classroom at this little three-letter institute in the Northeast of America, MIT. And this was a classroom about 50 or 60 years ago, and this is a classroom today. What's changed? The seats are in color. Whoop-de-do. Education really hasn't changed in the past 500 years. The last big innovation in education was the printing press and the textbooks. Everything else has changed around us. You know, from healthcare to transportation, everything is different, but education hasn't changed.
It's also been a real issue in terms of access. So what you see here is not a rock concert. And the person you see at the end of the stage is not Madonna. This is a classroom at the Obafemi Awolowo University in Nigeria. Now, we've all heard of distance education, but the students way in the back, 200 feet away from the instructor, I think they are undergoing long-distance education. Now, I really believe that we can transform education, both in quality and scale and access, through technology. For example, at edX, we are trying to transform education through online technologies. Given education has been calcified for 500 years, we really cannot think about reengineering it, micromanaging it. We really have to completely reimagine it. It's like going from ox carts to the airplane. Even the infrastructure has to change. Everything has to change. We need to go from lectures on the blackboard to online exercises, online videos. We have to go to interactive virtual laboratories and gamification. We have to go to completely online grading and peer interaction and discussion boards. Everything really has to change.
So at edX and a number of other organizations, we are applying these technologies to education through MOOCs to really increase access to education. And you heard of this example, where, when we launched our very first course -- and this was an MIT-hard circuits and electronics course -- about a year and a half ago, 155,000 students from 162 countries enrolled in this course. And we had no marketing budget. Now, 155,000 is a big number. This number is bigger than the total number of alumni of MIT in its 150-year history. 7,200 students passed the course, and this was a hard course. 7,200 is also a big number. If I were to teach at MIT two semesters every year, I would have to teach for 40 years before I could teach this many students.
Now these large numbers are just one part of the story. So today, I want to discuss a different aspect, the other side of MOOCs, take a different perspective. We are taking what we develop and learn in the large and applying it in the small to the classroom, to create a blended model of learning.
But before I go into that, let me tell you a story. When my daughter turned 13, became a teenager, she stopped speaking English, and she began speaking this new language. I call it teen-lish. It's a digital language. It's got two sounds: a grunt and a silence.
"Honey, come over for dinner."
"Hmm."
"Did you hear me?"
Silence. (Laughter)
"Can you listen to me?"
So we had a real issue with communicating, and we were just not communicating, until one day I had this epiphany. I texted her. (Laughter) I got an instant response. I said, no, that must have been by accident. She must have thought, you know, some friend of hers was calling her. So I texted her again. Boom, another response. I said, this is great. And so since then, our life has changed. I text her, she responds. It's just been absolutely great. (Applause)
So our millennial generation is built differently. Now, I'm older, and my youthful looks might belie that, but I'm not in the millennial generation. But our kids are really different. The millennial generation is completely comfortable with online technology. So why are we fighting it in the classroom? Let's not fight it. Let's embrace it. In fact, I believe -- and I have two fat thumbs, I can't text very well -- but I'm willing to bet that with evolution, our kids and their grandchildren will develop really, really little, itty-bitty thumbs to text much better, that evolution will fix all of that stuff. But what if we embraced technology, embraced the millennial generation's natural predilections, and really think about creating these online technologies, blend them into their lives. So here's what we can do. So rather than driving our kids into a classroom, herding them out there at 8 o'clock in the morning -- I hated going to class at 8 o'clock in the morning, so why are we forcing our kids to do that? So instead what you do is you have them watch videos and do interactive exercises in the comfort of their dorm rooms, in their bedroom, in the dining room, in the bathroom, wherever they're most creative. Then they come into the classroom for some in-person interaction. They can have discussions amongst themselves. They can solve problems together. They can work with the professor and have the professor answer their questions. In fact, with edX, when we were teaching our first course on circuits and electronics around the world, this was happening unbeknownst to us. Two high school teachers at the Sant High School in Mongolia had flipped their classroom, and they were using our video lectures and interactive exercises, where the learners in the high school, 15-year-olds, mind you, would go and do these things in their own homes and they would come into class, and as you see from this image here, they would interact with each other and do some physical laboratory work. And the only way we discovered this was they wrote a blog and we happened to stumble upon that blog.
We were also doing other pilots. So we did a pilot experimental blended courses, working with San Jose State University in California, again, with the circuits and electronics course. You'll hear that a lot. That course has become sort of like our petri dish of learning. So there, the students would, again, the instructors flipped the classroom, blended online and in person, and the results were staggering. Now don't take these results to the bank just yet. Just wait a little bit longer as we experiment with this some more, but the early results are incredible. So traditionally, semester upon semester, for the past several years, this course, again, a hard course, had a failure rate of about 40 to 41 percent every semester. With this blended class late last year, the failure rate fell to nine percent. So the results can be extremely, extremely good.
Now before we go too far into this, I'd like to spend some time discussing some key ideas. What are some key ideas that makes all of this work?
One idea is active learning. The idea here is, rather than have students walk into class and watch lectures, we replace this with what we call lessons. Lessons are interleaved sequences of videos and interactive exercises. So a student might watch a five-, seven-minute video and follow that with an interactive exercise. Think of this as the ultimate Socratization of education. You teach by asking questions. And this is a form of learning called active learning, and really promoted by a very early paper, in 1972, by Craik and Lockhart, where they said and discovered that learning and retention really relates strongly to the depth of mental processing. Students learn much better when they are interacting with the material.
The second idea is self-pacing. Now, when I went to a lecture hall, and if you were like me, by the fifth minute I would lose the professor. I wasn't all that smart, and I would be scrambling, taking notes, and then I would lose the lecture for the rest of the hour. Instead, wouldn't it be nice with online technologies, we offer videos and interactive engagements to students? They can hit the pause button. They can rewind the professor. Heck, they can even mute the professor. So this form of self-pacing can be very helpful to learning.
The third idea that we have is instant feedback. With instant feedback, the computer grades exercises. I mean, how else do you teach 150,000 students? Your computer is grading all the exercises. And we've all submitted homeworks, and your grades come back two weeks later, you've forgotten all about it. I don't think I've still received some of my homeworks from my undergraduate days. Some are never graded. So with instant feedback, students can try to apply answers. If they get it wrong, they can get instant feedback. They can try it again and try it again, and this really becomes much more engaging. They get the instant feedback, and this little green check mark that you see here is becoming somewhat of a cult symbol at edX. Learners are telling us that they go to bed at night dreaming of the green check mark. In fact, one of our learners who took the circuits course early last year, he then went on to take a software course from Berkeley at the end of the year, and this is what the learner had to say on our discussion board when he just started that course about the green check mark: &quot;Oh god; have I missed you.&quot; When's the last time you've seen students posting comments like this about homework? My colleague Ed Bertschinger, who heads up the physics department at MIT, has this to say about instant feedback: He indicated that instant feedback turns teaching moments into learning outcomes.
The next big idea is gamification. You know, all learners engage really well with interactive videos and so on. You know, they would sit down and shoot alien spaceships all day long until they get it. So we applied these gamification techniques to learning, and we can build these online laboratories. How do you teach creativity? How do you teach design? We can do this through online labs and use computing power to build these online labs. So as this little video shows here, you can engage students much like they design with Legos. So here, the learners are building a circuit with Lego-like ease. And this can also be graded by the computer.
Fifth is peer learning. So here, we use discussion forums and discussions and Facebook-like interaction not as a distraction, but to really help students learn. Let me tell you a story. When we did our circuits course for the 155,000 students, I didn't sleep for three nights leading up to the launch of the course. I told my TAs, okay, 24/7, we're going to be up monitoring the forum, answering questions. They had answered questions for 100 students. How do you do that for 150,000? So one night I'm sitting up there, at 2 a.m. at night, and I think there's this question from a student from Pakistan, and he asked a question, and I said, okay, let me go and type up an answer, I don't type all that fast, and I begin typing up the answer, and before I can finish, another student from Egypt popped in with an answer, not quite right, so I'm fixing the answer, and before I can finish, a student from the U.S. had popped in with a different answer. And then I sat back, fascinated. Boom, boom, boom, boom, the students were discussing and interacting with each other, and by 4 a.m. that night, I'm totally fascinated, having this epiphany, and by 4 a.m. in the morning, they had discovered the right answer. And all I had to do was go and bless it, "Good answer." So this is absolutely amazing, where students are learning from each other, and they're telling us that they are learning by teaching.
Now this is all not just in the future. This is happening today. So we are applying these blended learning pilots in a number of universities and high schools around the world, from Tsinghua in China to the National University of Mongolia in Mongolia to Berkeley in California -- all over the world. And these kinds of technologies really help, the blended model can really help revolutionize education. It can also solve a practical problem of MOOCs, the business aspect. We can also license these MOOC courses to other universities, and therein lies a revenue model for MOOCs, where the university that licenses it with the professor can use these online courses like the next-generation textbook. They can use as much or as little as they like, and it becomes a tool in the teacher's arsenal.
Finally, I would like to have you dream with me for a little bit. I would like us to really reimagine education. We will have to move from lecture halls to e-spaces. We have to move from books to tablets like the Aakash in India or the Raspberry Pi, 20 dollars. The Aakash is 40 dollars. We have to move from bricks-and-mortar school buildings to digital dormitories.
But I think at the end of the day, I think we will still need one lecture hall in our universities. Otherwise, how else do we tell our grandchildren that your grandparents sat in that room in neat little rows like cornstalks and watched this professor at the end talk about content and, you know, you didn't even have a rewind button? 
